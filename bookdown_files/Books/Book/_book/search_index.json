[["index.html", "R语言在心理学研究中的应用: 从原始数据到可重复的论文手稿(V2) 1 教学内容与课时 1.1 目录", " R语言在心理学研究中的应用: 从原始数据到可重复的论文手稿(V2) 胡传鹏 2024年04月16日 1 教学内容与课时 1.1 目录 第一讲：为什么要学习R 1.1 R在心理科学及社会科学中的运用 1.2 R语言使用的示例展示 1.3 课程安排 1.4 如何学好这门课 第二讲：如何开始使用R 2.1 要解决的数据分析问题简介[介绍我们的数据和拟解决的问题，对比R和传统flow] 2.1 如何安装？ 2.2 如何方便使用？Rstudio的安装与界面介绍 第三章：如何使用本课件/电子书资源 3.1 Git与Github 3.2 项目、文件与代码的规范化 第四章：如何导入数据 4.1 路径与工作目录 4.2 读取数据 4.3 了解R里的数据 （R语言中的对象） 第五章：如何清理数据一 R语言编程基础 5.1 R对象的操控 5.2 逻辑运算 5.3 函数 第六章：如何清理数据二 数据的预处理 6.1 Tidyverse简介 6.2 问卷数据的预处理:基本 6.3 实验数据的预处理:提高 第七章：探索数据: 描述性统计与数据可视化基础 7.1 描述性统计 7.2 探索性数据分析(DataExplorer) 7.3 ggplot2基础 第八章：R语言中的统计分析: 线性模型1 (t-test、anova等) 8.1 语法实现 8.2 分析的流程 第九章：R语言中的统计分析: 线性模型2(rm-anova、层级模型) 9.1 语法实现 9.2 分析的流程 第十章：R语言中的统计分析: 线性模型3(GLM) 10.1 语法实现 10.2 分析的流程 第十一章：R语言中的统计分析: 线性模型4(中介效应模型) 11.1. 多种分析方法的实现 11.2 代码整合与规范化 第十二章: 如何得到可发表的图像: 数据可视化进阶（3学时） 12.1 ggplot2的图层与面板控制 12.2 ggplot2与其他工具的结合 第十三章：待定 第十四章：待定 第十五章：待定 第十六章： 大作业 "],["lesson-1.html", "2 第一讲：为什么要学习R 2.1 R在心理科学及社会科学中的运用 2.2 R语言使用的示例展示 2.3 现场运行代码 2.4 课程安排 2.5 如何学好这门课 2.6 课程总结与期望 2.7 推荐", " 2 第一讲：为什么要学习R 序 本课程主要面向具有心理学背景的学生，包括教育学、社会学等相近领域的同学。今天的第一堂课旨在让南师大心理学的同学们了解这门课程的性质，判断它是否适合自己。我知道在开学初期，大家仍有机会退选课程。这里我想提醒大家，虽然R语言编程语言非常有趣，但它并不适合所有人。有些同学上完课后，在处理数据时仍然使用SPSS，这说明本课程对他们来说没有太大意义。如果只是因为学分而选择这门课，我认为没有必要，你完全可以选择一些更有趣的课程。在本课程的学习中，你可能会遇到一些情绪上的起伏，因为这与我们以前学习的知识不同，需要我们改变思考问题的方式。如果你不打算采用R语言进行数据分析，那就没有必要选择这门课。当然，我也希望通过这门课程，向大家展示学习R语言编程语言的价值，让大家认为经历情绪的起伏是有值得的。 我也希望大家更多地使用开源软件，放弃使用商业软件，尤其是在国内存在盗版泛滥的情况下。学完课程可以帮助你逐渐转向使用开源软件。此外，现在学习R语言比几年前更加轻松，因为现在有很多在线资料和脚本可以参考。完成这门课程后，你可能至少会接触一些代码，这些代码可以直接应用于分析某些数据。 本次课的主要目的在于帮助大家了解本课程的基本情况，为本课程做好心理准备。所以接下来，我们主要介绍为什么要开设这门课程、课程的内容是什么、需要做什么样的准备以及能收获什么。 2.1 R在心理科学及社会科学中的运用 2.1.1 数据科学 这门课的开设有其时代的大背景。作为在心理学院的课程，我们将这门课称为《R语言在心理学研究当中的应用》。但实际上，R语言是当前数据科学（data science）中主流的计算机语言之一。 正是数据科学在各种学科中的渗透和普及，让我们开设这门课程显得非常重要。那么什么是data science呢？ 数据科学是什么？ 在科学研究中有人认为，科学的革命是经过了几次范式转换的（参考链接）。最早期的是”实验”科学，研究者通过设计和完成实验，一个一个地去检验假设。随后是理论科学，在实验基础上进行归纳。随着计算机越来越发达，我们进入了”计算”时代，通过用各种计算模型模拟的方法，帮助我们去理解世界。但是现在，随着数据越来越多，通过数据驱动的方式就能发现很多新的东西。最近这些年，很多在科技领域尤其是在计算机领域取得的重大突破和进展都是依赖于大量数据的，也就是通过对数据进行“提炼”从而得到新的发现。比如说2023年初非常火的ChatGPT。作为现在全球最火的科技界产品之一，它背后的模型叫做LLM，Large Language Model，就是一个大语言模型，它依靠的就是大量语言材料的训练。 数据科学的内容 大概10多年前，数据科学就已经出现。大家也许对“数据科学”这个术语已经不再陌生。数据科学里面既涉及到计算机编程，也包括数理统计。当讨论具体应用领域的数据科学，比如心理学的科研领域，数据科学也需要domain-specific的知识，也就是这个领域的特殊性知识。 [此处插入关于数据科学的Venn图] 这意味着什么？ 意味着如果你仅仅懂计算机，那你不一定能懂data science的；如果你仅仅是懂数学和统计，那也不意味这你能解决一个data science的问题。必须要将计算、统计和领域特殊的知识进行结合。在心理学研究中，这对研究生提出一个新的要求。 2.1.2 数据科学的诞生——数字化时代 为什么会有data science？ 大家应该能直观地感受到：随着个人电脑的普及，互联网越来越发达，整个社会所产生的数据呈现爆炸式的增长。下图是一个可视化的例子。我们可以看到，在计算机出现之前人类产生的数据是非常少的，而计算机出现之后产生的数据越来越多。 我们也有了越来越多的个人电子设备以及其他的先进设备，它们所观察到的、产生的数据也是非常大的。这个图片相信很多人在朋友圈都被刷屏过。这是人类所能观察到的一个划时代的新的图像，尽管我们作为外行可能不知道它具体的内涵是什么，但是都知道它很酷。 此外，我们国家是互联网普及最高的国家之一，我们现在有百分之七十四（可能现在有更多了）的人都已经开始接入到互联网。这么多的人接入到互联网，所产生的数据可想而知，一定是海量的。所以现在我们很多电商，像淘宝这样的各类购物平台，它们在中国做的是非常好的，这也得益于海量的数据。包括最近像拼多多，听说在美国也是势如破竹，态势很猛。还有像TikTok，就是字节跳动，前一段时间在美国甚至要被封杀了，原因就是年青人都很喜欢用它。字节跳动的一个特点就是它很好地利用了中国大量的网民产生的海量数据，通过网民不断地使用它们的产品，不断地进行迭代。所以当它能出海的时候，去给海外的用户提供服务的时候，它的迭代已经非常成熟了。当然迭代的过程中需要大量数据的产生和调试，产品才能越来越成熟。 数字化对心理学研究的影响 近年来，随着人工智能的兴起，数据科学变得越来越重要。我们生活在大数据时代，心理学或心理科学也在这个时代中发展起来。我们收集的数据越来越多，我们的生活方式也在逐渐数字化。因此，数字化成为了新时代的重要组成部分。在数字化时代，心理学不可避免地会受到数据化的影响。 其实很早就有人关注数字化我们心理学的影响了，国内的研究者也经常会提到心理学和大数据。我们这里做一个不完全的概括，主要是这三个方面的显著变化。 Big n (sample size) 首先就是样本量很大。现在的数字化平台生产的数据非常大。我们传统的行为学实验可能只有几十、几百人的数据，上千已经是很不错的了，上万就比较费劲了。但是如果说我们能从互联网上抓取数据的话，那动辄就是上万甚至是百万级别的。 例如，与企业合作或者使用可穿戴设备，我们可以收集到大量的心理健康数据，这已经超越了传统心理学研究的范畴。 Big v (variables) 其次，我们现在面对的数据不仅仅是数量上的增加，还有数据维度的增加。以前我们可能只关注反应时间和准确率等少数几个变量，但现在我们可能需要处理大量的参数，比如个人的购物记录、聊天记录、身体活动数据等。比如我们使用手机，那其实我们产生了非常多的数据。你一天使用多久、点击多少次、点击了什么、在哪个地方、用的是什么APP，甚至包括你所处的地址，你在地球上的经纬度、当地的气温、湿度这些数据信息。这些数据的维度非常多，每个变量都可能对我们的研究产生影响。 Big t (time) 还有就是时间的跨度比较长。现在很多的APP一旦用户开始使用之后就会长期使用，如果能用于收集心理学的数据，就可以在很长的一段时间里记录很多的数据(Experience Sampling Method, ESM, or Ecological Momentary Assessment, EMA)。我们现在有了新的研究设备，比如通过手机应用程序不断追踪个体的心理状态，可能一天收集五六次，连续追踪一个月，这样就会收集到大量的数据。这些数据为我们提供了新的分析挑战，我们需要新的方法来处理这些数据。 这对于了解人类的心理和行为的规律来说其实是非常好的。对于发展心理学家来说，有这样的一个例子：有一个做语言发展的一个研究者他从自己的孩子出生开始就一直用视频记录孩子的成长（见science最近的文章）。儿童产生语言目前对于人类来说还是一个不能够完全理解的过程，从语言学上看这是一个很大的跳跃。这位研究者完全记录了孩子生长的过程，积累了大量的数据，并通过对视频数据的全方位分析，得到了以前心理语言学比较少能够得到的东西。 数字化时代的心理学研究 这里有几个例子。比如这个是利用手机里的数据预测人格，我们这里看到的纵轴就是人格的类型，不同的颜色表示使用不同的方法进行预测，横轴是相关系数。可以发现说手机里的一些数据和人格的倾向是密切相关的。 另外，像我们心理学的顶刊Psychological Science，上面也不定期的有研究是探索我们在数字的视觉中留下的痕迹跟我们的行为之间的关系。比如这篇文章发现我们个体在使用手机的行为上是非常的一致的。研究者通过大量的数据是得到了一个比较强的一个结论，尽管这个结论比较简单，但是基于数据量比较大，说服力是比较强的。 我们也可以利用互联网的平台来收集行为的数据。传统上我们是把被试带到实验室做实验，但现在由于互联网的发达，我们可以进行线上实验。在疫情期间，不少同学可能尝试过JsPsych或者巴普洛夫这样的平台，把实验编写好放在网上，并通过链接给被试发放被试费。这样我们在短时间内可以收到比传统实验室更大量的数据。同时，因为我们使用的其实还是传统的实验任务，只要事先验证过在线的实验和实验室实验的可比性，就可以利用互联网在线去收集更大量的数据来研究我们感兴趣的问题。 比如这里列举的实验，去年的R课上我们以此作为了示例。它就是通过在线的平台收集的数据，收集到了很多变量。它的研究一个目的是为了探测self regulation，自我控制或者自我调节，不同的测量方法是不是一致的，哪一个能更好地预测生活中的一些行为，比方说这里就是看哪一些与饮食控制有更强的相关。 数字化时代心理学研究方式的变化 数字化时代不仅给心理学提供了新的数据，实际上也变革了我们做研究的方式。比如说合作。在疫情以前，研究者建立国际合作主要是通过导师的联系、邀请讲座或者是开会时建立联系。但在疫情期间，因为大家都困在家里同时又都使用很发达的互联网，所以就有研究者直接在社交媒体上发起合作的倡议。比如有的人说我有个想法需要在不同国家收数据，想要有人和我一起收集数据，最后可以一起发表文章。在疫情期间很多研究就是这么展开的，并且因为这样的研究往往样本量比较大，也能发到很好的期刊上去。同时，除了个人的发起，研究者们也成立了更多的学术组织，比方说Psychological Science Accelerator (https://psysciacc.org/)心理科学加速器，这个组织就是专门组织在全球范围通过互联网进行合作研究。 此外，以前研究者需要去线下开会或者是参加工作坊，现在即便是疫情过去了，大家还是越来越习惯和更多地采用在线的方式进行学术讲座或是工作坊。所以呢互联网实际上是改变了我们心理学的方方面面。 2.1.3 为什么要学习R语言？{1-why-learn-R} 我个人总结了一些我们学习R语言或者说学习R，而不是Matlab或者python的理由。 首先，学习R是一个大的背景。R语言是一个开源的一个软件，它跟python、junior一样，基本绝大部分的基于R语言的工具都是开源的免费的，也说你基本上都能够（只要你的互联网是畅通的话）免费得到所有的内容。 第二，它是一个高级的语言，不需要和计算机的硬件直接进行交流，和我们日常的语言差不多。 第三，它有一个强大的community。我们在选择工具时，不仅要考虑工具本身的功能，还要考虑使用这个工具的社区和支持。因为一个好的社区可以提供大量的资源和经验，帮助我们更好地使用这个工具。因为现在的所有的开源的语言，它依赖于有多少人在使用它、有多少人在不断的进行开发，尤其是谁在开发这些新的东西。对于我们心理学或者是社会科学而言，绝大部分是使用R做数据分析。简单来讲，我们一开始作为新手肯定不会去开发什么工具的，就必须要把别人开发的工具拿过来。那谁为我们开发呢？肯定是这个community里的人，这就需要我们有一个比较成熟和强大的社区。而R语言本身就是由统计学家所开发的，所以它就是为了做数据分析而生的一门语言。同时，在这么多年的发展当中有大量的研究者，尤其是社会科学的研究者不断加入这个community，从初学者变成使用者最后变成开发者。 从做研究的角度来说，R可以在这三个方面提供强大的支持。 科学性 使用R有助于增强计算的可重复性。如果我们能够精确地重复我们的分析，并且得到相同的结果，那么我们的研究就更加可信。在讨论心理学研究的可重复性时，我们发现即使是有了公开的数据和代码，也很难保证研究的精确重复。一个研究发现，在14篇文章中，只有一篇能够完全精确地重复出来(https://doi.org/10.1177/09567976221140828)。这表明我们在数据分析的过程中，很多微小的步骤如果没有被完整记录下来，就很难保证研究的可重复性。为了解决这个问题，现在越来越多的人鼓励使用编程语言，如R语言，来记录数据分析的每一步。这样，我们可以从原始数据开始，记录下所有的数据处理步骤，从而确保研究的可重复性。例如，我与合作者2020年发表的一篇文章中(https://doi.org/10.1525/collabra.301)，我们公开了所有的数据和代码，并且有小组检查了这些数据和代码，发现能够得到与我们报告中大致相同的结果。 R会提供新的统计方法。IJzerman2018年的Collabra这篇文章(https://doi.org/10.1525/collabra.165)，我也是合作者之一，当时也通过互联网来合作收集的数据。在这个文章当中，他就使用了机器学习的方法，叫做（条件）随机森林，叫做conditional random forest。它实际上是在机器学习里面非常常见的一个方法。它的特点就是说即便只有比较少的数据，也能够得到比较稳健的结果。当然这个小数据是相对于机器学习里面的小的数据，因为机器学习里面可能动则就是上十万百万的数据。相比而言，我们的数据其实每一个都是很小的，就几百人上千人。所以当拿到这1,000多人的数据之后，他想去探索这么多变量之间到底哪些变量之间有一个比较稳定的关系，他就采用了随机森林的方法，最后也发现他感兴趣的那个变量，就是身体的温度和这个社交网络的复杂程度是有关系的。 R会提供更合适的方法。比方说我们实验室实验当中非常常用的反应时间，它基本上都是偏态的分布，对于这种偏态分布的数据到底应该采用什么样的一个模型，到底是用传统的线性模型还是应该用广义的线性模型。如果使用r，那我们可以很灵活的使用r里面比较新的一些回归模型的包。在这包里面我们可以使用最适合这个模型的，比方说GLM。我们甚至可以通过模型比较的方式找到哪一个模型是最适合的。也就是说正是因为在R有一个很强大的community，然后这里面有众多可以选择的r的工具包。这样我们就能够不仅仅是使用新的方法，它也可以帮助我们不断的去选出更加适合的方法。 美观 R语言提供了强大的绘图功能，便于调整细节，可以帮助我们在数据分析的每个阶段进行可视化。如ggplot2，它允许用户调整图表的每一个细节，以更直观、美观的方式展示数据。 我们后面会讲可视化的进阶，那一章的时候我们会把ggplot这个最常见的画图软件里面的每一个细节都掰开讲，这里我们只是稍微展示一下。我们可以把原始数据和group level数据结合到一起，然后再把每个被试的数据，把它的分布画出来。 最近几年非常流行的雨云图。当然我们还可以把多个（图）进行叠加，像这种被试类的实验设计我们可以把每个点都连到一起，可以看到在不同之间的一个变化，它是不是完全具有一致性的。然后我们把把这个box plot也加上去，这样的话我们能够看到极端点。我们同时还把这个分布加上去，当然这个分布目前的α值比较高，我们还可以把它调的低一点，就是说让他透明度再低一点，让我们看到这个分布之间的一个叠加。这样我们就可以在一个图上看到非常丰富的信息。 当然还有一个叫做ggridges的一个图，这个上面我们不仅仅看到可视化的效果，还可以直接把值标到上面。这样一个图给我们的信息量就非常大，当然在画图的时候我们不是单纯的追求这个信息量很大，我们要美观。要有足够的信息量同时也能够让大家不会一看到之后就不想看了，而是说看到之后能够立刻get到你想要传达一个什么样的想法，这个是很重要的。所以可视化这一点上面说实话我们即便在这个课上有两次课，但是我们只能教大家一些方法，大家最后画图的实际效果要依赖自己的taste，就是自己的一个口味和不断提升的感觉。 我们也可以画地图。我们可以把一些相关的数据在地图上进行映射，随着我们越来越多的能够得到不同地区的数据，把这些数据映射到地图上的时候，我们就会发现很有价值的信息。这个图是我最近画的一个图，就是我们在分析大团队科学中的样本被试到底是不是真的具有代表性(https://psyarxiv.com/avcsp/)。因为很多做这种就是跨国的研究的研究者总是会claim我们的这个研究从几十个国家来的，那么这个数据是能够推广到全人类的。是不是真的如此呢？我们看一下被试在我们这个图当中（的位置），我们就这边是中国的人口的一个（分布）那边就是他的那个被试的群体在不同省份的一个分布图，我们可以看到其实他选取的样本主要就集中在这两个地方，一个是广西一个是上海，其他地方的话其实数据量非常少。 当然我们还可以从其他的维度对他的样本代表性分析，这里主要是展示我们可以把数据映射到地图上面，这样一眼就看到他的数据到底行不行。现在把数据映射到地图上是越来越多的使用在心理学的理念当中。 实用性 R的实用性之一：适用于数据分析的各个阶段。首先，几乎科研每个阶段中涉及到的数据处理，均有对应的R包：计算样本量、读取数据、清理数据、处理缺失值、可视化、统计分析、生成PDF、甚至生成PPT。当然要掌握这里面的每一个流程的话其实是要花很多时间的，但是呢我们可以找到一个对心理学研究者来说最快的（方式）。比方说我们这个课上就会把整个过程中所有心理学的数据整个演示一遍。大家就可以照着这个流程去走，就不需要再去重新探索，这是我们这门课的意义。 此外，R语言还支持将数据处理和分析过程自动化，从而直接生成报告或演示文稿，甚至建个人网站。这意味着从原始数据到最终结果的展示，都可以通过R语言来完成。 R的实用性之二：适应数字化时代的需求。我们现在越来越多的大的数据，所以我们要使用一些更加fashion的一些方法，机器学习、深度学习什么的。那么r语言现在已经有很多这种框架了，如果我们能够掌握r的知识以后我们后面去拓展到这些部分相对来说是容易的。因为里面已经有一些比较成熟的框架。那么就像我们能够调用Tidyverse，我们同样也能够去调用这些机器学习的包，只不过我们要真正的合理的使用还是需要去了解了解背后的一些知识，不能盲目的使用R语言。 R的实用性之三：代码复用。一旦你编写了一段处理数据的代码，你可以轻松地在不同的数据集上重复使用这段代码，只需要对代码进行少量的修改。这大大提高了工作效率，特别是当你需要处理大量数据时。 R的实用性之四：强大的社区、众多的教程。我们选择R而不是Python的原因之一就是，更多的心理学研究者在使用R。我们有一个非常强大的社区。意味着有很多人教你做各种各样的事情，也就是说你如果你想做什么东西，99.9%的情况下你不需要自己去真的去从原理到到实现全部实现全部去做一遍。而是去搜索前人是怎么解决的。比如说你要就要做meta analysis你就搜索meta analysis视频，然后你能得到很多的教程，这时候你就去找一个好的教程就可以了。或者比方说我们要做混合线性模型，你就搜索一下肯定又会得到很多教程。 2.1.3.1 心理学的可重复性危机 在我们心理学领域从2011年开始出现了一个比较大的问题，就是可重复性的问题。大量发表的研究的结果无法被其他的研究者独立重复。那这个问题到底有多严重呢？最有代表性的应该就是这篇文章。在2015年的时候，一篇Science的文章专门报道了整个心理学领域的可重复性的问题。Science是跨领域的多学科的一个综合的期刊，能够发表到Science这个期刊的文章都是能够引起广泛的兴趣的，也是对整个科学界来说都很重要的。在这个文章中当中100个团队重复了2008年发表在心理学顶刊上的100个研究。他们的分析的发现大概只有36%的结果是能够被重复出来的。2015年这个结果是引起了非常大的震撼也被nature评价为2015年的年度的十大重要的论文之一。因为这个问题出现，研究者就做了很多的反思。当然我也是被这个问题所深深的震撼，现在还是在一直在寻求能够去做到更加严谨、可重复的透明的这种研究。这个是我在2016年的时候跟我们课题组的同学一起写的一篇对可重复性问题的一个介绍和思考，大家有兴趣的话可以看一下。 我这里没有去把这些心理学的计算上可重复的研究拿过来，有人对心理科学在science这个期刊上面有公开数据的文章进行了计算的可重复性的检验，也就是说按照研究者描述的方法去做一遍分析，看能不能得到跟研究者一模一样的结果。大家猜一下这个比例大概有多高。做一个区间吧30%以下、30%到50%、50%到80%、还是80%到百分之百。约为30%以下的举手，30%-50%呢，50%到80%呢。大家都很乐观啊，80%我就不问了吧。如果说我们考虑完全能够重复的话，他们在14篇文章里面只有1篇能够重复，是1篇还是2篇能够完全重复。然后有的是在作者的协助之下都得不到原来的结果，所以这个问题并没有那么简单。 2.1.3.2 利用R语言增强计算的可重复性 既然他是这么大的一个问题，那么为什么说r语言可以帮助我们解决计算的可重复性呢？首先是说可重复性它是有多个层面的。大家可以可以想一下，如果说你的这个结果是可以重复的，那么最简单的一个可重复是什么？就是计算的可输入性对吧。computational reproducibility，这个computation reproducibility说的是什么？假如你有一个数据，然后你做了一套分析，你把它报告出来了，我拿到你这个数据，我按照你描述的方法，我能不能得到跟你一模一样的结果。假如说你的计算的过程当中，没有一些随机的生成的过程，全部都是说我们用的这种可以求到解析解的这种这种算法的话，那就意味着我不仅仅要跟你的结论是一致的，而且是在数值上应该是一模一样的。你原来得到比方说t等于2.1，那我应该也得到就是t等于2.1，或者你得到的是f等于10，我应该也得到f值等于10，不然就说明什么计算上他是他是不可重复的。 2.1.3.2.1 记录数据分析的全过程 你们现在可能绝大部分是研一的同学，那么你们自己有做过数据分析吗？有的举手。做过数据分析吗？你们做毕业论文应该做过数据分析吧，都不举手，没有做过毕业论文？做个数据分析大家都是用的什么软件，SPSS？那你们都要通过手点击吧，你们现在还记得怎么点击的吗，一步一步的怎么点击过来的。 所以我们实际上按照传统的做数据分析的方法，都是用手动点击实现分析的对吧，尤其是前面那一部分。我们不是说你把数据录入到SPSS是以后的那部分，（说的是）比方说你用你在问卷上面，用问卷星收一批问卷，那里面可能有一些不太认真的吧，你要把它给删除掉对吧？有可能你会删除一些你认为是比较极端的也确实可能是极端的数据对吧？有可能你就是100个人里面或者是300个人里面你把某一两个（极端数据）你当时看到你就删除掉了，然后你最后认为你得到了一个干净的数据，你把它存起来以final或者是以最终数据作为后缀对吧，然后你就会基于那个数据把它打到SPSS里面对吧。但是如果说你要重复的话从前面到你那个最终数据你能够（重复出来吗？）可能一个月之后你就不一定记得为什么你删除某个数据了。那么这是很普遍的一个（原因）导致我们最后结果无法得到（重复结果）。如果我们用R语言编程语言来记录数据分析流程的话，就可以把我们整个数据分析的过程全部记录下来，也就说任何一个步骤出错了我们都可以找到，因为我们代码全部在那里。 我们这门课会从原始数据出发，从数据的预处理到后续的统计分析。我们会展示如何把每一个步骤都用代码记录下来，这样一个好处就是即便过了一两年之后，即便我们已完全忘记了当时是怎么处理的，但是代码还是可以告诉我们当时怎么做的，这一点就可以帮助我们去保证计算的可重复性。 2.1.3.2.2 跨机器的一致结果 另外一点，可以帮助我们达到跨系统或者是跨机器的结果。这个其实是在我们心理学的数据当中比如行为学的数据当中是不是很大的问题。为什么呢？因为我们行为数据的处理涉及到的步骤很少，即便里面包括一些随机化的过程，他的错误不会累积和放大。但是如果你们要去处理一些分析流程更长的一些数据，比方说像fMRI的数据，那么你在不同的机器之间的随机性或者浮点数据导致的这个差异，他就会随着你研究的步骤慢慢积累起来，也就是说即便你的这个系统刚开始的时候输了原始数据。经过了不同的系统不同的机器有不同的随机的非常微小的差异，经过一段时间之后也会累积成为很大的一个差异。我们后面会讲如何控制这种随机性导致的这个结果，如果我们使用比较好的使用包括像pandas？或其他的一些软件，我们实际上是能够达到某种程度上跨机器的一致性的。当然到了一个精度非常高的程度的话，其实就不是我们心理学家能够解决的问题，因为他涉及到一些计算机内部如何去控制浮点的精确度等一些技术细节的问题。但是我们可以怎么样呢？当我们学习了这些编程语言之后，我们能够去把计算机科学家在这方面做的改进纳入到我们的分析当中从而去改进我们自己的分析的结果。 那么在这个记录分析的全过程中我自己有一个例子。就是我们在2020年有一篇文章当中公开了数据。但是呢最后其实我们的数据跟统计的结果稍微有一点不一致，有一个读者他读了我们文章之后给我发邮件，我们后来是追溯到内部。之所以出问题就是我们用Excel来操作的，然后我们用Excel操作的时候不小心删除了几行。所以如果我们使用R做全部的数据分析的话，应该就不会出现这个问题，这是说R可以做全过程的一个记录。 那么另外一个例子就是我自己在2020年另外一篇文章当中，因为他的数据和代码全部是公开的，所以有一个叫做reproducible的团队，他们去对已经发表的这个文章的结果的可重复性进行评估。他们就对我的这个数据和代码尝试进行了一次重复。那么他们就是说行为的这个结果绝大部分是能够重复出来的，但是有一部分是没有重复出来，原因是因为他安不安装不了那个软件。这不是一个很小的问题啊，我后来花了两年的时间去专门把那个软件打了一个docker的包，我们后面会学到的这个docker就是为了去让我们能够保证跨机器的一致性。这个看起来是个很简单的问题，你发了一个文章，然后你做了某一个分析，结果别人连你的软件都装不上。那么我们如何去提高自己的研究结果的可重复性，那当然就是我们把软件让它变得更好安装。那个重复的它是用python写的，不是用r语言写的。他重复这个R语言部分呢，他的comments都是非常好的，就是说即便他没有在他自己的工作中没有使用R，但他也能够很好的读到我的（注释），他也能够知道我在做什么，非常详细。大家以后能够比较好的比较规范的写自己的r代码的话，那么同行的反应也是类似的，他会发现你的这个数据，第一个结构非常清晰，第二个你的代码非常清晰易懂，他很很快就知道你在干什么，他也能够很快的重复你的一个结果。至少这样的话，你就保证了自己的结果是非常的稳定的也非常靠谱的。 2.2 R语言使用的示例展示 我们已经讲完了第一部分关于为什么要学习的内容，希望大家在听完后仍然能够有学习的动力。接下来，我们将简单展示一些使用R语言的代表性情况。你现在看到的是其中最具代表性的情况之一，即遇到错误的情况。例如，在使用R语言时，实际上碰到错误的概率几乎是百分之百的，而即使你非常熟练，仍然可能出现很低级的错误，比如漏掉一个字符或者反引号。我们将在之后讲到数据类型时介绍不同数据类型需要对应的一些符号。 2.2.1 数据清洗 在数据清洗方面，我们一般会使用dplyr， 它是Data science里面非常常用的一个功能，需要进行各种数据转换、分组等等操作。但是数据清洗通常是数据分析中耗时最长的一个过程，即使是简单的数据也需要花费相对较长的时间进行清洗。虽然在使用SPSS的过程中，我们已经形成了一个非常快速的数据分析思维，但是在使用计算机语言进行数据分析时，这个过程完全不同，需要花费大量时间进行数据清洗。即使是行为学数据，甚至是简单的反应时数据，也需要进行相对较长时间的数据清洗。 这里面可能有几个图没有截过来，在数据科学中，无论你从事哪个领域，完成一个数据分析项目的时间通常会包括从数据清洗到最终分析以及报告撰写。其中，数据清洗通常会占用至少60%的时间，这个过程可能需要反复查看和修改。传统的做研究的方式并不习惯分享数据，因为整理和清洗数据需要很长时间。即使你的文章已经发表，清洗数据所花费的时间也可能会被认为是浪费的，尽管有时会带来间接的回报，如其他人可能会重新使用你的数据或发现你的分析的可重复性很高。 2.2.2 ggplot2画图 另一个耗时的任务是画图，尽管这些图看起来很漂亮，但需要不断地调整和修改。有时，研究者会花很长时间去完美地绘制图表，而这些时间可以用来完成其他任务。经常会有研究在这个社交媒体上，他自己比方说做统计分析发了一分钟，然后画图画了两天的时间，就是不断的去调，并不是他不会画图，而是他总是觉得画出来这个图不满意，然后就不断的调整，不断调整最后发现，时间就没了，并且呢你也会发现呢，就是当你掌握了不同的这个，画图的这个方法以后呢你会不断的去想，我能不能找到一个更合适的方法，去对他进行更好的一个可视化，我没有来得及把我自己之前的一个画的图的这个历程贴上去，我刚开始就是最简单的这个，跟我们在，Excel里面画的那个图是一样的，就是一个直方图上面加一个error bar，这是我一开始用来画的用APA的格式，后来就变成那个带散点的，再后来变成了那个raincloud，然后就是raincloud加box，然后加这个distribution，然后最后又回到了这个散点的，加上这个主题，就是groups这些。所以其实这个画图的时间需要大家当你熟练了以后也需要适可而止，差不多能够传达你的这个信息就可以停止了，要不然的话，这个画图的提高是没有止境的，有的也可能可以看有的时候也看到，就比方说一些好的期刊，比如像经济学人他们涉及到数据的时候，像比方说， 我们说Nature、Science或者PS对吧，nature Communications，当他们涉及到数据的图的时候，那都是非常漂亮的，包括他们配色包括的比例各方面，他实际上都是经过了，就是专业的人士进行调试的。那么有的时候其实我们要想要达到这个类似的效果的话，也需要花很长的时间去做这些细节的工作。但假如我们只是按照心理学传统的做法，Excel的那种一个bar图加一个error bar的那种非常快基本上几行代码就可以搞定。 2.2.3 心理学数据分析与结果汇报 针对于各种各样学科、心理学的这个数据的分析的包，也有心理学的研究者开发的包，例如蔡华杰老师的PLUS包，该包专门针对心理学数据分析，并包含很多有用的功能，非常适合心理学学生使用。例如，对于做T检验的研究，可以使用该包的功能，我们可以用他的这个T-test对吧,它可以将结果输出成为一个简单的三线表格，并且可以直接在命令中加入指令将结果输出成为一个Word。这个word文档里面就有这么一个表。你就可以直接复制打开到你的文档里面，这个是非简单的。他也告诉你，你的零假设是什么，我们这个是单样本的t检验，那么他告诉你这个假设就是双侧的，然后这个叫做均值，他不等于700，那么类似的可能我们还有其他的，比方说我们用这种配对样本t检验对吧，他同样的也可以得到这种非常适合我们输出的这个结果，而且他最近也把这个face back进去了，所以我们大家可以看到这里报告的信息肯定是比spss更加全面的。 2.2.4 Regression 那么另外就是关于这个regression这里面我们其实没有使用BruceR，如果我们使用BruceR的话，他有一些专门的回归模型，然后我们这里有个简单的回归模型，把回归模型直接做一个这样的输出，Bruce也是可以的。还有这种，这里应该也是一个简单的回归模型，还有就是我们也可以使用这个SEM，这个地方的话，我们可能要使用一个新的包，就不是bruceR，当然bruceR他也把那个，如果没有记错的话，把SPSS那个process整合进去了，这里我还没有尝试过，可能我们后面可以一起来探索。 2.3 现场运行代码 在今天的课程中，我们向大家展示了一段R语言代码。我之前在公众号或群组中提到过，所有的课件都会放在github同一个地方，供大家查看。比如，我最近在一个小时前更新的内容，包括第一节课的PPT。我通常会在上课前更新课件，以确保大家能够获取最新的信息。 去年的课程PPT，也就是2023年的课堂PPT，已经不再使用。虽然这些代码和PPT可能仍然有用，但与我们的课程配套的内容是我们当前每次课的课件，大家可以自己下载最新的课件。我刚才展示了一个简单的rmd，让大家看到如何进行简单的展示。如果你已经下载了，可以直接打开并查看。 我使用的电脑是我的助教提供的。在课程开始前，很多人还没有安装课件和代码，但我通过一个工具，将代码和课件下载到另一台电脑上，并成功打开。通过这个工具，我们可以看到各种信息，包括代码和文字信息。这些信息和代码可以混合编写，最终可以生成一个可以直接在电脑上运行的演示文稿。 在运行中，可能会遇到一些问题，比如某些包未安装。需要注意的是，R中的很多包看似独立，但实际上是基于其他包构建的。这意味着你需要安装所有依赖的包才能正常使用。特定的R包甚至可以帮助我们按照特定的排版格式生成符合学术标准的论文排版，符合一些学术杂志的要求。这个工具可以处理很多细节，比如作者信息、摘要、关键词、材料和方法等部分。你可以编写所有的细节，包括实验设计和数据处理过程。通过这种方式，我们可以确保我们的文档既符合学术标准，又能以一种清晰、易于理解的方式展示我们的研究内容和数据分析过程。 2.4 课程安排 我们这学期的课程的有三个原则：第一个就是即学即用，我希望我们在课堂上教的这些代码，大家都能够在自己的数据分析中使用而不是我们在这里学了一遍之后，自己还需要重新去学，或者这里面学的代码，都是大家用不上的，这个我们尽量避免。因为大家时间都很有限，如果能够帮助大家省一点时间的话我觉得或者少走一点弯路的话，减轻大家的心理负担。 第二个就是在做的过程中学习，我们会教大家怎么去安装，然后去了解这个里面的各种各样的功能，以及数据导入，以及各种各样的情况，基本上第二节课以后，我们就开始就直接就给大家讲这个代码。我们需要在做的过程中学习。 然后第三个就是逆向学习，逆向学习就是你先做，你先能够在哪里面实现这个东西，然后我在这个我演示的过程中，我用一个命令，比方说就是t test或者f test对吧，我能够得到这样的结果，大家首先说的就是你在你自己电脑上面，或者说你在这个云计算平台上面你能够实现这个功能，你能把这个代码抄下来，然后得到跟我一模一样的结果。然后你后面再去理解它，你先会做然后再去理解，这个是对于学习代码来说，我觉得是很好的一个做法。因为如果你看到一两本书之后，却没有写过几行代码，这个实际上是非常的浪费时间的一个事情，尤其现在大家的这个时间也比较紧张，我们的一个目标就是想要去压平这个学习曲线。因为如果大家知道学习，就是说以前大家认为这个R语言的学习曲线是非常陡峭的，就刚开始特别难，你要进步的话很慢，就是很难，也花很多时间。我们希望他尽量的快一点，然后可以慢慢的去后面的不断的学习。 那么参考教程的话，英文版的有这个（Naverro, Learning statistics with R: A tutorial for psychology students and other beginners. (Version0.6.1),https://learningstatisticswithr-bookdown.netlify.app），然后中文版的话，有一个叫做王敏杰老师的《数据科学中的r语言》，他实际上是一个公开的一个教程，那么大家可以把它当做参考书，因为这个书里面讲到了非常多的一些知识点。但是我们不会按照他的这个知识点进行讲解。然后另外一个叫做Tidyverse，张敬信老师的书(https://www.epubit.com/bookDetails?id=UB7db2c0db9f537)，这个我最近也买了。那他实际上跟我们的课堂的这个内容契合度是非常高的，因为Tidyverse就是我们最常使用的一个工具包，那么课程的安排的话我们就是，我们课程的内容的话其实就基本上跟我们的研究密切相关，因为大家都是研究生对吧，所以我们是希望：首先是跟数据分析这部分密切相关，然后等我把这部分解决之后，我们看到后面进阶的话，在设计实验的时候怎么把这个部分做好，那么在数据分析的过程中的话，我们就会有一个完整的流程：我们从原始数据到清理，然后到数据的探索，统一分析，然后分析完了之后统一推断，然后把它结果去验证，然后撰写一个报告。 我自己本人也非常喜欢可重复性和开放科学，因为我觉得它是科研中很重要的一个方面，我们后面也整合了一些有关我们如何跟他人进行协作的内容，帮助他人一起共同的合作来完成任务。还有如何保证我们的可重复性，如何采用一些更加先进的计算机技术，来帮助我们更好的保证我们的这个计算的可重复性。然后以及如何直接能够从代码到数据，生成一个PDF文件或者word文档，能够生成一个直接可以提交的一个版本。 2.4.1 课程大纲 我们不会按照传统的这种方法介绍R：先介绍R里面有什么数据，有什么对象，有什么语法规则，这些通通都不介绍。就是直接数据拿过来，我们要怎么用，怎么分析，第一步做什么。我们从第二章开始，就是安装，假如我们现在有了一个数据，我们需要用r语言，我们第一个问题就是把r安装到自己的电脑上面去，如果使用的是windows系统的话，最好把自己的用户名改成英文或者是拼音，不要用中文作为用户名，因为R语言它是英语的使用者开发的，所以说他的这个编码可能会对中文不是很友好。我们之前碰到过一个问题，就是当我们使用中文作用户名的时候，可能没有办法画图。 然后我们会后面会帮助大家解决一些安装中的问题，我们会介绍安装之后的各个界面，我们也会介绍如何更加方便的使用r，如果我们是使用原生态的r的话会非常的难用，相当于只是给我们提供了一个引擎就是做R计算的一个引擎。我们还要需要有一个写代码的一个界面，更加方便我们进行交互，那么我们会使用Rstudio，那目前的话，Rstudio应该是使用最广泛的，当然越来越多的人也使用Microsoft的那个VS code，但是我们使用R语言，其实也是挺方便的。 然后我们介绍各个界面以及如何开始我们的数据分析。我们现在拿到一个数据之后，首先要把这个数据导入进去，那么在SPSS里面大家知道，就是一个File导入，那么在里面呢我们除了这种方式以外，我们有其他的方式，我们也会进行批量的导入，我们不仅要导入一个数据，我们可能需要导入多个数据，导入完数据之后呢我们就有东西了。 我们的数据进入到里面了，那么我们从现在就开始认识R里面的这些数据，就是R里面的这个对象，然后我们会以这个为基础来讲解R里面的各个对象有什么特点。 首先我们要讲解数据和处理，因为这个过程有点复杂，所以我们会分成两次来讲解。我们会首先讲解单个对象在R中如何操作，一个一个的我们在Rstuido中都是可以看到的，然后讲解一些运算规则。这时候，我们就把R的一些基础知识放在这里了。然后我们讲解每个对象的特点之后，会展开介绍一些函数和规则等等。然后我们就可以开始自己的处理工作了。我们会按照tidyverse的风格进行基本的操作，或者我们叫做比较管道的操作。在这个过程中，我们可能需要给大家演示一下。如果大家能理解了，我们就可以进入下一个阶段。但是，如果有些同学对编程没有基础的话，第四章和第五章可能会比较难理解。所以我们会多停留一段时间，确保大家能够理解。 然后我们已经导入了数据，接下来就可以进行一些探索。看一下我们数据长什么样，他有什么样的一个模式，特点，我们应该对他进行什么样的一些分析，这就是这就是去了解我们的数据。在传统的心理统计学和SPSS中，不知道老师有没有要求大家查看原始数据。实际上在我们用做数据分析的时候，我们一定要去看，并且是在开始的时多的去看原始数据他到底是什么样子。我们不能只看统计指标，我们一定要看数据他到底是什么样子，这样的话就避免让我们发生一些非常非常基本的错误。 那么这个数据探索的话其实他包括两个部分。一个部分就是进行描述。第二个部分就是进行一些基础的一些可视化，所以探索数据部分其实已经包含了我们两个部分的一个知识点：一个是描述统计，另一部分就是最粗糙的可视化。因为这个时候，我们只需要自己看就行了，我们不需要给别人看，我们也不需要去把它做的非常精美，所以这个时候是最基础的这个可视化，然后的话我们就会接下来几章我们就会告诉大家如何用r语言实现大家常用的一些统计分析，因为这个可能是我们心理学常用的。 所以我们先展示一下，然后他的这个分析的流程是什么样的。这几个部分呢，其实应该是可以并行展开的，但是我们没法进行并行展开，所以我们只能依次的介绍。 然后呢，介绍完了之后呢我们会介绍一个目前来说在国际心理学界比较流行的一个做法，或者说大家在推荐的一个做法，就是看我们的这个结果是不是稳健的，那么这个时候比如说我们同样一批数据，我们采用多个方法来分析它，最后我们得到的结论是不是一致的，这个称之为Multivese。在心理科学进展上最近有一个文章，介绍这个Multivese的。大家有兴趣话可以去看一下。 那么到这个时候的话，其实我们基本上统计分析就基本上，如果我们只做传统的这个数据分析的话，我们就其实就已经做完了。那么我们接下来就是，我们汇报结果，这个时候我们如何得到一个可发表的图像，或者可发表的是一个插图，那么这个时候我们会进一步的讲我们如何进行拼图，如何操作每一个每一个这个元素然后，如何把多个图拼到一起，让他更加的美观，以及他的比例等等。然后呢我们会讲一下，这个叫做文学编程literature program就是，实际上他就是把我们前面讲到——就是在这一章以前，我们讲代码，所有的都是直接代码，那么我们在这个地方的话我们就开始介绍如何把r代码和文字进行混合，也就是说我们同一个文档里面既有文字描述也有代码，这个代码还是可以运行的对吧，他生成的图片还可以直接插到这个文档里面。 我们就要介绍这个Rmarkdown,它是用LaTex的语法,所以我们会介绍一些最基本的LaTex的语法，帮助大家进行排版，还有公式的撰写等等，然后到这里的话基本上就是我们主流的部分，一个人干活的话基本上就差不多了。但是我们知道，现在的已经都不是一个人干活对吧？所以我们要经常跟他人进行协作，所以我们后面介绍就是如何跟合作者或者导师进行协作那么这个地方就涉及到两个，一个是版本控制——其实一个人干活的话你版本控制也很重要，为什么呢？因为你可能前后代码有很多迭代对吧，你有的时候可能删除一个功能，但是你后来发现，这个删除功能它是有用的，但是如果你直接删除的话，你就找不到了，那么如何我们能够找到以前的版本，这个其实是，当大家写代码越来越多的时候很重要的，另外一个就是多人协作对吧，通过这个Github来进行多个人进行同时，完成一个数据分析，你完成这个t test，我完成f test，最后我们两个人，进行一起合并，这样的话我们能够更加快速，更加有效的进行协调。 然后，我们相当于是一个研究做完了，可以计划下一个研究，这里我们会介绍一些心理学常用的方法，包括meta-analysis元分析。元分析实际上就是我们把多个研究的这个效应量进行综合，综合起来之后我们就有一个用来估计样本量的一个东西，那么我们接下来就是做这个power analysis。以及我们如何在没有任何数据的情况之下，我们就可以把自己的分析数据的代码先写出来，那么这就涉及到这个假的数据对吧，fake data或者叫做simulator data。 如果我们涉及到这个数据非常复杂的话，我们可能会考虑如何进行变形处理。就是说我们的数据量比较大的话，如果我们做模拟的话有可能他要花很长的时间。但是我们的CPU有多个核对吧，我们是不是能够把多个核都用上来，让他更加快速有效的进行这个模拟，当然这个可能是我们看情况啊，如果说有时间的话，最后大家自己去搜索就可以了，因为对于传统的行为学的这个研究来说，除非我们用贝叶斯的这个混合线性模型，要不然的话基本上都是在可预期的时间内能够完成的。然后最后的话，我们基本上完成了整个分析。我们全部做完了对吧，然后可能，假如我毕业了，那么师弟师妹他们如何来重复我的研究？或者导师如何能够重复我的研究？那我可能过了一段时间之后，这个版本有不断的更新的，那么别人如何能够还能够重复我的研究？这个时候我们如何把这个版本，这个包给记录下来发布？现在比较主流的这种计算机的方法就是容器技术。用这个帮助我们来更好的达到这个computational reproduce ability，当然这个地方我可能只会做点介绍，大家如果真的要去完成这一点的话，可能也需要花点时间去琢磨。因为R语言他有的一些包可能会涉及到比较系统里面比较底层的软件的交互，所以呢他可能——就是说依赖于你使用的什么包，如果你使用的包是比较主流的，比较常规的，那可能，你把它打到docker里面是很容易的，如果你涉及到的包可能是比较新的，有可能会他有很复杂的一个底层的一个编译的话，那你有可能会就没有那么容易打包。 所以如果我们再回过头看一下，就是说我们整个教学大纲就是按照我们一个研究生，拿到数据之后会做什么一步一步的往下走，所以我们不会说很系统的去介绍R语言里的知识，我们就是碰到了什么我们就讲什么，那么我们会用什么数据呢？我们的数据就是我之前的公开数据。这里面包括了2019年在scientific data发表了一个数据，那么它的数据呢以问卷为主，那么还有原研究，我们后面会把这个发给大家。另外一个是实验数据，也是我2020年发表的一篇文章，我们就会从这些数据开始，然后一步一步的进行处理。 那么当然如果大家说OK，我想就在这个学习的过程中，顺便把我的数据给处理了，那我就把我自己的数据拿过来可不可以呢？也是可以的，你可以尝试，然后你碰到什么错误。如果说需要问的话呢也可以问，但是不能太超纲了。 我们这里面大家可以注意到一点，就是我们对统计方法的讲解没有涉及到很深，因为我们是R语言课对吧，所以是以R语言本身的操作为主，就是比方说大家这个里面设计的很复杂，我有一个链式中介模型要试试，或者什么调节模型。如果说你碰到那种技术性的问题，我们可能不会回答因为这个完全超纲了，但你说我碰到了一个数据导入的问题，这个没有问题，我们可以解答。因为现在是说一个是知识一个是操作，我们教的主要是操作，因为实际上如果你懂了，这种比较复杂的SEM的知识的话呢，你要操作起来很简单，可能就一两行代码就能够解决了或者复杂一点就是十几行代码就能够解决了，但是，如何设置这个模型，如何解读模型的输出，这个不属于我们R课的内容，而是属于你的统计知识的内容，大家清楚了吗？所以我们这个课，你可以把自己数据拿过来，但是如果你的数据非常specific分析的话，我们可能不在我们这个课程所覆盖的范围之内，但是可以帮助大家去把前面这个部分解决。你可能原来完全不知道怎么使用R，那么我们现在就教你怎么把这个数据导入怎么使用，做基本的数据分析。 那么到后面，你可能很容易就找到适合你的这个数据分析的工具包，那么你需要去阅读这个工具包相关的知识点，去把它用于自己的数据。有时候我们会以这个原始数据——两个原始数据，一个是问卷数据，一个是反应时的数据为基础，然后一步一步的去走完整的过程，那么中间大家可能会有一些要抄写代码的地方，这个是就是大家可以在课堂上实现。那么我们可能会需要，为了保证大家能够不断的进行这个讨论和反馈，我们需要分组。那我们可以加到群里面，然后后面需要进行分组，大家小组长最好能够负责带一些插线板之类的，这样的话就保证每个人都能够有电，在上课的时候有电来进行后续的操作。 我的想法就是最开始的时候我们就分组，那么大家可以比方说根据自己的兴趣，或者是我没有兴趣的话，我们可以由助教来进行随机的分组，那么分组之后呢大家比方说碰到了问题大家相互讨论，我不知道大家以前在做数据分析的时候是不是相互讨论，但是在写代码的时候，相互讨论是一个非常有用的一个东西，为什么呢？可能你写了一段代码之后你犯了一个非常小的错误，然后导致你的代码没办法运行，你自己看不出来为什么，因为你太熟悉了这个代码，但是别人来看的话可能一眼就看出来了。 所以呢大家可能就可以形成小组，在上课的时候练习的阶段，大家就可以相互来进行这个相互进行检查，同时呢我们也有几位助教，我们分组讨论的时候呢，助教就会跟我们一起来解决。 我们会后面会有一个我们课程的PPT，完成之后会把它上传到这个Github，为什么我有这么多助教呢？有一部分助教是会把我们上课的录音进行一个，就讲的这个东西进行一个文字的整理，然后让他更加的符合逻辑，把它变成文字稿。 有些同学或者老师，特别想学，但是好像没有时间过来，你觉得参加小组讨论也太麻烦了，后面就可以看这个文字稿。或者这个书也可以电子书也也是可以的。这大概就是我们这个课程的安排。 这个课程的整个大纲啊，大家跟我就一起体验。如果大家感觉很很糟糕的话，也可以继续跟我反馈，我后面在不断的改进。我们每年的结构都是有调整的。 我们每一节课希望能够解决一个问题，那么在这个解决问题中的话，有的时候我PPT讲的内容不一定能够完全解决你的问题，因为你有可能在你的电脑上去解决这个问题的过程中你碰到了新的问题，那么这个时候我们有很重要就是小组讨论。我和助教来一起帮助大家解答疑惑，这个基本上就是我们这个课的安排。 2.4.2 成绩分配 那么选课的同学，可能就会涉及到这个成绩的问题了。首先是出勤，只要大家来的话，就有10%的分数；第二个的话就是3个小作业，是单人任务，占比15%*3；第三个是一次大作业，分组完成，占比45%。大作业要求学生找到一篇顶级期刊上发表的文章，该文章应公开数据，最好是原始数据，然后按照文章中描述的方法使用R语言进行数据分析，并制作一个报告。对于线上学习的学生，如果对大作业感兴趣，也可以尝试。我们会通过在线平台进行评估和反馈。 在课程中，我们还会教授如何使用云平台（和鲸）来提交作业。这样，我们就可以直接从平台上运行学生的作业，避免了下载和路径问题。对于线上和线下学生来说，使用云平台可以更方便地提交和评估作业。 2.5 如何学好这门课 最关键的就是不要害怕这个课程，这课程其实没有大家想象的那么难，为什么呢？因为我们不需要成为r开发者，我们也不需要懂很多很多r里面代码，我们只需要成为一个合格的调包侠就可以了。别人开发的包我们能够合理的使用，这就是我们这门课的目的，大家如果说有志于要成为大神，比方说我要开发某一个包，或者是我要后面所有的东西都用R解决，包括给自己建一个网站什么的，这个东西不在我们的这个课程范围之内，也说大家能够用r语言，第一个就是消除r语言的畏难心理。能够用r的这个生态里面的一些包帮助大家解决问题，这就是我们这门课要达到的一个，让大家入门的一个目标那么当然我们也希望通过这个入门让更多的人，能够成为长期的这个R语言的使用者，能够长期的在这个community里面活跃。甚至有一天能够为他人答疑解惑，或者是明年的时候来给我当助教。 第二个就是敢于尝试，一个就是说大家一定要去不断的犯错，另外一个就是说你可以借助一些比较新的一些东西来去帮助你解决这个问题，我们有些助教就是,非常熟练的使用ChatGPT。假如你没办法使用ChatGpt，是不是还可以使用Bing的相关功能？它也非常的强大。我们最好是用英文进行搜索，因为相对而言，英文的这个社区要比中文的社区要强大很多，把你的问题用英文描述出来，然后在Bing里面搜索，99.9%的这个情况之下你都能够找到答案，如果你的问题描述是正确的话。你也可以很简单就把那个报错最关键的地方放到这个搜索框里面，基本上你也能够找到答案。R语言他再怎么统计，他也本质上也是一门计算机的编程语所以他还是有编程的这些成分在里面，那么编程他最大的一个特点就是你要需要去勇敢的尝试，不断的犯错，犯的错越多的话，学习的越多，尤其在课上。假如说你犯了200个错误300个错误，那么你后面自己分析数据的时候可能还是会碰到这些错误，但你就知道怎么解决了。所以只有多犯错才能多学习。 我们需要以计算机的这种思维方式思考，就是计算机他是非常非常机械的，你告诉他什么，你输入什么指令，他就给你什么结果。所以如果出了错误，一定是我们的指令或者是哪个地方出错了，所以很多时候你需要把这个逻辑想清楚。特别是你有一个比较复杂的分析问题，你要想我第一步做什么，第二步做什么，第一步的这个输入变量是什么，输出变量是什么，这个输出的变量，他能不能进入到下一步作为输入。简单的这种机械的操作的思考，能够帮助我们去使用这个R语言。 第三个当然就是我们在小组的讨论中一定要相互的帮助，我有一些朋友，他是学习计算机本科的，他们的快乐之一就是上课的时候，相互debug，相互帮助，因为写代码写多了之后你会发现他是一个非常有及时反馈的一个事情，你输入一行代码，他给你一个正确反应非常开心，输一行代码错了，能够解决了也非常开心。但有一种情况对于初学者来说，就是你犯了一个错误，结果你两三天解决不了，就非常的头疼。那这种情况的话，如果是有人能够帮你的话呢，就能够其实极大的促进正反馈。还有一个就是，遇到比较复杂的代码的话他确实有可能起作用了，但是你可能也不知道为什么，他不起作用你也不知道为什么，但是我们需要尽量减少这种情况。 在学习R语言的过程中，我们鼓励大家即学即用，将学到的知识和技能应用到自己的研究中。这不仅有助于巩固学习成果，还能够提高数据分析的效率和准确性。对于那些刚开始接触R语言的同学，可能会觉得很难。但是，通过不断的练习和使用，你会逐渐熟悉R语言的语法和结构，从而能够更加高效地处理数据。 此外，我们鼓励大家在使用R语言时，不仅要学会如何使用各种函数和包，还要理解这些函数和包背后的原理。这样，你不仅能够解决问题，还能够更好地理解数据分析的过程和方法。 2.6 课程总结与期望 总的来说，R语言是一个强大的工具，它可以帮助我们更有效地进行数据处理和分析。通过学习和使用R语言，我们可以提高自己的数据分析能力，更好地服务于我们的研究和工作。 学习R语言并不是一件可怕的事情。每个人都能学会，只要你有足够的练习和尝试。在学习过程中，你可能会遇到一些挑战，比如数据预处理和统计思维的培养，但这些都可以通过持续的训练和实践来克服。 我们这门课的最主要意义在于让初学者从完全不会到能够不害怕使用R。在心理学研究当中R语言也是慢慢地变得越来越流行，一些比较新的期刊会发表很多教程性的文章，就是专门教大家如何去使用各种各样的新的方法。其中有多篇关于R语言的使用的。 数据分析是一个需要长期训练和实践的过程。我们需要学会用计算机的方式去思考问题，这需要我们有很强的逻辑思维能力。同时，我们也需要相互帮助和支持，共同进步。 2.7 推荐 公众号：统计之都，它提供了很多关于统计学的科普内容和思维方式，这对于数据分析的学习非常有帮助。 书籍：statistical rethinking "],["lesson-2.html", "3 第二讲：如何开始使用R 3.1 要解决的数据分析问题简介 3.2 如何安装 3.3 如何方便使用？Rstudio的安装与界面介绍", " 3 第二讲：如何开始使用R 前言 数据分析的出发点是解决问题。也就是说，我们数据分析的过程都应该是问题导向的。因此，数据分析中的一个关键在于明确地知道自己要解决什么问题，要有强烈的问题意识，这对科研工作者来说尤为重要。无论是学习R语言或其他计算机语言，还是进行一系列的数据分析，其目的都是为了回答一个特定的问题。问题本身的重要性最终决定了数据分析的价值。 问题可以是科研中的理论问题，也可以是现实生活中的实践问题。例如，交通分流、道路设计、产品质量等的分析。实践问题也可以是收集证据来辅助决策。比如，在新冠疫情期间，行为科学(behavioral science)在公共决策中的作用受到了重视，通过大量的行为数据，可以帮助政府或决策者进行更好地决策。因此，数据分析的出发点是解决问题，这也是学习数据分析中首先需要明确的。让数据分析变得严谨和可重复，是为了更好地解决问题。我们必须时刻牢记这个最根本的出发点。 3.1 要解决的数据分析问题简介 3.1.1 第一个数据: 人类企鹅计划数据 为了帮助大家更好地学习如何使用R语言解决问题，我们提供了两个示例问题。这两个问题是我本人在先前的研究中遇到的，因此有心理学背景的读者可能会比较熟悉。 第一个问题是有关人类社会关系和体温调节之间的关系。我在读博期间参与了Hans IJzerman博士的这个项目。他关心的问题是人类的体温和社会关系之间的关系。从演化的角度讲，哺乳动物的生存需要苛刻的环境：身体的核心温度必须要维持在很窄的范围之内，哺乳动物才能够生存。作为哺乳动物，人类要在相对恒温的条件下才能生存。生物学的研究发现，哺乳动物会形成一个社群，通过这一社群来帮助其中的每个个体调节体温。由此，Hans对”人类身上是否也存在这种机制？“这一问题产生了兴趣，人们的社会网络本身，是否能够帮助我们去调节体温呢？Hans提出了自己的理论假设(IJzerman &amp; Hogerzeil, 2018)，并试图验证社交网络是否能够调节体温，于是进行了一项大规模的跨国实验，收集了来自十几个国家的数据。 Hans首先进行了一个预实验，随后收集了横跨12个国家的数据，包含了1,500多个人的信息，其中包括怀旧程度、对家的依恋、主观压力、新陈代谢和社交网络质量等多个问卷。这个数据集最初是通过Qualtrics在线收集，现在已经公开可以使用(Hu et al., 2019, sci data)。 请您设想自己是Hans课题组的成员，置身其中地思考如何从数据导入开始一步一步进行后续的数据分析。我们可能最终想要解决的问题是如何预测核心体温，其中一个重要的分析方法是有监督的机器学习(条件随机森林, conditional random forest)，我们将寻找能够预测核心体温的变量。在下图中，红色线以上的变量对预测核心体温有很强的相关性，反之则预测效果较弱。换句话说，虽然我们测量了很多变量，但通过机器学习算法，可以发现只有部分变量与核心体温有较强的相关：complex social integration (CSI)，CSI是社交网络的一个指标。在Hans的研究中，也使用了一些传统的问卷分析方法，比如调节分析。他发现是否处于亲密关系对CSI与体温之间的关系是存在调节作用的。 要进行整套分析的研究，首先需要将原始数据输入到R语言中，然后进行数据清理和描述性统计，例如数据质量、问卷信度、均值、标准差等统计指标。此外，也可以进行探索性的数据分析，例如相关矩阵，来探索变量之间的关系。最后，我们需要呈现研究结果，包括condition random forest的结果和mediation的结果，并将其整合到可视化报告中。原研究实际上并没有将数据分析和结果报告整合到一个完整的RMD文件中，而是采用传统的方式来准备手稿。在我们的课程中，将使用Rmarkdown来完成整个数据分析，并生成PDF文稿。 我们所提供的是一个大致的数据分析流程，其中还存在一些细节值得更多关注，例如如何去假定变量之间的关系。我们所进行的是别人研究的复现，通过复现去了解使用R语言进行数据分析的流程，以提高自己的技术。 3.1.2 第二个数据: 知觉匹配任务数据 这个实验是一项简单的认知实验。在认知心理学中，我们通常会在实验室中进行，让被试进行一些简单的按键反应。这个任务可以分为几个阶段：首先是学习阶段，在这一阶段我们会呈现几何图形（正方形、三角形和圆形）和三个人物标签（好人、普通人和坏人)，然后让被试在图形与人物标签之间建立联系，例如将三角形和好人联系起来。随后，我们会在电脑屏幕上呈现一个图形和一个标签，被试需要判断屏幕上呈现的图形和人物标签是否匹配。这是一个非常简单的任务，被试练习了二三十次后，就能够熟练地完成这个任务。 接下来，我们会让被试完成两个任务：匹配任务和分类任务。匹配任务需要被试做出一个决策，即图形和标签是否匹配，它的反应窗口非常短，大约在800到1100毫秒之间。分类任务需要被试判断图形是好人还是坏人，然后进行相应的按键操作，此任务也需要在非常快速的时间内进行反应。 我们的实验设计是一个2*2的被试内实验设计，自变量包括人物标签（自我 vs 他人）和效价（valence）。 在分类任务中，采用两种分类的标准：自我vs他人，好人vs坏人。在学习过程中，记住了四个人物标签和四个几何图形之间的配对关系，并训练匹配任务和分类任务。分类任务包括按身份和效价进行分类。 在研究中，我们想要了解即时学习到的社会意义是否会影响对几何图形的反应。我们发现，给几何图形打上社会标签后，会影响到反应时间。虽然总体上来说，好的自我反应最快，好的他人的反应也不错，但是坏的自我和他人都比较慢。这是一个总体的趋势，但并不是每个几何图形都是这样的。我们使用这种方式来绘制图形，既能看到总体水平上的结果，也能看到个体差异。这可以避免我们的过度推断。 上图右边是d prime，是信号检测论中的一个指标。在心理物理学中，信号检测论是一种常用的数据分析方法，用于认知心理学研究。它可以计算d prime，即敏感性，这比准确率更能判断信号是否敏感。在匹配的任务中，我们将匹配条件视为信号，不匹配条件视为噪音，以此来计算信号检测论。对于非匹配任务，如分类任务，我们也可以计算类似的标准和数据。我们还使用了一个名为Drift Diffusion Model的计算模型来分析数据，它是用Python工具包HDDM完成的，但也可以在R中使用。在这个数据中，导入数据可能有些复杂，因为每个被试的数据都有三个文件，需要进行合并和清理数据的预处理；在数据分析部分，我们进行数据清理和可视化。 如果使用传统的方式，我们需要使用Excel进行数据预处理，使用SPSS进行统计分析，使用Excel或PS进行图形绘制和美化，最后使用Word文档进行写作。如果使用R语言，我们可以在R语言中完成所有工作，从Tidyverse开始进行数据清洗和工具分析，使用GGPlot2进行图形绘制。我们将使用Markdown或papaja来输出。在BruceR中，我们特别关注T-test、方差分析和多重比较等内容。由于BruceR对心理学数据分析进行优化，因此非常方便。正如我们之前提到的，使用R进行分析可以保留所有分析过程，并且可以通过代码直接重复分析。而分析采用的代码和方法也非常灵活，新的方法也容易共享。 3.2 如何安装 首先是安装过程，可以在必应(bing.com)中搜索R语言官方网站，然后在该网站中下载安装程序（点击这里跳转到清华镜像下载）。该程序适用于不同的操作系统和版本。对于Windows系统而言，安装过程比较简单。但对于Mac系统，安装过程可能会稍微复杂一些。Mac系统可能会有两个版本，分别为arm64版本（苹果M系列芯片）和 x86_64版本（英特尔芯片），可根据自身情况下载。 使用中文语言安装R语言会更加方便，可以避免编码问题。下载完成后，运行安装程序。在安装过程中，你可以选择是否自定义一些安装选项，但是默认选项通常已经足够了。但为避免以后的使用中出现问题，请确保选择英文路径作为安装目录。安装完成之后，你可以开始实际学习操作。在此我也想提醒大家要把安装目录放在英文文件夹里，避免中文路径可能会出现的编码问题；对于 Windows 用户来说，应当尽量将电脑的用户名也改为英文，因为有时在绘图时会调用一个存在于用户名下的某个文件夹，一旦无法识别中文，那么就无法对该文件夹进行识别进而导致出错。 在上图你可以看到，此处默认选择了中文作为系统显示语言，主要是由于系统语言为中文，安装时自动采用中文显示。安装时可能看到了一些警告(warnings)，猜测是语言设置引起的，在R语言中也会遇到这种情况。这里有两种类型的警告：一种是称为”警告（warnings）“，另一种是”错误（errors）“。对于那些被警告的代码，我们需要仔细检查，看看是否会对我们的运行造成严重的影响。如果没有明显的影响，那么就可以忽略。 在读取一些文件的时候，我们可能会遇到一些问题。例如，遇到一些非UTF-8编码的语言编码。这意味着我们不是使用Unicode编码方式。为了解决这些问题，我们通常使用UTF-8编码方式，这是中文中非常常见的编码方式。在大部分计算机系统中，我们都可以使用UTF-8这一国际通用的编码方式。 3.3 如何方便使用？Rstudio的安装与界面介绍 大家都已经成功安装了R语言，现在看到的是console控制台界面。早期的R语言使用这个控制台进行输入和操作。比如，我们可以输入一个简单的命令，例如： a &lt;- rnorm(100) plot(a) 第一行命令即随机生成 100 个服从正态分布的数据（rnorm，即random generation for the normal distribution的缩写），并贴个“标签”叫 a，然后绘制散点图。在控制台中，我们可以使用代码与计算机进行交互，即这个窗口会对所输入的内容做出即时的反馈，当然也包括对代码中错误的反馈。 对于没有编程经验的同学来说，可能会对控制台感到非常陌生：为什么有些输入会被计算机执行，但是有些不会？还有，为什么有些输入会产生图形效果，有些则不会？这正是我们通过代码与计算机交互的方式。这种方式与图形界面的软件（如SPSS）有所不同。在使用SPSS软件时，我们通过点击屏幕来进行交互，并从菜单中选择选项；然而，在使用R语言或其他语言时，我们则是通过输入代码与计算机交互。对于其他计算机语言也是一样。 这样的交互方式也存在缺点：我们不知道哪些变量保留在内存中。当进行一系列操作时，我们不知道哪些操作是我们刚才输入的。它们存在于计算机的缓存之中。如果输入了许多变量，如a、b、c、d等，我们可无法回忆起这些变量的细节。因此，当我们学习R语言时，我们希望有一个更清晰、高效的地方来编写代码，而不是一次只写一行代码，然后再运行。因此，我们需要一个更友好的代码编辑器，而不仅仅是控制台。虽然控制台是与R进行交互的窗口，但功能还不够强大。 3.3.1 Rstudio的安装与界面介绍 幸运的是，我们可以使用RStudio来完成这项任务。在安装完R后，我们可以安装RStudio，有两个版本可以选择：Windows版和Mac版。安装时，我们需要将其安装在非中文目录下。安装后，我们可以选择64位系统。 打开RStudio后，显示出的是一个白色背景的界面，我们可以通过更改外观中的主题来改变界面颜色。在这个工具中有一个全局选项，然后选择外观，可以把界面改成你想要的样式。将界面背景改成灰色可以省些电，因为不用一直发亮光。 接下来讲讲界面的调整。我们的界面是由四个面板组成的，但是在刚安装RStudio时，我们看到的是三个面板的界面，界面可能还有点不一样。我们可以新建一个project来呈现一个新的面板。 现在我们有4个窗口。这4个窗口分别是脚本编辑区、控制台、环境和文件。 现在我们可以看到5个部分。1是顶部的菜单栏以及快捷方式栏。2是左上角的脚本编辑区，3是左下角的控制台，4是右上角的环境/历史/链接区，5是右下角的文件/图片/程序包/帮助/预览区。 脚本编辑区 脚本编辑区可以记录下我们想要写的代码，并且可以选择性地运行。我们也可以直接创建一个R脚本，然后输入代码。在输入代码的时候，RStudio会提示我们，根据我们之前输入的字符来预测我们可能想要输入的代码。这样的话，我们就不用记住很多代码，可以更方便地完成代码。我们可以将所有的代码放到一起，形成一个脚本文件，一般以 .R作为文件后缀。我们可以用RStudio打开它，这样可以更好地编写R代码。 数据对象的具体内容也将显示在脚本编辑区域。 右上角这个地方，实际上它不是一个窗口，而是多个窗口，第一个选项卡，这是一个被称作environment的窗口，类似于人的工作记忆一样，会存储我们运行中的所有的数据和变量。现在这块区域是空白的，因为我们没有进行过任何的数据的操作和读取。如果我们运行了一些有关变量的R指令，涉及的变量就会在这块区域中出现。 我们最好经常留意环境区域的变化，如查看新生成的对象是否正确等。 第二个窗口是history,我们所有运行过的代码会在这里被列出，就像我们刚才输入的指令，都会保存在里面。可以命令运行历史。 第三个窗口是链接（connections），可以建立与外部数据库、脚本的链接。 控制台 控制台界面 Console，即控制台，展示了所有程序的交互结果。代码的运行结果与报错都会在该窗口展示。在Rstudio中不仅有Console控制台，还有一个teminal，这是一个与windows系统进行交互的界面。在Mac OS中也有相同的Termianl终端。我们可以让它进行在后台安装软件之类的工作，不过我们使用的不多。 Console，即控制台，展示了命令运行的结果，包括计算出的值、提示、报错等信息。也可以在这个区域直接缩写命令行，按回车键运行。但此区的命令为即使命令，无法保存。点击清扫按钮即可将当前的内容清除。 在后面我们使用github的时候，我们可能会较多的涉及到Teminal相关的内容。 右下角窗口集 右下角有许多子窗口，我们一般会频繁地用到其中的两三个，一个是files，一个是plot，可能还有一个help。 当我们对某个指令的具体功能和提供的参数不是很了解的时候，我们可以在help中进行输入和搜索，它就会提供给我们一些相应的解释和说明，我们可以详细的了解相关的函数和包。 file窗口是我们的文件浏览器，我们可以看到我们打开的文件夹里有什么文件。并且可以进行打开。因为我们打开的是一个R project文件，它会自动将工作目录关联起来。 plot界面是我们画出来的图的展示的地方，我们使用plot等指令绘制的图表将会在这个界面中呈现。我们可以通过手动拖拽窗口的大小来调整输出图片的大小，并且可以进行保存等操作。 若想保存图片，可点击窗口上的export按钮，选择想要保存的格式，最好是保存为pdf，可以保证图片的分辨率。 除此之外，我们还有packges选项卡，在其中我们能看到我们安装的包，并且对其进行管理。 3.3.2 测试Rstudio 现在，我们可以对我们的安装过程进行一个检查，如果我们能够在Rstudio中正常的输出这样的图表的话，就说明我们的安装不存在问题。 image-20230309144756488 典型问题 由于大家的输入都是以中文为主，所以输入法很多时候会保持在一个中文输入的状态，这个时候你的标点符号也会是中文的，但是所有R的指令需要的都是英文的标点符号。如果你输错了，可能就会发生问题，比如出现unexpected input这样的报错。 3.3.3 R语言中的包 包（package）是R中非常重要的一个概念，我们在前面提到过package窗口，它管理的对象就是我们的包。 RStuidio 中 package 管理界面 Packages 是一种模块化的方式，用于扩展R的基本功能。Package包含了函数、数据集、文档和其他需要扩展R的元素，这些元素按照特定的结构和命名方式进行组织和存储，方便用户引用和使用。它能够以类似R的方法对R的功能进行扩展。有了各种各样的包，才能让R语言的生态变得丰富。 包里一般会包含一些有特定功能的函数，有些也包括一些数据集，能够让你去方便的测试它的一些函数；也包含了说明文档，来解释包和各个函数的功能。 3.3.3.1 包的介绍与调用 R包的基本分类大概包括： Base:（安装R时自带的包） 其他:由社区所贡献的、内容丰富、领域特异性（数据可视化、统计分析、机器学习、深度学习、自然语言处理、图像处理）的包。 R报系统：有些相关的包会朝着同一个风格进行开发，形成一个系统，例如tidyverse; easystats。他们有自己特殊的语法风格。 使用包通常需要先安装，可以通过 install.packages() 函数进行安装，然后使用 library() 函数加载包。例如： install.packages(&#39;tidyverse&#39;) library(tidyverse) 注意：在使用install.packages() 时，括号内要加引号！括号、引号均为英文输入状态下的半角符号！ 由于 tidyverse 是一系列包的集合，因此在安装时会下载多个包；有时在安装一些包时，这些包的运行又依赖于其他的包，因此下载时会将这些包都下载下来。 3.3.3.2 镜像的选择 有些安装包安装起来会比较慢，这可能是因为用了国外的镜像网站。镜像网站是一个将完全相同的网站放到了国内的服务器，每个网站都有自己的网址。一般默认使用的是 CRAN镜像（R 语言的官方包管理库）；如果想要加快下载速度，就需要切换镜像网站地址（点击这里查看），如切换到清华镜像： # 查看当前默认使用的镜像： getOption(&quot;repos&quot;) ## CRAN ## &quot;@CRAN@&quot; # 切换清华镜像 options(repos = c(CRAN = &quot;https://mirrors.tuna.tsinghua.edu.cn/CRAN/&quot;)) "],["lesson-3.html", "4 第三讲：如何导入数据 4.1 路径与工作目录 4.2 Windows系统中的地址栏和R中的地址栏斜线方向不同。如果直接复制Windows地址栏的地址作为绝对路径，可能会出现错误。为了避免手动拼接路径出错。 4.3 读取数据 4.4 了解R里的数据 （R语言中的对象）", " 4 第三讲：如何导入数据 4.1 路径与工作目录 工作目录（working directory）是指当前的工作台（console）所进行交互的目录/文件夹，简而言之，即R会在其中寻找文件的文件夹。如果没有指定工作目录，R会默认使用用户文件夹作为工作目录。当打开R project时，R会自动将这个R Project 所在的目录作为工作目录，通常为用户文件夹。但是，如果数据文件不在用户文件夹中，那么需要告诉R去哪里找这些文件。这就是为什么我们需要设置工作目录。可以使用“get.wd”命令来查看当前的工作目录。 4.1.1 相对路径与绝对路径 建议为每一个数据分析项目设立一个文件夹，并在其中创建一个R项目。当每次打开项目时，它都会自动打开所在的位置。 路径是指文件（文件夹）所在位置，包含绝对路径和相对路径： 绝对路径和相对路径 绝对路径指的是文件或文件夹在硬盘上的真实路径，而相对路径则是指相对于当前文件（或当前工作目录）的路径，如，（D:/Document/r_course/chapter_3/data）。 相对路径是相对于当前文件（或当前工作目录）的路径，如”data”或者”./data”。相对路径的好处在于，当复制整个文件夹时，相对路径的位置不会改变。 相对路径是相对于当前路径的位置，而绝对路径则是相对于硬盘的起始位置。 如果有一个子文件夹叫做data，那么它就是当前文件夹的子文件夹，也可以称为子目录，而当前文件夹则是它的父文件夹，也可以称为父目录。如果复制这个文件夹到另一个电脑上，它们之间的相对关系不会改变，但如果把它从D盘复制到E盘，那么绝对路径就会改变，相对路径仍然保持不变。因此，建议把所有文件都放在一个文件夹中，这样所有文件的相对路径都是相对于这个文件夹的，方便编写代码。如果文件夹放在不同的位置，每次引用或查找文件时都需要使用绝对路径，非常麻烦。 在Windows系统中，每个磁盘都有自己的盘符，例如C、D、E、F盘。绝对路径是从盘符开始一直写到当前文件夹，相对路径则是当前数据所存储的文件夹。由于Windows系统中可以分成三个盘，每个区都是自己的盘符，然后基于这个盘符又可以重新定义一个新的路径。也就是说，对于路径的定义是不太一样的。因此如果把所有的分析都放在一起的话，那么用相对路径是更好的。比如写代码时，不要把C盘D盘这些一整串写进来，如果换电脑，这个C盘可能会变成D盘，或者换成苹果系统，可能就没有C盘了，而是以“用户”开头。 对于Mac和Linux系统，它们只有一个根目录，没有盘符的概念。最常用的是“用户”，比如你的用户名，然后有一个“主目录”，这就是用户的文件夹。当使用苹果系统或者Linux系统时，基本上很多时候都是基于这个“用户”文件夹或者“users.html4c”进行操作。除了“主目录”以外，它可能还有一个“下载”，是下载的文件所放的位置，还可能有一个“图片”，包括所存的各种图等等，所以它是在一个文件夹下面有不同的子文件夹，比如有“etc”、“用户”还有其他的“根目录”等等。我们主要就是基于“用户”，然后建立自己的一系列的文件。 获取及修改工作目录的基本操作 查看当前工作目录：getwd() 修改当前工作路径：setwd(““)在引号里修改你的工作路径 4.1.2 Here包的使用 这是一个常用的包，包名叫做Here。Here这个包可以帮助方便快捷地去定位当前新的目录。使用后，能迅速获取项目的工作路径。它另外一个好处是不用考虑系统，直接能够把它的一系列文件夹调出来。 4.2 Windows系统中的地址栏和R中的地址栏斜线方向不同。如果直接复制Windows地址栏的地址作为绝对路径，可能会出现错误。为了避免手动拼接路径出错。 可以使用here函数来自动补全路径。这个函数会自动将当前文件夹的路径补全，只需要在路径中加入文件夹名即可。例如，可以使用here(“data”, “match”)来获取data文件夹下的match文件。这个小技巧可以帮助避免一些错误，特别是在不同电脑之间切换时。 4.3 读取数据 工作目录是R与硬盘交互的接口，可以使用getwd()函数来获取当前工作目录的绝对路径。使用相对路径来引用子文件夹的位置。接下来查看数据的相对路径和绝对路径。假设工作目录是在桌面上，数据存储在”data”文件夹中，其中包含两个子文件夹，一个是”match”，包含178个文件，另一个是”pengiun”，只有一个文件。相对路径中的一个点(.)代表当前文件夹，数据在”data”文件夹中，“match”文件夹下有178个文件。如果要读取某个文件，必须完整地写出文件名。对于”pengiun”文件夹，它的相对路径是”./data/pengiun”。 4.3.1 read.csv函数 了解数据的相对路径，可以使用”read.csv”函数来读取数据。需要给数据一个名称，例如”pengiun.data”，并使用”read.csv”函数来读取数据。在函数中，需要指定相对路径。这里建议使用相对路径而不是绝对路径，因为绝对路径需要每次更改代码。 “head = TRUE”是函数中的一个参数，用于指定文件是否包含列名。如果文件的第一行是列名，则将其设置为”TRUE”，否则为”FALSE”。在R语言中，假用false表示，真用true表示。读取这样的txt文件时，需要使用分隔符来分隔不同列的数据。为了更直观地了解这个过程，可以打开csv文件。虽然这是一个csv文件，但也可以使用txt打开，如果使用文本编辑器打开，数据看起来会很混乱，因为它包含换行符。 当使用函数读取时，行是自动识别的，而列必须使用分隔符来识别。在这里，使用逗号作为分隔符。这样，就可以将数据读取为行和列。可以使用data table包来方便地展示读取后的数据。为了方便在PPT中展示数据，可以使用head函数来大致地查看数据。读取数据后，按照逗号分隔后，第一行是列名，可以将其识别为表头。从第二行开始是数据，第一个列的第一个值是1975，后面是1995等等。这是一个问卷数据，每个编号代表一个问卷和其items。可以用不同的方式对其进行编号，比如uncertainty等等。 对于常用的数据文件，可以使用here来定义相对路径，然后使用import来读取数据。读取后，可以看到有247个变量。对于.out文件，可以使用read.csv来读取，但是需要将separator参数设置为斜杠加t，表示以tab为分隔符。在进行统计分析时，需要明确定义用于分割数据的符号。除了逗号和tab键，还可以使用分号、空格或分行符等多种方式。 read.csv是最常用的数据读取函数，但实际它是read.table的一个特殊形式。因此，也可以使用read.table来读取数据。但是，由于.out文件是自定义的一个txt文件，无法使用bruce-r的import函数进行导入。这种情况很常见，需要清楚自己的数据结构，包括分隔符和编写模式，以便正确读取数据。 完成这些操作后，R中的环境environment应该发生了变化。在右上角的环境中，可以看到两个相应的数据match data和PenguinData。PenguinData是一个符号，可以用任何其他符合命名规则的名称来代替它，比如abcd，比如xyz。这里存储一些可操作和创建的对象，将其称之为R对象。导入的对象的种类被称之为数据框，有关数据框的知识将在之后讲到。 4.3.2 命名 在命名时，应尽量简短，避免使用中文。如果发现文件目录没有问题，但仍然出现错误，有两种可能性。一种是拼写错误，例如拼写单词penguin时少一个字母或交换两个字母的位置。另一种可能是文件夹的大小写不匹配，例如使用大写字母开头的文件夹，但在输入路径时使用小写字母。在这种情况下，需要仔细检查并逐行对比当前工作目录和文件夹的实际目录是否匹配。最有效的方法是直接点击文件的属性，查看其目录和名称是否与输入的完全相同。 4.4 了解R里的数据 （R语言中的对象） data structure R语言中的几个常用对象包括：向量、矩阵、数组、数据框和列表。其中，向量是一列数字或其他相同类型的元素，而矩阵是一个二维的数据结构，有长和宽，可以看作是将向量按行排列而成的。数组是在矩阵的基础上再往上拓展，变成了一个三维的数据结构。 在R语言中，更多地使用数据框，它看起来像矩阵，但每个列可以是不同类型的元素。而列表则更加复杂，每个元素都可以是不同类型的对象。这些对象在编程软件中非常常用，比如Python和Matlab。 大家可以查看王敏杰老师和张敬信老师的两本书，这些书将更清楚地介绍这些基础知识。 王敏杰老师的书名为《数据科学中的R语言》，系统介绍了Hadley Wickham的tidyverse和ggplot2包，另附电子版课程，完全免费，网址 https://bookdown.org/wangminjie/R4DS/ 。 张敬信老师的《R语言编程：基于tidyverse》，https://github.com/zhjx19/introR "],["lesson-4.html", "5 第四讲 &amp; 第五讲 ：数据导入 – 数据类型与结构 5.1 加载数据 5.2 数据导入 5.3 读取数据 5.4 赋值 5.5 数据类型 5.6 数据结构 5.7 函数 5.8 if 条件语句", " 5 第四讲 &amp; 第五讲 ：数据导入 – 数据类型与结构 今天我们正式开始用r来处理数据。记得去年教授这门课程时，有同学遇到了报错和乱码的问题，无法正常显示，这是因为部分语言设置不正确。为避免此类问题，大家可以将自己的编程语言设置为英文，这样就不会出现乱码了。也就是说，当你遇到报错时，如果系统默认语言是中文，那么在输出时可能会有乱码，无法被识别，这是一个小问题。将界面语言设置成英文的代码如下： # set local encoding to English if (.Platform$OS.type == &#39;windows&#39;) { Sys.setlocale(category = &#39;LC_ALL&#39;,&#39;English_United States.1250&#39;) } else { Sys.setlocale(category = &#39;LC_ALL&#39;,&#39;en_US.UTF-8&#39;) } # set the feedback language to English Sys.setenv(LANG = &quot;en&quot;) 在开始分析数据数据之前，我们需要加载分析数据时所需要的包，一般使用library()函数。但更推荐大家使用名为packman（Package Management ）的包来加载，使用pacman::p_load不仅可以批量加载包，而且遇到没有安装的包时会自动为我们安装。 if (!requireNamespace(&#39;pacman&#39;, quietly = TRUE)) { install.packages(&#39;pacman&#39;) } pacman::p_load(bruceR,here) 5.1 加载数据 既然我们要开始处理数据，而R作为一种工具，是解决数据分析问题的关键。我们需要将R整合到数据分析流程中，用R来完成整个数据分析过程。数据分析的第一步通常是获取数据。对于心理学专业的同学来说，大多数人获取数据的方式是通过自己进行实验和发放问卷来收集数据，并通过问卷平台或实验数据收集工具将数据汇总。而在其他情况下，获取数据的方式可能会有所不同。例如，你毕业后可能会在其他地方工作，此时你遇到的数据可能不是通过实验收集的，而是需要通过网络或其他途径来获取，或者可能是别人交给你的未经整理的数据。 在处理数据之前，你可能需要进行一些额外的步骤，比如通过网络爬虫技术来爬取数据，这是一种获取数据的方式。假设现在大家手头已经有了一些数据，接下来，我们要探讨的是如何将这些数据导入到我们的数据分析软件中。对于本科毕业论文，我们都知道SPSS是如何导入数据的。 同样，我们也可以尝试在R环境中进行类似的操作，比如在R Studio的界面中，我们通过File-Import Dataset选项中手动导入数据，选择数据打开后，在右上角的 Environment 界面中会显示已导入的数据的名称。但是如果我们要导入的数据非常多时，或者我们需要将许多分散的数据合并成一个数据时，使用手动点击的方式一个个导入就会非常费时费力，因此代码会是效率更高的选择。 在课程中，我们会遇到两个主要的数据集，一个是Human Penguin Project问卷数据集，另一个是Perceptual matching 实验数据集。我们刚才通过点击操作完成了数据导入并查看了问卷数据，现在则要通过代码来完成相同的操作，包括尝试选择一些变量进行初步统计。这就是我们在数据分析中遇到的第一个问题。本节课的主要目标就是要解决这一问题。 5.2 数据导入 5.2.1 数据的“地址”——路径 如果我们希望通过代码来导入数据，那么首先需要告诉电脑数据“住在”哪里（address），即到哪个文件夹来寻找数据。如果只是在文件夹中以点击的方式来寻找我们想要的数据，比要在 R4Psy 中找到名为 penguin_rawdata.csv 的数据，则需要按照以下步骤进行点击（以 Macos 系统为例）： 如果将点击文件夹的名称按顺序组合起来，并且中间使用斜杠(/)作为分隔符，就形成了计算机寻找文件的路径：“./data/penguin/penguin_rawdata.csv” 或 “data/penguin/penguin_rawdata.csv”（注意，路径的前后都需要有引号，单引号双引号都可以，具体原因会在这节课的结尾解释）。 但是很容易发现，在 R4Psy 文件之前仍然存在文件夹，如果一直追溯的话会发现路径会变得非常非常长： 完整的路径写成代码的话应该表示为：“/Users/cz***/Documents/github/R4Psy/data/penguin/penguin_rawdata.csv”。 5.2.2 绝对路径与相对路径 对于这两种路径，后者（即完整的路径）称为绝对路径，而前者（不完全的路径）称为相对路径，二者区别在于是否要设置一个文件夹所谓搜索的起始点，比如在上面例子中，相对路径设置了 R4Psy 作为搜索的起始点，而绝对路径则从硬盘所在的文件夹开始搜索。显然，就写法而言相对路径会更加轻松。而这个起始文件夹被称为工作路径(working directory，绝对路径 = 工作路径+相对路径)，如果设定了工作路径，在工作路径之前的内容就可以省略不写。 在RStudio 的 global options-General中可以设置默认的工作路径；当然，也可以使用setwd()函数来手动修改，使用getwd()函数来查看当前的工作路径；另外， Rmarkdown(.Rmd) 和Rproject(.Rproj)这两种文件对于路径的处理会比较特殊，它们会默认将文件所在的地址作为工作路径，这非常有利于和别人分享你的结果：我们一般会将打包放在和 Rmarkdown 文件相同路径的文件夹中，如果你将 Rmarkdown 及数据打包分享给他人时，别人使用你的 Rmarkdown 来加载数据，比如 R4Psy 文件夹，尽管在别人的电脑中 R4Psy 之前的路径和你完全不同，但别人运行 Rmd 文件时完全不用对路径做任何修改，也不用重新修改默认的工作路径。 对于工作路径的设置，除了使用 setwd()函数外，还可使用 bruceR 包中的set_wd(ask = T)或set.wd(ask = T)进行设置，另参数 ask = T 可以调出可视化界面通过点击的方式进行选择。 在 Windows 系统中道理是一样的，但如果大家点击地址栏的话，会发现路径的分隔符使用的是反斜杠（\\）而非上里面例子中所展示的(/)： &lt;img src=&quot;pic_new/chp4/abspath.jpg&quot; alt=&quot;abs_path&quot; style=&quot;zoom:50%;&quot;/&gt; 在 R语言中，路径分隔符要求为斜杠（/），注意不能直接从路径栏中直接复制地址。这是一个非常细节的问题，但也有更加方便的方式来解决这个问题，即使用here::here()函数，只需要在这个函数中按顺序依次输出，比如对于上面的相对路径就可以写为： here::here(&#39;data&#39;,&#39;penguin&#39;,&#39;penguin_rawdata.csv&#39;) 5.3 读取数据 5.3.1 手动导入 在设置好工作路径后，可以在 Files 窗口中直接点击数据，这个过程与 SPSS 或 Excel 里导入数据的操作类似。 5.3.2 代码导入 当然，我们更推荐使用代码的方式来导入数据： penguin_data = bruceR::import(here::here(&#39;data&#39;, &#39;penguin&#39;, &#39;penguin_rawdata.csv&#39;)) ncol(penguin_data) ## [1] 247 nrow(penguin_data) ## [1] 1523 head(penguin_data) tail(penguin_data) 在上面的代码中，我们使用了 bruceR 包中的 import 函数来导入 penguin_rawdata.csv文件，并将数据命名为penguin_data（即等号’=’）；导入成功后，可以在 Environment 界面中看到这个名称，如果在 console 界面中输入penguin_data，就会返回具体的数据内容；使用 head()或tail()可以查看数据的前 5 行或最后 5 行。ncol()和nrow()分别查看数据的列数与行数。 （如果 bruceR包已经加载了，在代码中bruceR::部分是可以省略不写的，对于简单的数据处理来说没有问题，但随着数据处理与分析的难度增加，会用调用非常多的包，有时候不同包之间会存在有相同变量名的函数，如果不声明函数来源于哪个包的话，要么 R语言会提示函数存在冲突，要么本来想用 A 包中的函数，但实际上执行了 B 包中的同名函数，因而出现报错。因此建议大家现在就养成在函数声明其来源的好习惯） 如果大家之前使用点击的方式来导入数据，会发现在 console 中会显示导入数据的代码，代码中使用的是read.csv()函数，而我们更推荐使用import()函数作为替代，因为import()函数对于心理学常见数据几乎都是适用的(如txt,csv,sav,dta等等，可通过?bruceR::import()查看具体信息)，而read.csv()只适用于 csv 类型的数据。 5.4 赋值 在导入数据时，我们使用了等号进行赋值。而在R中，赋值操作符可以使用“&lt;-”，也可以使用“=”，二者是等价的。 x = 1 y = x + 5 ## cat函数将值的内容输出到屏幕上 x ## [1] 1 y ## [1] 6 ## 重新对 x 进行赋值 x = 1000 ## x x ## [1] 1000 y ## [1] 6 通过上面的例子可以发现，对 x 进行赋值，并利用 x 进行计算后，更改 x 的值并不会影响 y的结果。此外，如果仅仅只是利用数据进行计算而没有复制操作的话，计算结果并不会被保存： object &lt;- 10 object ## [1] 10 # 利用 object 进行计算但不赋值 object + 2 ## [1] 12 # object 数值并没有发生改变 object ## [1] 10 此外需要注意的是，R语言中的变量名的命名有一系列的规则： 区分大小写，’Object’和’object’是两个不同的变量名； 变量名内不能使用空格，但可以用下划线代替空格； 变量名开头不能是数字和一些特殊符号(如+-*/) ； 5.5 数据类型 在读取数据后，我们需要知道数据中的具体内容有哪些，但这里就需要了解一些与数据类型相关的知识。在 SPSS 中的变量视图中，不同的变量可能会有不同的数据类型，如名义、标度、有序等，Excel 中对于单元格也可以去选择常规、数值、货币、日期等类型，R 语言也是一样。 可以观察一下我们导入的 pengui_data 数据，可以发现对于每个单元格来说，大概可以分为两类，一种是数字，一种是文字；而数字也有整数和小数之分。凭直觉来说，数字是可以进行加减乘除等运算的，而文字（如‘Oxyford’）不能（实际上也确实如此）。 imp3 在R语言中，数据类型可以简单分为三类： 5.5.0.1 数值型(numeric) 包括浮点型(double,即小数)、整型(int)等，可以进行数学运算； # 简单做个加法 1 + 1 ## [1] 2 ## 直接输入pi会返回圆周率 pi ## [1] 3.141593 ## 幂运算 10^3 ## [1] 1000 ## 取对数 log(1) ## [1] 0 使用class()函数（即类）可以查看具体是什么数据类型，比如： class(pi) ## [1] &quot;numeric&quot; 5.5.0.2 字符串(character) 即文字内容，但需要注意的是，字符串的前后需要被英文的引号所包裹，单引号双引号都行。无论内容是什么，只要前后存在引号，R语言都会将其识别为字符，哪怕其中是乱码： class(&#39;1 + 1&#39;) ## [1] &quot;character&quot; class(&quot;ㄤ娇鐢ㄦ鏂囦欢锛&quot;) ## [1] &quot;character&quot; ## 之前提到的路径，其实也是字符串，所以需要加引号 class(getwd()) ## [1] &quot;character&quot; 5.5.0.3 逻辑值(logical) 即布尔值。大家肯定都听说过计算机使用二进制 0 和 1来表示“关”和“开”或“假”和“真”；逻辑值与此类似，即True 为真（对），False 为假（错），分别可以简写为 T 和 F。逻辑值通常是比较运算的结果： 2 &gt; 1 ## [1] TRUE T == TRUE ## [1] TRUE T &gt; F ## [1] TRUE class(2 &gt; 1) ## [1] &quot;logical&quot; 常见的比较运算符如下图： 5.6 数据结构 之前的内容介绍了三种数据类型（相当于数据中的某一个单元格），数据类型之间可以相互组合，进而形成了丰富的数据结构： 5.6.1 向量(vector) 数值、字符串、逻辑值都可以各自组合在一起，可以分别组成数值向量、字符串向量、逻辑向量。可以使用c()来建立向量（c可以理解为 combine），但需要注意的是，向量里的类型必须相同，因此使用 class()来查询向量类型时，会返回向量中具体的元素类型，而不是 vector。 ## 数值型向量 v1 &lt;- c(1,2,3,4,5) # 对于连续数字可以使用冒号简写，即从1到(:)5 # 在连续数字或单个元素时可以省略c() v1 = 1:5 ## 字符型向量 v2 &lt;- c(&#39;apple&#39;,&#39;pear&#39;,&#39;banana&#39;,&#39;strawberry&#39;,&#39;lemon&#39;) # 每个元素都要写一遍引号非常麻烦 # 因此可以使用 bruceR 包中的 cc()函数，开头结尾有引号即可 v2 &lt;- bruceR::cc(&#39;apple,pear,banana,strawberry,lemon&#39;) ## 逻辑型向量 v3 &lt;- c(T,F,F,T,T) ## 使用 class查看，并不会返回vector class(v1) ## [1] &quot;integer&quot; 5.6.1.1 类型转换 那么可能会有人有疑问，如果在一个向量里面同时包含不同类型的数据会出现什么结果： ## 数值和逻辑 x1 = c(T,2) class(x1) ## [1] &quot;numeric&quot; ## 数值和字符串 x2 = c(2,&#39;a&#39;) class(x2) ## [1] &quot;character&quot; ## 逻辑值和字符串 x3 = c(T,&#39;a&#39;) class(x3) ## [1] &quot;character&quot; ## 数值型、逻辑型和字符串 x4 = c(1,T,&#39;a&#39;) class(x4) ## [1] &quot;character&quot; 可以发现，如果数值型和逻辑值同时出现，则会被强制转换为数值型向量；如果数值型或逻辑型与字符串同时出现，则会强制转换为字符串向量，这就是 R语言中数据类型的强制转换机制。当然这也提醒我们，不同类型之间的数据是可以相互转换的，而在 R 语言中也存在以 as.开头的函数执行转换操作： as.character(1:3) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; as.numeric(c(&#39;1&#39;,&#39;2&#39;,&#39;a&#39;)) ## Warning: NAs introduced by coercion ## [1] 1 2 NA as.numeric(c(T,F)) ## [1] 1 0 在上面例子中，需要注意第二个例子。字母本身并不能转换成数字，因而使用 NA 进行替换；而 NA(Not Available) 指数据中的缺失值。 和 as.系列函数相类似的还有is.系列函数，用于判断数据是否是某个数据类型，比如 is.character(1) ## [1] FALSE is.na(1) ## [1] FALSE is.numeric(1) ## [1] TRUE 5.6.1.2 向量化操作 在 R语言中，向量有着独特的运算方式： x = 1:6 x + 1 ## [1] 2 3 4 5 6 7 x * x ## [1] 1 4 9 16 25 36 可以发现，对 x 向量进行加一的运算会返回对x 中每一个元素进行加一运算；而向量之间的相乘会返回每个元素乘积所形成的向量，这种操作称为向量循环。 5.6.1.3 向量的索引 如果想要提取向量中的某个值，可以在中括号中输入数字向量进行索引： v2 &lt;- c(&#39;apple&#39;,&#39;pear&#39;,&#39;banana&#39;,&#39;strawberry&#39;,&#39;lemon&#39;) ## 查找第 1 个元素 v2[1] ## [1] &quot;apple&quot; ## 查找第 2 和第 4 个元素 v2[c(2,4)] ## [1] &quot;pear&quot; &quot;strawberry&quot; ## 查找第 1 个到第 4 个元素 v2[1:4] ## [1] &quot;apple&quot; &quot;pear&quot; &quot;banana&quot; &quot;strawberry&quot; ## 负索引（ = 删除元素） ## 删除前两个元素 v2[-c(1:2)] ## [1] &quot;banana&quot; &quot;strawberry&quot; &quot;lemon&quot; ## 也可以利用索引对元素进行更改 v2[1:2] = bruceR::cc(&#39;APPLE,PEAR&#39;) v2 ## [1] &quot;APPLE&quot; &quot;PEAR&quot; &quot;banana&quot; &quot;strawberry&quot; &quot;lemon&quot; 5.6.1.4 因子(factor) 对于心理学专业的同学来说，因子这个词应该非常熟悉了，因子分析是对于问卷数据来说是常用的方法之一，但这里的因子与因子分析中的因子概念并不相同。 在统计中，我们将数据分为称名数据、顺序数据、等距数据、等比数据四种类型，而因子这一数据结构（容器），专门用来存放称名数据和顺序数据。 相较于字符串，直接用字符向量也可以表示分类变量，但它只有字母顺序，不能规定想要的顺序，也不能表达有序分类变量。 x &lt;- c(&#39;good&#39;,&#39;better&#39;,&#39;best&#39;,&#39;bad&#39;,&#39;worse&#39;,&#39;worst&#39;) # 使用sort函数进行排序，结果只会按照字母顺序排序 sort(x) ## [1] &quot;bad&quot; &quot;best&quot; &quot;better&quot; &quot;good&quot; &quot;worse&quot; &quot;worst&quot; ## 可以使用 factor 来创建因子，并使用 levels 参数来规定具体的顺序 x1 &lt;- factor(x,levels = c(&#39;best&#39;,&#39;better&#39;,&#39;good&#39;,&#39;bad&#39;,&#39;worse&#39;,&#39;worst&#39;)) sort(x1)#排序 ## [1] best better good bad worse worst ## Levels: best better good bad worse worst 因子通常用于表示有限集合中的元素，但输入的类型可以是整型，也可以是字符串。 5.6.2 数据框 如果将向量视为一列，不同列拼接在一起就形成了我们常用的数据(比如 penguin_data)，我们称之为数据框(dataframe)，显然，数据框要求形状必须是方形，即每一列的长度必须相等。我们可以尝试手动定义一个数据框： v1 &lt;- c(1,2,3,4,5) v2 &lt;- c(&#39;apple&#39;,&#39;pear&#39;,&#39;banana&#39;,&#39;strawberry&#39;,&#39;lemon&#39;) v3 &lt;- c(T,F,F,T,T) df1 = data.frame(col1 = v1,col2 = v2,col3 = v3) class(df1) ## [1] &quot;data.frame&quot; df1 ## col1 col2 col3 ## 1 1 apple TRUE ## 2 2 pear FALSE ## 3 3 banana FALSE ## 4 4 strawberry TRUE ## 5 5 lemon TRUE 上面例子中，我们首先建立了三个等长但类型不同的向量，之后使用data.frame()函数将三个向量“打包”成一个数据框并赋值给df1，其中，col1、col2、col3 分别为 v1、v2、v3 的列名，当然列名也可以其对应的向量的名称重合，尝试一下将列名修改成与向量名一致： ## colnames()可以查看数据框列名 colnames(df1) ## [1] &quot;col1&quot; &quot;col2&quot; &quot;col3&quot; ## 对 colnames 进行赋值可以修改列名 ## 但在之前，介绍一个字符串的拼接函数 ## 将&#39;v&#39;与向量 1:3分别拼接，输出字符串向量 newname = paste0(&#39;v&#39;,1:3) newname ## [1] &quot;v1&quot; &quot;v2&quot; &quot;v3&quot; colnames(df1) = newname df1 ## v1 v2 v3 ## 1 1 apple TRUE ## 2 2 pear FALSE ## 3 3 banana FALSE ## 4 4 strawberry TRUE ## 5 5 lemon TRUE 5.6.2.1 数据框的索引 5.6.2.1.1 数字索引 数据框的索引与向量的索引非常相似，但不同的是，我们需要在行和列两个维度上进行索引，在中括号中，第一个数字对行索引，第二个数字对列索引，中间需要使用英文逗号进行分隔。以 penguin_data为例： # 选取第一行、第二列； penguin_data[1,2] ## [1] 1 # 如果想全部选取行或列，可以使用空索引 # 比如，选取第一列的所有行，不写行的索引就行 # 查看前五行 head(penguin_data[,1]) ## [1] 1975 1995 1995 1988 1991 1995 # 如果要使用非连续的数字进行索引，要带上c() colnames(penguin_data[ ,c(1,3,5)]) ## [1] &quot;age&quot; &quot;ALEX2&quot; &quot;ALEX4&quot; 5.6.2.1.2 名称索引 在行或列的位置输入列的名称即可，比如查看出生日期和地址： head(penguin_data[,cc(&#39;age,Site&#39;)]) ## age Site ## 1 1975 Oxford ## 2 1995 Oxford ## 3 1995 Oxford ## 4 1988 Oxford ## 5 1991 Oxford ## 6 1995 Oxford 当然上面的例子使用的是列名，对于数据框来说，也有行名(使用rownames()添加名称)，默认使用的是行数的字符串形式当做列名，只是大部分时候，我们不会使用行名进行索引。 5.6.2.1.3 美元符 美元符$是对列索引的一种方式，在 Rstudio 中，数据框后如果紧接$，就会自动弹出可视化窗口显示这个数据有哪些列名，也会提示列的类型和具体数据内容，非常方便；但局限在于，美元符一次只能提取一个列。 ## 使用美元符来提取出生日期 head(penguin_data$age) ## [1] 1975 1995 1995 1988 1991 1995 小练习 尝试把以下表格中的内容创建为数据框，并把第一列改为因子 5.6.3 矩阵与数组 矩阵和数据框类似，都是二维的数据；但不同点在于，数据框允许不同列的类型不一样，而矩阵中所有单元格的数据类型必须相同： #创建矩阵 m1 &lt;- matrix(c(1:9),nrow=3) m1 ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 相同维度的矩阵可以继续组合，形成数组： #创建三维数组 a1 &lt;- array(1:24,dim=c(3,4,2)) a1 ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 13 16 19 22 ## [2,] 14 17 20 23 ## [3,] 15 18 21 24 5.6.4 列表 多个相同元素可以组合成向量，多个向量可以组合成矩阵或数据框，而不同元素、向量、矩阵或数据框仍然可以继续组合，进而形成了列表： # 使用list 来创建列表 l1 &lt;- list(1, c(&#39;a&#39;,&#39;b&#39;), c(T,F)) l1 ## [[1]] ## [1] 1 ## ## [[2]] ## [1] &quot;a&quot; &quot;b&quot; ## ## [[3]] ## [1] TRUE FALSE # 列表里面也可以容纳列表 # 将l1与 penguin_data合并成一个列表 l2 = list(l1, penguin_data) 列表的索引与向量类似，但需要注意的是，使用中括号对列表进行索引，输出结果的类型仍然是列表；如果希望将数据还原成其原本的形式，就需要使用双中括号（[[]]）： # 索引第一个元素并查看类型 l1[1] ## [[1]] ## [1] 1 class(l1[1]) ## [1] &quot;list&quot; # 使用双中括号 class(l1[[1]]) ## [1] &quot;numeric&quot; 对于列表中还套着列表就需要多次索引，比如通过索引来找到l2中的l1的第一个元素1（数值型），就需要两次索引，其中第一次索引返回l1列表，第二次索引从l1列表中找到第一个元素。 l2[[1]][[1]] ## [1] 1 class(l2[[1]][[1]]) ## [1] &quot;numeric&quot; # 如果使用一个中括号进行索引，返回数据类型仍然是列表 class(l2[[1]][1]) ## [1] &quot;list&quot; 5.7 函数 在导入数据的时候，我们使用了import()函数，在设置路径时，使用了here(),getwd(),setwd()等函数。在R中，函数是一种用于执行特定任务或计算的代码块。函数接受输入参数，执行特定的操作，并返回结果。如果我们不知道一个函数是什么，有什么用处。在R中，我们可以在Console中使用“?函数名”来打开帮助文档，以import()函数为例： 函数一般都需要输入参数，比如import()函数中接受的参数为： 但很容易注意到，file 参数后面没有’=‘，其余所有参数后面都有’=’且设定有具体的值，比如 给encoding 参数赋值为NULL，这种方式是为了给参数设定一个默认值：在导入 penguin_data的例子中，我们仅仅输入了路径，别的参数如 encoding并没有被说明，这种操作之所以可行的原因就在于当我们没有给出某个参数的具体输入时，函数就会直接使用默认值，默认值的设置为函数的使用带来极大的便利。 但在导入 penguin_data的例子中，还有个问题是，我们输入路径时，也并没有声明一定是输入给 file 参数，为什么函数“知道”我们想输入给谁呢？这是因为，在函数中，如果有多个参数的话，函数会默认按照输入的参数的顺序进行匹配，我们只输入了一个参数，因而会与 file 参数进行匹配。 在我们的讨论中，似乎出现了两种参数：一种是函数内设定的参数名称，如 file、encoding 等，一种是我们实际输入的内容，如具体的路径。前者称为形式参数(file)，后者称为实际参数(输入的具体路径)。在不输入形参的情况下，函数默认会按照输入实参的顺序与形参进行匹配；如果我们声明形参的实参时，就可以按照我们想要的顺序来输入实参，比如： ## 没有声明形参 penguin_data = bruceR::import( here::here(&#39;data&#39;, &#39;penguin&#39;, &#39;penguin_rawdata.csv&#39;), ### file NULL ### encoding ) ## 如果声明形参，顺序可以调换 penguin_data = bruceR::import( encoding = NULL, file = here::here(&#39;data&#39;, &#39;penguin&#39;, &#39;penguin_rawdata.csv&#39;) ) ## 当然，如果按照这个顺序不输入形参的话会报错，大家可自行尝试 5.7.1 函数的调用 如果在已经加载包的情况下，可以直接使用某个包里的函数；如果没有调用某个包，还想使用其中的函数，就需要使用::来调用（比如bruceR::import()） 5.7.2 自定义函数 我们使用的函数有不同的来源，一种是来自 R 语言内置的 base 包的函数，一种是从 CRAN 中安装的第三方包中的函数，大部分时候上面的函数都可以满足我们的需求，我们要做的只是调用即可，但也会出现这些函数不能完全满足我们的需求情况，比如计算平均数和标准差通过内置函数mean()和sd()能实现，但我们希望在文章中输出为\\(Mean±SD\\)的形式，这时候我们就需要在已有函数的基础上稍作改动，即自定义函数。 5.7.2.1 函数的组成 函数定义通常由以下几个部分组成： - 函数名: 为函数指定一个唯一的名称，以便在调用时使用； - 参数: 定义函数接受的输入值。参数是可选的，可以有多个； - 函数体: 包含实际执行的代码块，用大括号 {} 括起来 - 返回值: 指定函数的输出结果，使用关键字return。 #定义一个函数：输入x和y，返回3倍x和5倍y的和 mysum &lt;- function(x,y){ result = 3*x+5*y return(result) } #mysum:自定义的函数名 #x,y:形式参数 #result = 3*x+5*y:函数体 #return(result):返回值 #调用函数,x=1,y=2，省略形参 mysum(1,2) ## [1] 13 # 当然也可以声明形参 mysum(y=1,x=2) ## [1] 11 ### 尝试为函数设定默认值 mysum2 &lt;- function(x = 6,y = 7){ result = 3*x+5*y return(result) } ### 省略参数 mysum2() ## [1] 53 ### 如果只输入一个参数会怎么样 mysum2(5) ## [1] 50 小练习：定义一个函数,输入值a,b,c,返回(a+b)/c;并计算abc分别为123时得到的值 #myabc &lt;- function(***){ # result = *** # return(***) #} #用合理的代码替换以上“***”,删除每行前的“#”,即可运行 5.7.2.2 函数的简写 上面所介绍的函数是完整的写法，但也可以使用一些方法去简化函数的书写： return()的省略： 在一些非常简单的函数中，如果省略return()，函数则会返回最后一个计算出的表达式的值： mysum_simpli1 = function(x,y){ ## 由于会默认返回计算数值 ## 因此这里也没必有赋值给 result的操作了 3*x+5*y } 函数体的简写： function(x)可以简写为\\(x)，以mysum函数为例： mysum_simpli2 = \\(x,y) {3*x+5*y} # 如果只有一行的话，为了书写简洁大括号甚至都可以省略 mysum_simpli3 = \\(x,y) 3*x+5*y 5.8 if 条件语句 当函数被调用时，如果输入的参数不符合预期，函数可能会抛出一个错误。这些错误信息对于程序员来说是非常重要的，因为它们指明了输入参数的问题所在。因此，当编写函数时，我们应该尽可能提供清晰的错误信息，以便用户能够理解并纠正错误。 比如，在学习数据类型时，我们提到字符型不能进行加减乘除等数学运算，比如在mysum函数中如果输入的内容为字符串，就会出现报错：non-numeric argument to binary operator。而在我们自定义的函数中，同样可以按照我们自己的想法来输出报错，但这需要对输出的结果进行判断：如果输出没有问题，就返回输出结果；如果出现错误，就需要给出为什么出现错误。这种判断其实是一种逻辑判断，即if-else条件语句： if-else 上面的语句含义为：如果(if)满足某个条件(condition，为逻辑运算)，就执行 Expr1，否则(else)，就执行Expr2。 举个例子，对于 mysum3 函数我们可以进行一下改进：在运算之前首先对输入的数据类型进行判断，如果输入为数值型，则进行运算并返回结果，否则就在屏幕上显示，x和 y 必须要为数字。 mysum3 &lt;- function(x = 6,y = 7){ if(is.numeric(x) == T &amp; is.numeric(y) == T){ result = 3*x+5*y return(result)} else{print(&quot;x and y must be number&quot;)} } #print：输出指定的内容 #is.numeric:判断是否为数值型。是则返回T，否则返回F # &amp; : 表示“且” mysum3(5,6) ## [1] 45 mysum3(&#39;a&#39;,&#39;b&#39;) ## [1] &quot;x and y must be number&quot; "],["lesson-6.html", "6 第六讲：如何探索数据: 描述性统计与数据可视化基础 6.1 描述性统计", " 6 第六讲：如何探索数据: 描述性统计与数据可视化基础 6.1 描述性统计 本节课主要讲述探索性数据分析和数据可视化。 探索性数据分析是数据科学中的一个重要概念，可以帮助我们发现数据中的规律和问题。 在传统的心理学中，我们通常会清楚地知道要进行什么样的分析，但是在数据科学中，可能面临的是一个未知的数据集，我们不知道其中的规律和数据的结构。因此，探索性数据分析非常重要。我们还将介绍数据可视化的方法，它可以帮助我们更好地理解数据。 6.1.1 Part1-探索性数据分析 在进行数据分析时，如果数据结构不清晰，甚至不确定数据是否干净，那么探索性数据分析（EDA）就变得非常重要。EDA的目的是在不确定数据结构和分析方法的情况下，通过描述和可视化等方式初步了解数据，从而帮助我们决定下一步使用何种统计方法或数据分析方法。在现在的大数据时代，EDA已经成为数据科学家们非常惯用的数据分析方法。 进行EDA时，需要了解数据的基本信息，比如数据的列数、变量类型、变量值的范围以及变量之间的关系等。 （1）准备工作：安装包并运行+读取数据 # 检查是否已安装 pacman if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # 如果未安装，则安装包 #更简洁的代码可以是这样 if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman # 加载所需要的R包 pacman::p_load(&quot;tidyverse&quot;) # 读取数据，并命名数据集 df.pg.raw &lt;- read.csv(&quot;./data/penguin/penguin_rawdata.csv&quot;, header = TRUE, sep=&quot;,&quot;, stringsAsFactors = FALSE) df.mt.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) #比如：human-penguin-project的这个数据集叫做df.pg.raw，这是原始的数据。match的数据集叫df.mt.raw。 （2）了解相关列（名称以及变量意义） colnames(df.mt.raw) # 使用colnames()查看列名，观察有哪些变量（适用于小数据集） ## [1] &quot;Date&quot; &quot;Prac&quot; &quot;Sub&quot; &quot;Age&quot; &quot;Sex&quot; &quot;Hand&quot; ## [7] &quot;Block&quot; &quot;Bin&quot; &quot;Trial&quot; &quot;Shape&quot; &quot;Label&quot; &quot;Match&quot; ## [13] &quot;CorrResp&quot; &quot;Resp&quot; &quot;ACC&quot; &quot;RT&quot; DT::datatable(head(df.mt.raw, 3)) # 了解数据内容，通常使用head(),在这里是查看了前三行数据。 （2-1）查看总体情况 如果需要了解常用的描述数值的统计量，如平均值中位数标准差等（即集中量数和差异量数等），可以使用psych这个包（主要用于问卷分析）。这个包里面有个describe函数，是用来描述数据大体情况的。Bruce R包中的describe函数，也可以帮助我们描述数据情况。 DT::datatable(summary(df.mt.raw)) #或者使用psych包 DT::datatable(psych::describe(df.mt.raw)) （2-2）查看特定值 如果想知道变量的平均数、中位数和标准差等统计量，在数据框中，每个变量（即列）都有一个 n 表示行数，均值、标准差、中位数等常用的描述数据的值也会显示出来。如果想提取这些值，可以使用 summarize() 函数。例如，可以使用管道将数据框作为输入，然后使用 summarize() 函数来计算某个特定变量（例如 RT）的均值、标准差和行数。也可以找到自己感兴趣的变量的值。 # 使用dplyr包中的summarise()函数 df.mt.raw %&gt;% summarise(mean_RT = mean(RT), sd_RT = sd(RT), n_values = n()) ## mean_RT sd_RT n_values ## 1 0.7149504 0.1508394 25920 # summarise函数不会忽略缺失值，如果计算的列中有缺失值，则会有报错。需要注意的是，许多汇总数据的函数（例如均值或方差）**不会自动忽略缺失值**，因此需要在计算均值时添加一个参数（例如 `na.rm = TRUE`）来忽略缺失值。在使用 `mean()` 函数时，可以使用 `?mean` 命令来查看具体的参数输入。 总之，可以使用描述性统计指标（例如均值和标准差）来了解数据框的情况。 6.1.2 Part2-数据可视化-ggplot2的基本使用 6.1.2.1 了解ggplot2 ggplot2是一种图形语法，它的核心是用图层来描述和构建图形。可以将数据映射到不同的图层中，然后将这些图层叠加起来形成最终的图形。所谓gg源于“grammar of graphics”，即图形语法。 比如，可以用ggplot2来绘制被试的正确率情况。可以从这个包中调用名为ggplot的函数，然后将数据框作为参数传入。 aes(Aesthetics)是一个映射关系，它决定了如何将数据映射到图形空间中，并选择使用什么样的几何图形进行可视化。在ggplot2中，可以选择使用不同的几何图形，比如条形图bar。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=ACC)) + # 确定映射到x轴的变量 geom_bar() + # 绘制直方图 theme_classic() # 设定绘图风格 6.1.2.2 练习 可以使用数据来练习ggplot2绘图。首先，先加载tidyverse库，然后读取数据，载入数据到df.mt.row中，最后使用ggplot2绘制图形。 由于前面的课程已经讲解了如何使用library和读取数据，因此这节课是基于前面的数据处理知识来讲解的。 可以使用一个大的data frame，其中有很多行和列，其中一个是acc，它有很多值，例如-1、0、1。 （1）首先，使用一个命令来指定数据集，即以data frame作为输入，然后使用data参数和aes参数将数据映射到图层上。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=ACC)) + # 确定映射到x轴的变量 geom_bar() + # 绘制直方图 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 现在，将x设置为ACC，这样只选择以它作为x轴。此时，它自动将x轴合并，并找到了它的独特取值，即-1、0和1。如果将数据映射到二维空间中，它只有x轴，没有y轴。它自动补全y轴，使用的是count，即每个取值的计数。将这个数值映射到一个几何图形中，例如线段line、点point或条形图bar。选择使用条形图来表示count，因为它适合描述计数数据。因此，我们将数据以直方图的方式进行表示。最后，我们指定了以classical主题呈现图形，ggplot包中包含了很多预设的主题，可以根据自己的需要进行选择。在这个代码中，我们可以注释掉theme_classical()，然后运行代码，看看结果是否正确。如果没有出错，我们会发现画出的图形与之前不同，它有一个灰色的背景和方格。实际上，如果我们不使用theme_classical()，R会使用默认的风格来画图，这个风格可能与我们想要的不同。因此，使用theme classical可以帮助我们选择适合我们要的风格来进行可视化。 （2）此外，我们还注意到，R的默认风格与我们在心理学或社会科学中常用的图形有所不同，例如字比较小，X轴的0点位置也不同。因此，如果我们要在论文中呈现出这个图形，我们需要改变这些默认设置。我们可以使用scale_y_continuous命令来调整Y轴的连续数据，例如将X轴固定在0的位置。我们可以使用“scale_y_continuous()”命令来调整Y轴的连续数据。如果Y轴是离散数据，需要使用“discrete”命令。此外，我们可以在“theme”中调整字体、颜色和大小等细节，以满足特定目的。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT)) + # 确定映射到x轴的变量 geom_histogram() + # 绘制直方图 stat_bin(bins = 40) + # 设定连续变量分组数量 scale_x_continuous(name = &quot;RT&quot;) + # 命名x轴 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.1.2.3 常用图1-密度图 对于离散数据，我们可以使用条形图来表示，而对于连续数据，我们可以使用直方图或密度图。在直方图中，我们使用高度来表示数据的多少，而在密度图中，我们使用平滑曲线来表示数据的分布情况。在绘制图形时，我们可以使用“geom”命令来选择几何图形，如条形图或密度图。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT)) + # 确定映射到x轴的变量 geom_density() + # 绘制密度曲线 scale_x_discrete(name=&quot;RT&quot;) + # 命名x轴 scale_y_continuous(expand=c(0, 0)) + # x轴在 y=0 处 theme_classic() # 设定绘图风格 6.1.2.3.1 叠加绘图 我们可以将直方图和密度曲线叠加在一起，这样更能看到它们的分布情况。在这里，我们常用的叠加方式是将alpha参数设置为透明度，数值从0到1之间变化，越小表示透明度越高。我们可以将aes放在geom里面，这样就能够将图形相互叠加，看得更加清楚。alpha是多个条件画图时十分常用的参数。建立好数据映射关系后，我们可以先画一个直方图，然后在其上叠加一个密度图。 如果我们不想显示alpha设置透明度对应的图例，可以用guide隐藏起来。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=RT, # x轴的变量 y=after_stat(density), # y轴对应的是密度曲线 alpha=0.8)) + # 透明度 geom_histogram() + # 绘制直方图 geom_density() + # 绘制密度曲线 guides(alpha=FALSE) + # 隐藏透明度alpha设置带来的图例 theme_classic() # 设定绘图风格 ## Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use &quot;none&quot; instead as ## of ggplot2 3.3.4. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.1.2.4 常用图2-箱型图 箱型图也是我们常用的一种图形，其中黑线表示median，box表示50%的quater，25%和75%的quater。如果我们将x换成一个以上的变量，就会出现四个以上变量的取值。 ggplot2::ggplot(data=df.mt.raw, # 指定数据 aes(x=Label, # 确定映射到xy轴的变量 y=RT)) + geom_boxplot() + # 绘制箱线图 theme_classic() # 设定绘图风格 我们以四个独特的取值作为x，然后按照这四个条件将RT分成四组。每一组都画成一个box plot，按照x的顺序排列，形成四个box plot的图。因为RT是连续的数据，我们可以对它计算统计指标，例如median和四分位。这些数据分成上下两部分，上面50%下面50%、下面25%下面75%、上面25%下面75%。我们可以看到大量的数据都集中在这个附近，同时也可以看出它有一些变化。这一步主要用于帮助我们初步了解不同条件下的反应情况。 另一个值得观察的是label。它按照字母顺序排序，因为它是字符类型的数据。在画图时，它会按照abcd的顺序进行排序。所以我们可以看到这个immoral在other之前，而i在m之前。这个顺序不一定是我们想要的，所以有时我们需要改变它的顺序。我们可以将Lable改为factor，并确定它的levels。然后我们可以使用mutate函数，例如label=factor(levels=c(“c”,“b”,“a”,“d”))，来改变它的顺序。这是画图时需要注意的小细节。 6.1.2.5 常用图3-散点图 如果我们有两个变量，并且想要描述它们之间的关系，我们需要进行探索性分析。我们需要把x轴和y轴分别赋值，然后用点来描绘它们之间的关系。比如，我们可以看一个人在前测和后测时体温的变化。我们可以用ggplot来描述这个关系，x轴是Temperature_t1，y轴是Temperature_t2。 ggplot2::ggplot(data=df.pg.raw, # 指定数据 aes(x=Temperature_t1, # 确定映射到xy轴的变量 y=Temperature_t2)) + geom_point() + # 绘制散点图 scale_x_continuous(name = &quot;Temperature_t1&quot;) + # 修改X轴的名称 scale_y_continuous(name = &quot;Temperature_t2&quot;) + # 修改Y轴的名称 theme_classic() # 设定绘图风格 ## Warning: Removed 30 rows containing missing values or values outside the scale range ## (`geom_point()`). 此外，我们可以用点来描绘这个数据的关系，如果它们沿着对角线，就是一个高度相关的关系。我们也可以加一个回归线，用LM来拟合一个平滑的线来代表它们之间的关系。 当然，在进行数据探索之前，我们也可以对数据进行预处理。比如，我们可以用penguin data来求出两个问卷的平均分。如果我们需要求出两个问卷的平均分：我们可以使用penguin raw data作为输入，然后使用mutate命令生成两列新的变量。一列叫做stress_ave，表示stress的平均得分。另一列是对手机的依赖，我们同样可以求它的均分。这样我们就得到了两个新的变量，一个叫做stress_ave，另一个叫做phone_ave。 # 利用管道符，可以帮助我们更简洁地合并数据处理和可视化的过程。 df.pg.raw %&gt;% dplyr::mutate(stress_ave=rowMeans(.[,c(&quot;stress1&quot;, &quot;stress2&quot;, &quot;stress3&quot;,&quot;stress4&quot;, &quot;stress5&quot;, &quot;stress6&quot;,&quot;stress7&quot;, &quot;stress8&quot;, &quot;stress9&quot;,&quot;stress10&quot;, &quot;stress11&quot;, &quot;stress12&quot;,&quot;stress13&quot;, &quot;stress14&quot;)]), phone_ave=rowMeans(.[,c(&quot;phone1&quot;,&quot;phone2&quot;,&quot;phone3&quot;,&quot;phone4&quot;,&quot;phone5&quot;, &quot;phone6&quot;,&quot;phone7&quot;,&quot;phone8&quot;,&quot;phone9&quot;)]) ) %&gt;% ggplot(aes(x=stress_ave, y=phone_ave)) + geom_point() + geom_smooth(method=&quot;lm&quot;) + # 在散点图上叠加回归线，语法可以查找帮助文档 theme_classic() ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 18 rows containing non-finite outside the scale range ## (`stat_smooth()`). ## Warning: Removed 18 rows containing missing values or values outside the scale range ## (`geom_point()`). 然后我们把这个data frame作为输入到ggplot里面去，X轴是stress的均值，Y轴是对手机依赖的均值。我们用这个几何图形来表达我们的数据，然后用smooth找到它们相互之间关系的一个回归线。这个回归线是用的linear model，它还有一个回归线的CI，就是95%的一个置信区间。最后是我们这个画图的主题。看到阴影，实际上是R里默认的一个输入，它不仅有一个回归线，还有一个回归线的95%置信区间。 ggplot2是数据可视化中非常重要的工具，它可以帮助我们化繁为简，不需要为每个参数设定值，因为它有很多默认选项。这样，我们可以快速地进行数据探索，而不需要关注太多细节。 虽然图表中有许多元素和参数，但我们可以手动控制每个部分，如点的大小、颜色、XY轴的名字、刻度和字体大小等。我们可以在同一图表中叠加多个元素，如散点图、回归线、轴和分布等，以显示丰富的信息。ggplot是一个非常丰富的生态系统，包含许多不同类型的图表，我们可以根据需要进行选择。 通过Tidyverse下面的数据处理，我们可以直接用ggplot画图，从原始数据到最终图表都可以一个管道完成。ggplot是一个非常常用的工具，可以精确地定制我们想要呈现的图表，并直接输出为PDF格式，从而方便我们提交给杂志。 6.1.3 Part3-Data Explorer （1）此外，Data Explorer也是一个很不错的数据探索工具，可以帮助我们快速探索数据。我们可以使用安装工具包来实现可视化，比如plot_string，它可以将DataFrame中的所有列名以可视化的形式表达出来，类似于思维导图中的树形图。另一个是plot_intro，它可以显示一些信息，比如有多少个离散数据列，有多少个连续数据列等等。我们可以看到，对于我们的匹配数据，离散列占56％，连续列占43％，所有列都是缺失值的占0％。每个数据至少都有一些值，完整的行占97.46％。缺失观测值的数量也可以通过可视化方式快速了解。 # load DataExplorer pacman::p_load(&quot;DataExplorer&quot;) DataExplorer::plot_str(df.pg.raw) DataExplorer::plot_intro(df.mt.raw) （2）这是数据探索包的一个独特特点，它可以帮助我们快速可视化数据。关于缺失值，我们可以使用plot_missing命令将具有缺失值的列可视化。大多数列都没有缺失值，只有一个响应列有2.5％的缺失值。 DataExplorer::plot_missing(df.mt.raw) （3）我们可以看到，几乎所有数字化变量的计数都可以用条形图表示。例如，性别可以用female，male，2和1表示。我们可以看到，大多数人是右撇子，而匹配和不匹配的比例是一致的。如果我们在匹配条件下看到匹配比不匹配多或不匹配更多，那么可能存在问题，因为我们的实验设计是一致的。同样，我们的实验条件应该是平衡的，因此看起来应该是一模一样的。 DataExplorer::plot_bar(df.mt.raw) （4）我们可以使用plot bar将所有变量以bar图的形式呈现出来。我们还可以根据match条件将数据分成matched和mismatched两部分，并用bar图表示每个部分的比例。在大多数情况下，matched和mismatched是平衡的。我们还可以使用histogram来快速绘制所有变量的分布情况，特别是连续变量的分布情况。我们可以使用gg plot来检验数据是否符合正态分布。 DataExplorer::plot_correlation(na.omit(df.pg.raw[, 2:30])) （5）我们也可以绘制一个表之间的相关矩阵，这样就更有意义了。在这时，这个函数na.omit对我们来说非常有意义，因为它可以去除缺乏值的行，否则会报错。 "],["lesson-7.html", "7 第七讲：如何进行基本的数据分析: t-test和ANOVA", " 7 第七讲：如何进行基本的数据分析: t-test和ANOVA {r setup, include=FALSE}2 knitr::opts_chunk$set(echo = TRUE) library(tidyverse) # Wickham的数据整理的整套工具 pdf.options(height=10/2.54, width=10/2.54, family=\"GB1\") # 注意：此设置要放在最后 ## 语法实现 7.0.1 t检验 7.0.1.1 理论基础 我们来讲讲t检验。最简单的是单样本t检验，它将样本的均值与某个固定值进行比较，这个固定的值通常被认为是该样本所在的总体。在这种情境下，按照NHST的逻辑，我们首先假设\\(H_0\\)为真，即样本确实是从总体均值为这个固定值的分布中抽样出来的。然后根据\\(H_0\\)的统计模型来计算当前数据或者更加极端数据的概率。如果概率很低，那么在一次抽样中这样的小概率事件不太可能发生，因此拒绝假设\\(H_0\\)。\\(p\\)值与假设水平进行比较时，我们假设\\(H_0\\)为真。我们有三个选择，等于、大于或小于。如果不等于，那么是双样本检验，如果小于或大于，则为单样本检验。 下面具体来介绍这些检验的不同类型。 首先是单样本检验，当p值小于等于\\(\\mu\\)时，\\(H_0\\)大于等于\\(\\mu\\)。另一种单样本t检验是当\\(H_0\\)大于等于\\(\\mu\\)时。 接下来是双样本检验，其零假设是：两个样本来自同一个总体。我们可以用p值或t值检验来比较两个样本之间的差异。如果两个样本的数据是高度相关的，使用配对样本t检验。如果两个样本的数据是毫不相关即完全独立的，则使用独立样本t检验。 在R语言中，有多个函数可以用于t检验，这些函数包括在R-base包，也包括在其他的包中。我们将会介绍Bruce R包，它是为心理学数据分析而进行优化过的，更方便。 Bruce R包集成了很多底层R包，有机整合了其中的R函数，功能主要覆盖： 1.基础R编程（自动设置文件夹路径、一站式数据导入导出、数据匹配拼接等） 2.多变量计算（极简化计算多题项平均分/总分及反向计分、数值重新编码等） 3.信度与因素分析（量表信度分析、探索性因素分析EFA、主成分分析PCA、验证性因素分析CFA） 4.描述与相关分析（描述统计、频数统计、相关分析、相关系数差异性检验） 5.t检验与方差分析（单样本/独立样本/配对样本t检验、多因素被试间/被试内/混合设计方差分析ANOVA、简单效应检验与多重比较） 6.普通与多层回归分析（多种回归模型结果的完整报告与表格输出、多层线性模型HLM补充分析） 7.中介与调节效应分析（普通与多水平的中介效应、调节效应、有调节的中介效应等，包括简单斜率分析、条件中介作用、链式中介作用、跨层调节作用等） 8.统计与绘图辅助工具（总均值/组均值中心化处理、时间序列交叉相关分析与格兰杰因果检验、ggplot2绘图主题theme_bruce、颜色卡/配色方案展示等） ####解决问题 问题： 1. “在penguin数据中，参与者对于家的依恋水平是否小于/等于/大于均值水平(假设在总体水平上，人们对家庭的依恋水平(HOME)均值为3.5)？” 2. “在penguin数据中，男女生在亲密关系经验(ECR)中的得分是否存在显著差异？” 3. “在penguin数据中，参与者在不同时间的温度差异是否具有显著性？” library(bruceR) ## ## bruceR (v2023.9) ## Broadly Useful Convenient and Efficient R functions ## ## Packages also loaded: ## ✔ data.table ✔ emmeans ## ✔ dplyr ✔ lmerTest ## ✔ tidyr ✔ effectsize ## ✔ stringr ✔ performance ## ✔ ggplot2 ✔ interactions ## ## Main functions of `bruceR`: ## cc() Describe() TTEST() ## add() Freq() MANOVA() ## .mean() Corr() EMMEANS() ## set.wd() Alpha() PROCESS() ## import() EFA() model_summary() ## print_table() CFA() lavaan_summary() ## ## For full functionality, please install all dependencies: ## install.packages(&quot;bruceR&quot;, dep=TRUE) ## ## Online documentation: ## https://psychbruce.github.io/bruceR ## ## To use this package in publications, please cite: ## Bao, H.-W.-S. (2023). bruceR: Broadly useful convenient and efficient R functions (Version 2023.9) [Computer software]. https://CRAN.R-project.org/package=bruceR ## ## These packages are dependencies of `bruceR` but not installed: ## - ggtext, vars, phia, GPArotation ## ## ***** Install all dependencies ***** ## install.packages(&quot;bruceR&quot;, dep=TRUE) library(dplyr) library(tidyr) df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 首先载入所需的包。BruceR包载入后会有一大串提示信息，包括载入的包、主要函数和网址等。与先前操作相似，我们使用相对路径读取数据，可以查看前5行。 df.pg.mean &lt;- df.pg.raw %&gt;% dplyr::filter(sex &gt; 0 &amp; sex &lt; 3) %&gt;% dplyr::mutate(ECR_mean = rowMeans(select(., starts_with(&quot;ECR&quot;)),na.rm = T), HOME_mean = rowMeans(select(., starts_with(&quot;HOME&quot;)),na.rm = T), sex=as.factor(sex) ) %&gt;% dplyr::select(sex, ECR_mean, HOME_mean, Temperature_t1,Temperature_t2) 其次，我们需要进行数据预处理，计算家庭依恋的均值。这需要使用之前学过的数据预处理函数和方法，我们将使用管道进行操作。为了计算第二个问题，我们首先筛选出两个性别，即男性和女性。然后，我们对数据进行操作，计算两个变量的得分均值，用mutate生成两个新变量。我们使用命令na.rm来去除包含缺失值的数据，以避免出现错误。我们可以使用命令将性别转换为factor，以便在进行t-test时更容易分类。在使用R语言时，不同的函数可能会有微小的区别，因此我们需要注意这些细节。在选择数据时，应该只选取感兴趣的变量进行分析，如年龄、性别、亲密关系、问卷得分和温度测量值等。 进行t检验时，需要输入数据框、y变量和x变量，同时需要注意是否是配对样本和是否满足方向性。 问题一（单样本t检验）： bruceR::TTEST(df.pg.mean, &quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;&gt;&quot;) ## ## One-Sample t-test ## ## Hypothesis: one-sided (μ &gt; 3.5) ## t approximation invoked. ## Descriptives: ## ─────────────────────────── ## Variable N Mean (S.D.) ## ─────────────────────────── ## HOME_mean 1490 3.99 (0.73) ## ─────────────────────────── ## ## Results of t-test: ## ─────────────────────────────────────────────────────────────────────────────────────────────────── ## t df p Difference [95% CI] Cohen’s d [95% CI] BF10 ## ─────────────────────────────────────────────────────────────────────────────────────────────────── ## HOME_mean: (HOME_mean - 3.5) 25.97 1489 &lt;.001 *** 0.49 [0.46, Inf] 0.67 [0.63, Inf] 2.42e+119 ## ─────────────────────────────────────────────────────────────────────────────────────────────────── bruceR::TTEST(df.pg.mean, &quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;&lt;&quot;) ## ## One-Sample t-test ## ## Hypothesis: one-sided (μ &lt; 3.5) ## t approximation invoked. ## Descriptives: ## ─────────────────────────── ## Variable N Mean (S.D.) ## ─────────────────────────── ## HOME_mean 1490 3.99 (0.73) ## ─────────────────────────── ## ## Results of t-test: ## ────────────────────────────────────────────────────────────────────────────────────────────────── ## t df p Difference [95% CI] Cohen’s d [95% CI] BF10 ## ────────────────────────────────────────────────────────────────────────────────────────────────── ## HOME_mean: (HOME_mean - 3.5) 25.97 1489 1.000 0.49 [-Inf, 0.52] 0.67 [-Inf, 0.72] 6.84e-04 ## ────────────────────────────────────────────────────────────────────────────────────────────────── result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y=&quot;HOME_mean&quot;, test.value = 3.5, test.sided = &quot;=&quot;, file = &quot;./output/chp7/single_t.doc&quot;) }) writeLines(result.ttest, &quot;./output/chp7/single_t.md&quot;) # .md最整齐 mean difference代表两组数据的均值差异，Cohen's *d*是t-test的一个效应量。test side表示双尾还是单尾。test value是要检验的变量。factor variance表示是否对因素进行反转，digital参数指小数点后保留的位数，file参数用于保存结果到word文档。 使用capture output函数可以整理结果并复制到results.ttest变量中。将结果复制到名为results.ttest的变量中，并将结果写入一个名为single_t.md的文件中。在根目录中打开R4Psy项目文件，我们可以看到已经运行过的输出结果，其中包括一个符合心理学格式的三线表格，其中包含了均值、t值、自由度、p值、协方差和效应量等信息。这个表格非常干净。 问题二（独立样本t检验）： result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y=&quot;ECR_mean&quot;, x=&quot;sex&quot;) }) writeLines(result.ttest, &quot;./output/chp7/inde_t.md&quot;) 然后是独立样本t检验，比较男性和女性之间的差异。使用了ECR_mean作为样本得分，用性别作为分组变量。结果是two-side的，有描述性的结果，结果包括每组的n、均值和方差齐性检验。如果方差不齐性，可以使用校正方法。t检验的结果，包括t值、df值、p值、mean difference、Cohen’s d值。 问题三（配对样本t检验）： result.ttest &lt;- capture.output({ bruceR::TTEST(data=df.pg.mean, y = c(&quot;Temperature_t1&quot;, &quot;Temperature_t2&quot;), paired = T) #是否为配对样本t检验？默认是FALSE }) writeLines(result.ttest, &quot;./output/chp7/pair_t.md&quot;) 配对样本t检验也很简单，只需要将y变成两个值，然后使用t1和t2进行检验。 请大家注意，我们之前讲过如何管理和放置你的R-Project，可以避免路径错误。如果你不熟悉路径，可以从github上下载整个文件夹，打开R4psy文件夹中的R-Project文件。我们的助教已经在群里分享了GitHub链接。打开链接后，点击绿色按钮下面的“code”，再点击“download zip”，就可以下载整个文件夹。下载后解压，把文件夹放在一个完整的文件夹里。文件夹里有一个叫做“R4Psy.Rproj”的文件，双击它就可以打开Rstudio。 在console里输入代码时，如果按回车后，有时可能看到一个大于号“&gt;”或加号”+“。其中“+”表示代码没有输完，可能是缺少反括号或者某些代码没有输入完整。可以补上反括号或按退出键（“ESC”，一般在键盘的左上角）退出。Rstudio是一个好的IDE，可以帮助我们导航代码。在Rmarkdown里，代码块以chunk1、chunk2等命名，可以快速定位到代码框。有些chunk没有命名，只有编号。也可以通过outline来定位。 如果你想要快速运行代码，推荐使用快捷键control+回车，在Mac上是command+回车。也可以通过查找keyboard shortcut来找到更多快捷键。 在进行统计分析后，可以使用软件包reports来标准化报告结果，包括均值、标准差、t值、p值和Cohen’s d等。 7.0.2 方差分析 我们将使用BruceR和Tidyverse软件包，但需要注意，安装BayesFactor包可能会出现错误，因为它依赖于Rtools包。如果出现错误，可以在官方网站上搜索如何安装BayesFactor包。注意安装依赖包，否则可能会出现问题。在使用BruceR时，可能会遇到一些警告和问题，需要自己解决。 7.0.2.1 拟解决问题 “在match数据中，被试在匹配任务和不匹配任务上的反应时是否存在差异？” “在match数据中，被试在不同身份(self vs other)与不同效价(moral vs immoral)的条件组合下反应时是否存在差异？” df.match &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) %&gt;% # 拆分单元格内字符串 tidyr::separate(col=Shape, into=c(&quot;Valence&quot;,&quot;Identity&quot;), sep=&quot;(?&lt;=moral|immoral)(?=Self|Other)&quot;) %&gt;% dplyr::select(Sub, Valence, Identity, everything()) %&gt;% dplyr::filter(ACC == 1) %&gt;% # 只选择回答正确的数据 dplyr::filter(!is.na(RT)) %&gt;% # 剔除缺失值 # remove outliers below and above 3rd sd dplyr::filter(RT &gt; quantile(RT, 0.0015) &amp; RT &lt; quantile(RT, 0.9985)) %&gt;% dplyr::mutate(RT = as.numeric(RT)) %&gt;% dplyr::mutate(Valence = as.factor(Valence), Identity = as.factor(Identity)) df.match.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE)%&gt;% tidyr::separate(., col=Shape,into = c(&quot;Valence&quot;,&quot;Identity&quot;), sep = &quot;(?&lt;=moral|immoral)(?=Self|Other)&quot;)%&gt;% # 拆分单元格内字符串 dplyr::select(Sub,Valence,Identity,everything())%&gt;% dplyr::filter(ACC == 1) %&gt;% # 只选择回答正确的数据 dplyr::filter(RT &gt; quantile(RT, 0.0015) &amp; RT &lt; quantile(RT, 0.9985)) %&gt;% # remove outliers below and above 3rd sd dplyr::mutate(RT = as.numeric(RT)) %&gt;% dplyr::mutate(across(&quot;ACC&quot;, as.factor)) %&gt;% dplyr::mutate(Valence = as.factor(Valence)) %&gt;% dplyr::mutate(Identity = as.factor(Identity)) %&gt;% dplyr::filter(!is.na(RT)) #剔除缺失值 DT::datatable(head(df.match.raw, 10), fillContainer = TRUE, options = list(pageLength = 5)) 在数据清理时，可以先读取数据，然后进行管道操作，将Shape列分割成Valence和Identity两列，并选择Base、Valence、Identity和Everything列。然后进行数据清理，只选择反应正确的反应时间，并去掉缺失值和极端反应时间。我们使用了三个标准差的方法来去除极端值。我们将反应时间转换为数值型，将情感和身份转换为因子型。我们将所有操作应用于数据框df.match，这是经过预处理的数据。 df.match.mean &lt;- df.match.raw %&gt;% dplyr::group_by(Sub, Match) %&gt;% dplyr::summarise( n = n(), rt_sd = sd(as.numeric(RT), na.rm = T), rt_mean = mean(as.numeric(RT), na.rm = T), acc_mean=mean(as.numeric(as.character(ACC)),na.rm=T)#对于factor数据，先转成character，再变成numeric ) %&gt;% dplyr::ungroup() ## `summarise()` has grouped output by &#39;Sub&#39;. You can override using the `.groups` ## argument. 我们需要求出每个被试在匹配和不匹配任务上的均值，才能进行比较。为此，我们使用分组进行预处理，分组的目标是为了得到每个组的描述性统计，如RT的mean和SD，以及正确试次的个数。我们可以使用summarize快速得出这些统计信息。接着，我们可以使用t-test或方差分析来比较匹配和不匹配反应是否有差异。 问题一（单因素被试内设计）之单因素方差分析： #比较被试在匹配任务和不匹配任务上的反应时是否存在差异 #单因素被试内设计（长型数据） result.anova &lt;- capture.output({ df_within &lt;- df.match.mean%&gt;% bruceR::MANOVA(subID=&quot;Sub&quot;,#被试id dv=&quot;rt_mean&quot;,#因变量，因为是MANOVA，实际上因变量可以再后面设置很多个 within=&quot;Match&quot;) #设置条件，因素分析的条件 }) ## ## * Data are aggregated to mean (across items/trials) ## if there are &gt;=2 observations per subject and cell. ## You may use Linear Mixed Model to analyze the data, ## e.g., with subjects and items as level-2 clusters. writeLines(result.anova, &quot;./output/chp7/anova_1.md&quot;) # 若不符合球形假设要加上：sph.correction = &quot;GG&quot; 在方差分析中，我们可以使用BruceR包中的MANOVA()函数。参数分别是subID,dv和within，即可直接对其进行操作并输出结果。这几个参数符合心理学的命名规范。我们可以得到match和mismatch两种条件的描述性统计，包括f值、p值、eta2、pash2、95%置信区间和generalized eta2。因为这是一个完全被试内设计的分析，所以不需要进行方差齐性检验。 问题二（两因素被试内设计）之重复测量方差分析： df.match.mean &lt;- df.match %&gt;% dplyr::group_by(Sub,Sex, Valence,Identity) %&gt;% dplyr::summarise( n = n(), rt_sd = sd(as.numeric(RT), na.rm = T), rt_mean = mean(as.numeric(RT), na.rm = T) ) %&gt;% dplyr::ungroup() ## `summarise()` has grouped output by &#39;Sub&#39;, &#39;Sex&#39;, &#39;Valence&#39;. You can override ## using the `.groups` argument. 对于被试身份和效价这两个因素，我们可以直接从match中选出数值对，然后进行groupby和summarize操作。需要注意的是，我们现在是以subject、variance和identity为基础进行分析。 df.match.within &lt;- df.match.mean %&gt;% dplyr::select(-c(n, rt_sd)) %&gt;% dplyr::mutate(Valence = paste(&quot;A_&quot;, Valence, sep = &quot;&quot;),#将变量的名称进行修改转换，不生产新的变量 Identity = paste(&quot;B_&quot;, Identity, sep = &quot;&quot;)) %&gt;% # 将morality和identity组合名称起来生成一个新的变量&quot;Conds&quot; tidyr::unite(&quot;Conds&quot;, Valence:Identity, sep = &quot;&amp;&quot;,remove=TRUE) %&gt;% tidyr::pivot_wider(names_from = Conds, values_from = rt_mean) head(df.match.within) ## # A tibble: 6 × 6 ## Sub Sex `A_immoral&amp;B_Other` `A_immoral&amp;B_Self` `A_moral&amp;B_Other` ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7302 female 0.706 0.724 0.662 ## 2 7303 male 0.741 0.754 0.720 ## 3 7304 female 0.756 0.730 0.757 ## 4 7305 male 0.685 0.659 0.641 ## 5 7306 male 0.778 0.763 0.660 ## 6 7307 female 0.751 0.677 0.716 ## # ℹ 1 more variable: `A_moral&amp;B_Self` &lt;dbl&gt; #str(df.match.within) #被试在不同身份(self vs other)与不同效价(moral vs moral)的条件组合下反应时是否存在差异） result.anova &lt;- capture.output({ res_rmANOVA_1 &lt;- bruceR::MANOVA(data=df.match.within, dvs=&quot;A_immoral&amp;B_Other:A_moral&amp;B_Self&quot;, dvs.pattern=&quot;A_(.+)B_(.+)&quot;,#正则表达式 within=c(&quot;A_&quot;,&quot;B_&quot;))#表示2个主条件 }) ## ## Note: ## dvs=&quot;A_immoral&amp;B_Other:A_moral&amp;B_Self&quot; is matched to variables: ## A_immoral&amp;B_Other, A_immoral&amp;B_Self, A_moral&amp;B_Other, A_moral&amp;B_Self writeLines(result.anova, &quot;./output/chp7/anova_2.md&quot;) 如果我们有的是宽数据，我们可以先将宽数据转成长数据，然后选择变量并按照Bruce R的命名方式在前面加上a和b表示自变量a和自变量b。接着，我们可以使用unite函数将Valence和Identity合并成一个新的变量，再使用pivot wide将其转换为更宽的形式。这样，我们就可以看到每个条件下的性别、被试id和条件。 最后，我们将结果写成md格式，得到一个简洁的表格，其中包含F值、p值、partial eta square、generalized square和partial eta square的95%置信区间。长数据也可以使用同样的方法处理。 #被试在不同身份(self vs other)与不同效价(good vs bad)的条件组合下反应时是否存在差异） #head(df.match.mean) result.anova &lt;- capture.output({ res_rmANOVA_2 &lt;- bruceR::MANOVA(data=df.match.mean, dv=&quot;rt_mean&quot;, within=c(&quot;Valence&quot;,&quot;Identity&quot;), subID=&quot;Sub&quot;) }) ## ## * Data are aggregated to mean (across items/trials) ## if there are &gt;=2 observations per subject and cell. ## You may use Linear Mixed Model to analyze the data, ## e.g., with subjects and items as level-2 clusters. writeLines(result.anova, &quot;./output/chp7/anova_3.md&quot;) 简单效应分析： result.check &lt;- capture.output({ sim_eff_1 &lt;- res_rmANOVA_1 %&gt;% bruceR::EMMEANS(&quot;A_&quot;, by=&quot;B_&quot;) # 简单效应分析 }) writeLines(result.check, &quot;./output/chp7/check.md&quot;) #sim_eff_1 &lt;- res_rmANOVA_1 %&gt;% #EMMEANS(&quot;B_&quot;, by=&quot;A_&quot;) 和上一个同理，只是转换了不同的形式 除了基础分析方法，我们还可以使用EMMEANS进行简单效应分析和多重比较矫正等操作。通过输入之前的方差分析结果，我们可以使用EMMEANS a by b来进行简单效应分析，以比较在不同条件下的主效应是否存在差异。 我们要分析a在b的不同条件下的效应，包括other和self条件。在other条件下，a的效应是b条件的一个主效应，我们称之为简单效应。我们可以看到，在b的两个条件下，a的简单效应实际上应该有一个交互作用。在other条件下，它不显著，在self条件下，它很显著。 此外，我们还可以看到a的quantity，即在self条件下，a的主效应moral和immoral之间差异非常大，quantity大于0.8，是一个大的效应。 "],["lesson-8.html", "8 第八讲：如何进行基本的数据分析: 相关与回归 8.1 什么是相关 8.2 相关-代码实现 8.3 什么是回归", " 8 第八讲：如何进行基本的数据分析: 相关与回归 8.1 什么是相关 相关分析是一种统计技术，用于测量两个变量之间线性关系的强度和方向。它涉及计算相关系数，相关系数的取值为[-1, 1]，其中-1表示两个变量完全呈负相关，0表示无相关，1表示完全正相关。 Pearson(皮尔逊)相关 Pearson相关系数是最常用的方法之一，用于衡量两个变量之间的线性相关程度，取值范围为-1到1之间，其值越接近于1或-1表示两个变量之间的线性相关程度越强，而越接近于0则表示两个变量之间线性相关程度越弱或不存在线性相关性。皮尔逊相关系数适用场景是呈正态分布的连续变量，当数据集的数量超过500时，可以近似认为数据呈正态分布 Spearman(斯皮尔曼)相关 Spearman等级相关系数用于衡量两个变量之间的关联程度，但不要求变量呈现线性关系，而是通过对变量的等级进行比较来计算它们之间的相关性。 与皮尔逊相关系数相比，斯皮尔曼相关系数没有那些限制，比如要符合正态分布、样本容量要超过一定数量(比如30个). Kendall(肯德尔)相关 Kendall秩相关系数也用于衡量两个变量之间的关联程度，其计算方式与Spearman等级相关系数类似，但它是基于每个变量的秩来计算它们之间的相关性。 肯德尔相关系数，又称肯德尔秩相关系数，它也是一种秩相关系数，不过，它的目标对象是有序的类别变量，比如名次、年龄段等。 假想问题 在penguin数据中，参与者压力和自律水平的相关水平？ 8.2 相关-代码实现 # 检查是否已安装 pacman if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # 如果未安装，则安装包 # 使用p_load来载入需要的包 pacman::p_load(&quot;tidyverse&quot;, &quot;bruceR&quot;, &quot;performance&quot;) # 或者直接使用 easystats这个系列 pacman::p_load(&quot;tidyverse&quot;, &quot;bruceR&quot;, &quot;easystats&quot;) 检查工作路径 - 导入原始数据 # 检查工作路径 getwd() ## [1] &quot;/Users/hcp4715/Library/CloudStorage/OneDrive-Personal/Teaching/Grad_R_course/R4PsyBook/bookdown_files/Books/Book&quot; #读取数据 df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 首先需要将数据导入R中，并进行数据清洗和转换。可以使用Tidyverse包中的函数来选择和转换数据。在进行反向计分后，使用mutate函数来计算每个问卷的得分。 然后选择性别、压力和自我控制这三个变量，并使用Bruce R中的相关分析方法来计算它们之间的相关性。 需要注意的是，当有多个变量需要进行两两相关性分析时，需要进行P值的多重性校正。这里最后得到的是一个宽数据，需要在此基础上进行进一步的分析。 df.pg.corr &lt;- df.pg.raw %&gt;% dplyr::filter(sex &gt; 0 &amp; sex &lt; 3) %&gt;% # 筛选出男性和女性的数据 dplyr::select(sex, starts_with(&quot;scontrol&quot;), starts_with(&quot;stress&quot;)) %&gt;% # 筛选出需要的变量 dplyr::mutate(across(c(scontrol2, scontrol3,scontrol4, scontrol5,scontrol7, scontrol9, scontrol10,scontrol12,scontrol13, stress4, stress5, stress6,stress7, stress9, stress10,stress13), ~ case_when(. == &#39;1&#39; ~ &#39;5&#39;, . == &#39;2&#39; ~ &#39;4&#39;, . == &#39;3&#39; ~ &#39;3&#39;, . == &#39;4&#39; ~ &#39;2&#39;, . == &#39;5&#39; ~ &#39;1&#39;, TRUE ~ as.character(.))) ) %&gt;% # 反向计分修正 dplyr::mutate(across(starts_with(&quot;scontrol&quot;) | starts_with(&quot;stress&quot;), ~ as.numeric(.)) ) %&gt;% # 将数据类型转化为numeric dplyr::mutate(stress_mean = rowMeans(select(.,starts_with(&quot;stress&quot;)), na.rm = T), scontrol_mean = rowMeans(select(., starts_with(&quot;scontrol&quot;)), na.rm = T) ) %&gt;% # 根据子项目求综合平均 dplyr::select(sex, stress_mean, scontrol_mean) 使用head查看一下前五行 bruceR::Corr() results.Corr &lt;- capture.output({ bruceR::Corr(data = df.pg.corr[,c(2,3)], file = &quot;./output/chp8/Corr.doc&quot;) }) writeLines(results.Corr, &quot;./output/chp8/Corr.md&quot;) # .md最整齐 Bruce R默认采用pearson相关，也可以选择使用spearman或者kendall。如果有多个即两个以上的个变量，两两之间计算相关，往往还需要对P值进行多重校正。如果只有两个变量，就不需要了。 在保存文件时，要注意文件名和文件类型的后缀。 也可以用散点图来展示变量之间的相关性。 #绘制相关散点图 pairs(df.pg.corr[,c(2,3)]) RStudio有4个panel，最右下角的面板是用来画图的，有时运行画图命令之后图没有输出，可能就是由于这个面板的区域留得不够大，导致图无法呈现；这种情况下只需要把面板拖拽至合适的大小即可。 成功之后会自动输出一张图，图上是stress_mean和control_mean这两个变量的相关矩阵。 图的右侧是从-1到1的legend图例，颜色越深相关越高、颜色越浅相关越低；矩阵中的数字即为两个变量的相关系数，这里的值为0.05，可见这两个变量相关性很弱；如果相关显著，在相关系数的右上角会自动呈现星号，这个图中没有出现，表明二者相关并不显著。 结果也可以输出为word文档 但是需要精确地指出所需要的变量，也就是对应的columns。如果不指定，它会将所有变量两两组合计算相关；例如计算性别和一个连续变量之间的相关，但是这种情况下不能使用pearson。 如果一个二分变量（如性别）和一个连续变量进行相关分析，不应该使用pearson，应该用点二列相关，或者直接使用t检验。 在ezstates中有一个类似的包correlation，它的好处在于它会输出更多的信息，因此很适用于探索性的分析的阶段（这个时候对于变量之间的关系没有大概的了解，需要探索一下）。 它就会以一个可视的形式展现变量间的关系，便于快速发现哪两个变量之间相关变强，哪两个变量相对弱。 8.3 什么是回归 回归模型是通过对观测数据进行拟合来描述变量之间的关系。回归允许我们估计因变量如何随着自变量的变化而变化。 很多经典统计（T-test,方差分析,相关,回归分析）都有共同的技术，GLM。 广义线性模型（Generalized Linear Model，GLM）, 或者线性混合模型（Generalized Linear Mixed Model）,本质上就是Y=A+BX。通常会将Y作为预测项，有时候会在预测项上加上一个误差，这是可以扩展的，我们也可以假设他是一个非线性的关系，当它是线性的时候，我们实际上是在预测一个正态分布的均值，如果我们不是预测均值，我们可以通过一个转换，使用一个链接函数，转化后的参数仍然能用这种方法来组合预测，这个自变量或者因变量可以是非连续的变量。 ## 回归-代码实现 \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\\] 假想的问题 “在penguin数据中，我们希望找到参与者压力和自律水平关于性别的回归？” # 数据预处理 df.pg.lm &lt;- df.pg.corr %&gt;% # 将sex变量转化为factor类型 mutate(sex = as.factor(sex)) %&gt;% # 自变量为scontrol和sex group_by(scontrol_mean,sex) %&gt;% # 根据分组获得stress的平均值，。groups属性保留了之前的group_by summarise(stress_Mean = mean(stress_mean),.groups = &#39;keep&#39;) 希望找到压力，自律以及性别之间的关系。 在此之前，首先做了correlation。 在数据预处理阶段，把性别转换为factor类型,然后再把数据进行group by。 数据预处理之后,可以做一个探索分析，先用ggPlot画一个图，用不同的颜色分别表示男性和女性的数据。 绘制散点之后继续绘制趋势线时，使用了geom_smooth(), 这里自动使用了formula,y ~ x，表示用x来预测y。这里y即为strength， x即为self-control。两条线看起来是交叉的。图中呈现的粗略的结果支持在女性中存在相关, 在男性当中不存在，之后可以对它进行检验。 # 使用ggplot()画图 ggplot(df.pg.lm, aes(x = scontrol_mean, y = stress_Mean, color = sex)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + scale_color_discrete(name = &quot;Gender&quot;, labels = c(&quot;Female&quot;, &quot;Male&quot;)) + theme_minimal() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 直接使用lm linear model，它是base中的一个函数。这里是R中进行回归常用的公式写法, 也就是左边为因变量，右边为自变量, 运行后R就自动输出结果。 因此在R中因变量自变量要选择清楚。 这个例子中因变量是主观压力stress，三个自变量分别是sex性别、scontrol_mean、以及二者的交互作用（因为sex和scontrol_mean之间可能存在交互作用），那么代码就可以写成mod &lt;- lm(stress_Mean ~ scontrol_mean + sex + scontrol_mean:sex, df.pg.lm)。 如果后续需要做线性回归及相关的分析，需要掌握这种写法。 公式会作为第一个参数被输入到函数中，也可以更完善的补全它的argument，也就是formula=之后的内容，在这个例子中其他的argument被省略了、为默认值。 运行代码之后,线性回归的结果被保存在变量mod中，然后可以用mod_summary将结果提取出来。 # 建立回归模型 mod &lt;- lm(stress_Mean ~ scontrol_mean + sex + scontrol_mean:sex, df.pg.lm) # 使用bruceR::model_summary()输出结果 result.lm &lt;- capture.output({ model_summary(mod, std = T, file = &quot;./output/chp8/Lm.doc&quot;) }) writeLines(result.lm, &quot;./output/chp8/Lm.md&quot;) 输出的结果中包括显著性检验，可以看到selfcontrol、sex以及它们之间的交互作用是否显著。 整个模型的一个决定系数指这个模型解释了多少变异,adjusted是调整之后的数据。 number of observations 为53。 推荐一个很好的包：performance,它可以进行常用的统计模型分析,包括Anova、T-test等。它会检查模型的各个方面,比如posterior predictive check，inlinearity，方差齐性，共线性，正态性检验等等。 它可以告诉你数据是否符合做这个模型的假定的assumption。 它还可以进行模型比较,比如我们有模型1是有交互作用的,模型2是没有交互作用的，它可以对模型1和模型2进行比较，告诉你哪个模型能够更合理地解释数据的变异,可以用model comparison这个函数来对三个模型进行比较,这个在easy state里面有。 在这一部分,有两个内容需要强调。 首先是需要掌握公式如何写,尤其是对于第一次在R中接触回归模型的同学,需要注意这个固定的写公式套路。 其次是如果需要做简单的回归分析,那么使用LM就可以；在心理统计学中,一个变量对另外一个变量的预测作用、多个变量对一个变量的预测作用、变量为二分变量这些情况，LM都可以适用。 可以自己试着编写一条进行回归模型比较的函数，并把相关指标建议附在控制台的输出结果中 P_anova&lt;- function(model1, model2, file = NULL){ if(is.null(file)){ cat(&quot;\\033[1;32m Tips by P： AIC：Lower values indicate better fit for model comparison. BIC：Lower values indicate better fit for model comparison. liglik: The smaller the absolute value of logLik, the better the model fits. deviance: The smaller the deviance value, the better the model fit. Chisq: chi-square value(χ²) of likelihood ratio test. Pr(&gt;Chisq): P-value of likelihood ratio test. \\033[0m\\n&quot;) cat(&quot;\\033[1;32m Model Comparison Table \\033[0m\\n&quot;) result&lt;-anova(model1,model2) print_table(result) }else{ cat(&quot;\\033[1;32m Tips by P： AIC：Lower values indicate better fit for model comparison. BIC：Lower values indicate better fit for model comparison. liglik: The smaller the absolute value of logLik, the better the model fits. deviance: The smaller the deviance value, the better the model fit. Chisq: chi-square value(χ²) of likelihood ratio test. Pr(&gt;Chisq): P-value of likelihood ratio test. \\033[0m\\n&quot;) cat(&quot;\\033[1;32m Model Comparison Table \\033[0m\\n&quot;) result&lt;-anova(model1,model2) print_table(result) print_table(table, file = file) } } 可以自己试着写一条建立回归模型的函数，基lmerTest、bruceR和一些数据清理的包，这行代码可以输出两个模型（可以是：lm、glm（二项分布、泊松分布、高斯分布）、lmer）的方差分析表、回归系数、标准化系数、(随机效应)、随机效应的显著性检验、输出各自模型的三线表到word文档、两个模型之间的比较（AIC、BIC、LogLik以及似然比检验 /F 检验的结果）。Tips: compare.except指模型二与模型一相比剔除的解释变量，暂不支持对多层线性模型随机斜率的比较。 P_regress&lt;- function(formula, data, family = NULL, compare.except = NULL, digits = 3, nsmall = digits, robust = FALSE, cluster = NULL, test.rand = FALSE, file1 = NULL, file2 = NULL, file3 = NULL){ if(is.null(compare.except)){ if (class(formula) == &quot;formula&quot; &amp; grepl(&quot;\\\\|&quot;, deparse(formula))){ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) model1 = lmer(formula = formula, data = data) if(!is.null(file1)){ print_table(model1,file = file1)} ranova = ranova(model1) cat(&quot;\\033[1;37m The significance test of random effects \\033[0m\\n&quot;) print_table(ranova) }else if(class(formula) == &quot;formula&quot; &amp; grepl(&quot;family&quot;, deparse(formula))){ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) if(family == &quot;binomial&quot;){ model1 = glm(formula = formula, data = data,family = binomial()) if(!is.null(file1)){ print_table(model1, file = file1)} }else if(family == &quot;gaussian&quot;){ model1 = glm(formula = formula, data = data,family = gaussian()) if(!is.null(file1)){ print_table(model1, file = file1)} }else{ model1 = glm(formula = formula, data = data,family = poisson()) if(!is.null(file1)){ print_table(model1, file = file1)} } }else{ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) model1 = lm(formula = formula, data = data) if(!is.null(file1)){ print_table(model1, file = file1)} } }else{ if (class(formula) == &quot;formula&quot; &amp; grepl(&quot;\\\\|&quot;, deparse(formula))){ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) model1 = lmer(formula = formula, data = data) if(!is.null(file1)){ print_table(model1, file = file1)} ranova = ranova(model1) cat(&quot;\\033[1;37m The significance test of random effects \\033[0m\\n&quot;) print_table(ranova) # model1 ranova string&lt;- formula_paste(formula) string1&lt;-gsub(&quot; &quot;, &quot; \\\\\\\\+ &quot;, compare.except) new_string &lt;- gsub(string1, &quot;&quot;, string) formula = as.formula(new_string) model2 = lmer(formula = formula, data = data) regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model2 ranova = ranova(model2) cat(&quot;\\033[1;37m The significance test of random effects \\033[0m\\n&quot;) print_table(ranova) if(!is.null(file2)){ print_table(model2, file = file2)} #model2 ranova P_anova(model1, model2,file = file3)#模型比较 }else if(class(formula) == &quot;formula&quot; &amp; grepl(&quot;family&quot;, deparse(formula))){ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model1 if(family == &quot;binomial&quot;){ model1 = glm(formula = formula, data = data,family = binomial()) if(!is.null(file1)){ print_table(model1, file = file1)} string&lt;- formula_paste(formula) string1&lt;-gsub(&quot; &quot;, &quot; \\\\\\\\+ &quot;, compare.except) new_string &lt;- gsub(string1, &quot;&quot;, string) formula = as.formula(new_string) regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model2 if(!is.null(file2)){ print_table(model2, file = file2)} P_anova(model1, model2,file = file3) }else if(family == &quot;gaussian&quot;){ model1 = glm(formula = formula, data = data,family = gaussian()) if(!is.null(file1)){ print_table(model1, file = file1)} string&lt;- formula_paste(formula) string1&lt;-gsub(&quot; &quot;, &quot; \\\\\\\\+ &quot;, compare.except) new_string &lt;- gsub(string1, &quot;&quot;, string) formula = as.formula(new_string) regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model2 if(!is.null(file2)){ print_table(model2, file = file2)} P_anova(model1, model2,file = file3) }else{ model1 = glm(formula = formula, data = data,family = poisson()) if(!is.null(file1)){ print_table(model1, file = file1)} string&lt;- formula_paste(formula) string1&lt;-gsub(&quot; &quot;, &quot; \\\\\\\\+ &quot;, compare.except) new_string &lt;- gsub(string1, &quot;&quot;, string) formula = as.formula(new_string) regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model2 if(!is.null(file2)){ print_table(model2, file = file2)} if(!is.null(file3)){ P_anova(model1, model2,file = file3)} } }else{ regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) model1 = lm(formula = formula, data = data) if(!is.null(file1)){ print_table(model1, file = file1)} string&lt;- formula_paste(formula) string1&lt;-gsub(&quot; &quot;, &quot; \\\\\\\\+ &quot;, compare.except) new_string &lt;- gsub(string1, &quot;&quot;, string) formula = as.formula(new_string) regress(formula = formula, data = data, family = family, digits = digits, nsmall = digits, robust = robust, cluster = cluster, test.rand = test.rand) #model2 if(!is.null(file2)){ print_table(model2, file = file2)} P_anova(model1, model2,file = file3) } } } 可以自己试着编写一条回归分析中简单斜率检验的函数，并输出简单斜率检验图。适用于绝大多数调节模型。 P_simpleslopes&lt;- function(y,x,mod,mod2 = NULL, data, main.title = &quot;Simple Slope Test Graph&quot;, way = 2){ P_dif&lt;- function(x){ return(length(unique(x))) } if(is.null(mod2)){ datamod = unlist(select(data, mod)) if(P_dif(datamod) == 2){ a = paste(x,mod, sep = &quot; * &quot;) formula = paste(y,a,sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 1 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(formula), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;x&quot; = x, &quot;mod&quot; = mod) model = lm(formula = y ~ x * mod, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, jnplot = T) data1 = result$slopes names(data1)[1] = mod names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m Simple slope:\\033[0m\\n&quot;) print_table(data1) interact_plot(model, pred = x, modx = mod, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) }else{ a = paste(x,mod, sep = &quot; * &quot;) formula = paste(y,a,sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 1 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(formula), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;x&quot; = x, &quot;mod&quot; = mod) model = lm(formula = y ~ x * mod, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, jnplot = T) data1 = result$slopes a = data1[,1] a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data1[,1] = a names(data1)[1] = mod names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m Simple slope:\\033[0m\\n&quot;) print_table(data1) interact_plot(model, pred = x, modx = mod, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) } }else{ if(way == 3){ datamod = unlist(select(data, mod)) datamod2 = unlist(select(data, mod2)) if(P_dif(datamod) == 2){ if(P_dif(datamod2) == 2){ a = paste(x, mod, mod2,sep = &quot; * &quot;) c = paste(y , a, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 3 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 0.00 :\\033[0m\\n&quot;) print_table(data1) names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 1.00 :\\033[0m\\n&quot;) print_table(data2) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) }else{ a = paste(x, mod, mod2,sep = &quot; * &quot;) c = paste(y , a, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 3 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) data3 = data.table(slopes[[3]]) names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; sd = sd(unlist(select(data, mod2))) mean = round(mean(unlist(select(data, mod2))),3) b = round(mean - sd,3) c = round(mean + sd,3) cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(b), &quot;(- 1 SD):\\033[0m\\n&quot;) print_table(data1) names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(mean), &quot;(Mean):\\033[0m\\n&quot;) print_table(data2) names(data3)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data3)[2] = &quot;Effect&quot; names(data3)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(c), &quot;(+ 1 SD):\\033[0m\\n&quot;) print_table(data3) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) } }else{ if(P_dif(datamod2) == 2){ a = paste(x, mod, mod2,sep = &quot; * &quot;) c = paste(y , a, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 3 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) a = unlist(data1[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data1[,1] = a names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 0.00 :\\033[0m\\n&quot;) print_table(data1) a = unlist(data2[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data2[,1] = a names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 1.00 :\\033[0m\\n&quot;) print_table(data2) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) }else{ a = paste(x, mod, mod2,sep = &quot; * &quot;) c = paste(y , a, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 3 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) data3 = data.table(slopes[[3]]) a = unlist(data1[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data1[,1] = a names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; sd = sd(unlist(select(data, mod2))) mean = round(mean(unlist(select(data, mod2))),3) b = round(mean - sd,3) c = round(mean + sd,3) cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(b), &quot;(- 1 SD):\\033[0m\\n&quot;) print_table(data1) a = unlist(data2[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data2[,1] = a names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(mean), &quot;(Mean):\\033[0m\\n&quot;) print_table(data2) a = unlist(data3[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data3[,1] = a names(data3)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data3)[2] = &quot;Effect&quot; names(data3)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(c), &quot;(+ 1 SD):\\033[0m\\n&quot;) print_table(data3) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) } } }else{ datamod = unlist(select(data, mod)) datamod2 = unlist(select(data, mod2)) if(P_dif(datamod) == 2){ if(P_dif(datamod2) == 2){ a = paste(x, mod, sep = &quot; * &quot;) b = paste(x, mod2, sep = &quot; * &quot;) ab = paste(a, b, sep = &quot; + &quot;) c = paste(y , ab, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 2 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod + x * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 0.00 :\\033[0m\\n&quot;) print_table(data1) names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 1.00 :\\033[0m\\n&quot;) print_table(data2) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) }else{ a = paste(x, mod, sep = &quot; * &quot;) b = paste(x, mod2, sep = &quot; * &quot;) ab = paste(a, b, sep = &quot; + &quot;) c = paste(y , ab, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 2 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod + x * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) data3 = data.table(slopes[[3]]) names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; sd = sd(unlist(select(data, mod2))) mean = round(mean(unlist(select(data, mod2))),3) b = round(mean - sd,3) c = round(mean + sd,3) cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(b), &quot;(- 1 SD):\\033[0m\\n&quot;) print_table(data1) names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(mean), &quot;(Mean):\\033[0m\\n&quot;) print_table(data2) names(data3)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data3)[2] = &quot;Effect&quot; names(data3)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(c), &quot;(+ 1 SD):\\033[0m\\n&quot;) print_table(data3) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) } }else{ if(P_dif(datamod2) == 2){ a = paste(x, mod, sep = &quot; * &quot;) b = paste(x, mod2, sep = &quot; * &quot;) ab = paste(a, b, sep = &quot; + &quot;) c = paste(y , ab, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 2 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod + x * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) a = unlist(data1[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data1[,1] = a names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 0.00 :\\033[0m\\n&quot;) print_table(data1) a = unlist(data2[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data2[,1] = a names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) = 1.00 :\\033[0m\\n&quot;) print_table(data2) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) }else{ a = paste(x, mod, sep = &quot; * &quot;) b = paste(x, mod2, sep = &quot; * &quot;) ab = paste(a, b, sep = &quot; + &quot;) c = paste(y , ab, sep = &quot; ~ &quot;) cat(&quot;\\033[37m PROCESS Model Code : 2 (Hayes, 2018; www.guilford.com/p/hayes3)\\033[0m\\n&quot;) cat(&quot;\\033[34m- Outcome (Y) :&quot;, paste(y), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Predictor (X) :&quot;, paste(x), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 1 (M) :&quot;, paste(mod), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Moderator variable 2 (M) :&quot;, paste(mod2), &quot;\\033[0m\\n&quot;) cat(&quot;\\033[34m- Formula :&quot;, paste(c), &quot;\\033[0m\\n&quot;) data = rename(data, &quot;y&quot; = y, &quot;mod&quot; = mod, &quot;x&quot; = x, &quot;mod2&quot; = mod2) model = lm(formula = y ~ x * mod + x * mod2, data = data) result&lt;- sim_slopes(model, pred = x, modx = mod, mod2 = mod2, jnplot = T) slopes = result$slopes data1 = data.table(slopes[[1]]) data2 = data.table(slopes[[2]]) data3 = data.table(slopes[[3]]) a = unlist(data1[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data1[,1] = a names(data1)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data1)[2] = &quot;Effect&quot; names(data1)[6] = &quot;t&quot; sd = sd(unlist(select(data, mod2))) mean = round(mean(unlist(select(data, mod2))),3) b = round(mean - sd,3) c = round(mean + sd,3) cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(b), &quot;(- 1 SD):\\033[0m\\n&quot;) print_table(data1) a = unlist(data2[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data2[,1] = a names(data2)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data2)[2] = &quot;Effect&quot; names(data2)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(mean), &quot;(Mean):\\033[0m\\n&quot;) print_table(data2) a = unlist(data3[,1]) a = round(a, 3) a[1] = paste(a[1], &quot;(- SD)&quot;, sep = &quot; &quot;) a[2] = paste(a[2], &quot;(Mean)&quot;, sep = &quot; &quot;) a[3] = paste(a[3], &quot;(+ SD)&quot;, sep = &quot; &quot;) data3[,1] = a names(data3)[1] = paste(mod,&quot;(mod)&quot;,sep = &quot; &quot;) names(data3)[2] = &quot;Effect&quot; names(data3)[6] = &quot;t&quot; cat(&quot;\\033[1;37m While&quot;,paste(mod2), &quot;(mod2) =&quot;,paste(c), &quot;(+ 1 SD):\\033[0m\\n&quot;) print_table(data3) interact_plot(model, pred = x, modx = mod, mod2 = mod2, interval = TRUE, x.label = x, y.label = y ,colors = &quot;seagreen&quot;,main.title = main.title) } } } } } "],["lesson-9.html", "9 第九章：如何进行基本的数据分析: 中介分析 9.1 process", " 9 第九章：如何进行基本的数据分析: 中介分析 9.1 process 今天我们会讲解SEM的一些简单的操作和画图，画图的部分甚至可以掰开讲：怎么画一个直方图怎么画一个展示反应时的图，箱线图和小提琴图，以及云雨图。 在接下来的课程中，第11章想要达到一个目标是我们的能够做出来图可以符合格式，大家投稿的时候就可以直接用。第12章的话实际上就是把第10章和第11章的内容进行结合，我们会教授一个papaja包，这个包适合我们APA格式的一个写作。如果我们能够把自己的代码和数据和文字全部整合到一个东西里面去，这样的话就可以直接生成一个APA的手稿，这是12章我们想介绍的。第十三章我们原本想讲github的使用，但是考虑到github可以在后续的学习中反复使用练习，我认为可以移动到下周来讲，然后第14章有一些干货的内容，比如如何对效应量进行综合，这是在meta analysis常用的工作如何进行样本量的规划，这里面最重要的是power analysis。 这里面可能还会涉及到一些大家以前从来没有——至少我在读研究生的时候从来没有的一个东西就是我们如何在计划研究的时候就把整个代码写出来。我们一般来说是自己先有了数据，然后再去写代码，我们现在我们课题组慢慢的做法是变成一开始就做预注册，做完预注册了以后，就会自己开始写一些伪数据，我们叫假数据，它的结构是跟我们的实验的设计是一模一样的，那么这个时候我们就开始用这样的假数据把自己分析数据的代码就写完，也就是在你没有收集数据之前， 就可以把代码写完。 其实我用SEM用的很少，主要都是认知心理学舒颜，神经成像之类的东西，很少去做问卷相关的研究。但是我其实对传统的SEM很感兴趣。这里推荐一些经典的文章，比如Baron和Kenny1986年关于中介调节分析的文章。 那么process是Andrew Hayes，引进的一个很重要的一个工具包，这个工具包某种程度上解放了绝大部分人，他的引用量也非常高 那么process这个SPSS插件出来以后，应该是给很多人都非常大的帮助。 是2012年左右的时候，他开始还是小的一些工具包，后来就变成了一个插件，后来就越来越好用。大家可以看到， 整个R的这个生态体系 它其实是一个后起之秀，即便如此 因为R是一个开放的生态系统 所以只要有足够的人对它感兴趣的话 很快它就能够迅速的发展。 首先是导入这两个包，我们用之前一样的方式来去安装，我建议大家就是现在在讲课的时候，如果手头上有这个代码的话 可以把这个地方先运行一下，因为你可能需要安装新的包。 # 检查是否已安装 pacman if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # 如果未安装，则安装包 # 使用p_load来载入需要的包 pacman::p_load(&quot;tidyverse&quot;, &quot;bruceR&quot;, &quot;performance&quot;, &quot;lavaan&quot;, &quot;lavaanPlot&quot;) 导入我们的数据 df.pg.raw &lt;- read.csv(&#39;./data/penguin/penguin_rawdata_new.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) %&gt;% dplyr::select(., age, language, avgtemp,socialdiversity, DEQ, starts_with(&quot;ALEX&quot;), starts_with(&quot;ECR&quot;), starts_with(&quot;HOME&quot;), starts_with(&quot;KAMF&quot;), starts_with(&quot;SNI&quot;), ) # 这些包含原始题目的数据集，将在SEM lavaan中直接使用 刚才助教应该发给大家 大家把它保存在我们原来这个data Pangolin里面 然后叫Pangolin raw data new 为什么用这个新的数据呢 因为它这里面是有一个 有一个新有一个变量 我们原来那个数据里面没有包含 那么假如我们对某几个问卷感兴趣 我们就把它都选出来 然后把其他的这个变量都忽略掉 那么我们这里大概就是要做两个工作 一个工作就是我们要演示两个工作 一个工作就是我们重复一下 艾特曼在2018年发表的 Clever Psychology那篇文章中的一个分析 也就是我们这个数据来源 Pangolin的数据来源 它本来就是在2018年那个文章中 是以最主要的一个研究性的文章对吧 那么我2019年那个文章 实际上是把这个数据进行一个描述 那么第二个工作就是说 我们可不可以对一些问卷来 对它的这个问卷的结构进行一个确认 那么采用CFA的方式 那么或者我们对不同的问卷进行的关系对吧 有没有这个中介调节 采用ICM的方式来做一个分析 这大概就是我们要展示的这几个工作 这个地方是一个数据运输里的过程 就是我们怎么去求 那么在这个 我们这次增加里面不仅仅有一个 我可以给大家看一下 我们这个数据里面 不仅仅增加了一个 就是叫full data 还有一个new data 这个new data是我们对full data进行处理的 然后还有一个codebook 就是关于这个应该是最 就是数据最全的一个 但是它都是原始数据 这个是最全的一个数据 那么刚才孟征发给大家那个 亚述包里面应该有 那么我们还有一个codebook 就是对这个数据本身的一个描述 这个数据它的每一个column 代表的是什么东西 那么这里的有一些数据是没有的 我们把它给去掉了 因为它可能涉及到背后的隐私的问题 那么这里会涉及到一些问卷 比如说这个叫做Alex 它实际上是一个关于 那个肃清障碍的一个问卷 它的这个参考文献是在这 就是叫做Toronto Laximedia 这个我还不知道什么 Alex Thymia Scare 然后这个ERC是关于这个情绪的 和attachment的一个问卷 这个home就是说对家的依恋 是一个互联网出现以后 也不是互联网出现 就是也是一个跟这个依恋相关的一个问卷 那么这个KAMF是一个新的问卷 它实际上是关于人的 会不会产生这种感动的一个情绪 那么我们就是说在process里面 它实际上是一个简化的一个SEM 它只需要对我们变量的题目内部求一个平均分 然后这样就可以了 那么它不需要去了解 这个每一个问卷里面的item 和它的维度 以及和整个问卷是不是有对应的 以及它的loading是什么 那么为什么说它是一个简化的模型呢 这里可能涉及到一个问题 就是说我们到底 当我们用总分代表一个量表得分的时候 它代表的到底是什么 它实际上代表的就是每一个item在这个 维度上面的loading完全是1对吧 它完全是相等的 这是一个非常强的assumption 然后也是一个非常简化的一个模型 那么通过SEM的一些CFA的分析的话 我们其实可以更好的估计它在每个潜变量 就是我们的一个问卷它可能是一个测量了一个或者多个 潜变量的一个问卷一个工具对吧 那么当我们用总分代表一个 潜变量或者一个维度的得分的时候 我们认为所有的item在维度上面或者潜变量 上的loading对吧 这个负荷都是1 但是用LCM的话我们可以把更加精准的建构出来 对于假如说我们要用process对吧 那我们就直接就把每一个维度上的得分求出来 当我们这里没有用process 我们用process没有做他们这些问卷的一个处理 而是去复制了Pandroid data里面的结果 感觉我应该需要展示一下原来那个论文才对 一会儿增加一下 那么在原来这个论文里面 它大概就是有这么一个关系的一个结果 那么我可以简单的跟大家说一下 我们这个项目对吧 原来这个数据项目叫做Human Computing Project Computing是什么是企鹅对吧 人类企鹅计划 它为什么要起这么一个名字呢 它就是觉得我们人类就像企鹅一样 有一个共同的一个体温调节 因为企鹅它生活在一个非常寒冷的地方对吧 它经常就是为了降低它取暖的能耗 它会很多企鹅在一起群聚到一起 这样的话就是说每个在一群里面 每个企鹅它都能够保持着一个相对恒定的体温 但是对于这个群体当中的每一个个体来说 它的能耗是比较小的 大家知道我们要维持一个恒定的体温的话 我们是要去燃烧一些卡路里对吧 那么如果说我们只有一个个体的话 它维持一个恒定的体温所需要的卡路里是非常多的 这样对于人类来说 现在好像卡路里是一个多余的东西 但是对于动物来说 当你在野外的话 其实你要去获得能源获得食物 本身就是一个有动物意志在做的一个事情对吧 所以它需要去尽量减少这个能耗 所以它就会在一起形成这么一个机制 那么这个Human Pending Project想说 就是说我们在人类的身上是不是也找到这种 我们做哺乳动物在演化当中形成的这种 群体体温条件的一些我们说一些traces 就是我们在演化当中 我们现在在现代化社会里面对吧 但是我们的身心上面可能还是遗留了一点点 我们原来做哺乳动物 通过群体来调节我们体温的这些遗迹对吧 所以这是这个项目的一个总的一个大胆理论 那么在这个过程当中 它做了一个什么工作呢 就是测量了很多问卷 并且测量了每个人的一个核心体温 这个核心体温就是说我们自己 Cold Body Temperature 这个时候在文章里面它是用CDP 就是Cold Body Temperature 我们这里是用Belian Temperature 就是Average Temperature 那么这个Cold Body Temperature是什么 就是我们身体核心所需要的一个温度 身体的核心是哪些呢 基本上就是内脏对吧 也就是说我们如果内脏不能够保证 这个恒定的体温的话 基本上就会直接影响到你的生命的安危 包括你的大脑 所以对哺乳动物来说最重要的 其实有的时候像比如说手对吧 你的手和脚 它即便失温了之后 它可能也不会影响你的生存 然后可能手动坏了对吧 有的人他比方说就结织掉了 那么它是会影响 但是它不会影响生存 它还是可以继续活下来对吧 但是如果你的核心体温 如果你不能保持恒定的温度的话 那就就是30的一个问题了 所以最感兴趣的 就在这个语言内容中 最感兴趣就是核心体温的温度 那么我们想如果说想要知道这个人 他我们人类上是不是遗留了这种 群体调节体温的遗迹的话 那我们应该关注的是什么 就是我们的社会关系对吧 你跟社会群体关系相关的一些变量 它会不会影响体温 这就是他最关系的 Hans Eisenberg最关系的一个问题 所以当时他测了很多跟社会关系 社会网络相关的一些变量 就比方说你有 你在过去的一个月当中对吧 你跟多少人联系 然后你比方说平时上班的时候 跟多少人联系 下班之后跟多少人联系 周末跟多少人联系 跟你的父母 跟你的你是不是跟你的 比方说partner对吧 住在一起等等 就有很多这样的问题 那么通过这个问题 他可以得到三个 社会关系网络的一个指标 一个指标就是叫做social diversity 就是你的社交网络 是不是很多元化 还是说你的社交网络就很单一 然后你就只跟父母 跟家人交往对吧 那你的这个社交网络就很单一 然后你有家人有同事 然后还有你自己比方说 你有很多个兴趣爱好对吧 你跳舞有跳舞的朋友 你唱歌有唱歌的朋友 然后你下棋有下棋的朋友 然后等等等等 如果说你的这个 关系网络非常的 就是不同的 在不同的场合跟不同人交往对吧 这样的话就会形成一个 很diverse的一个social network 那么这个social diversity 就指的是这个东西 然后他也测量了比方说 你的身高体重等等等等 然后还测每个人他和赤道说 隔的这个距离 the distance from the equator 就是你赤道这个距离 因为你赤道隔得越远的话 就意味着你这个纬度越高对吧 纬度越高 就意味着当地的气温本身是越低的 "],["lesson-12.html", "10 第十二讲：从分析到手稿 10.1 通过Papaja撰写论文", " 10 第十二讲：从分析到手稿 10.1 通过Papaja撰写论文 经过之前的学习，同学们熟悉了Rmarkdown格式的书写和使用，并了解其保存形式为.rmd文件。Rmarkdown文件包含文字、代码以及对代码的编译，能够方便地记录丰富的内容，并且可以输出为html等多种格式。于是，就有人想到一站式的解决方案：直接将Rmarkdown输出为pdf文档或word文档，并通过在Rmarkdown文件中的代码对文字图片进行排版以符合出版要求。本节课即将讲到的Papaja包适用于心理学手稿的准备，符合APA第6版的版式标准。 10.1.1 Part1: Papaja包的安装 我们为本堂课准备了一个.rmd文件，如果papaja包安装成功后，打开该文件并点击knit按钮，在相同工作目录下将生成一个与.rmd相同文件名的pdf文件，该文件即为APA论文格式的例子。 注意的是，在安装papaja时可能会存在的几个问题： （1）安装速度过慢。 解决方法：开启*梯*子*下载。 （2）knit过程中提示需要更新包，点击确认后仍存在相同提示。 解决方法：手动更新该R包，例如找到该包卸载后重新下载等。 （3）有些包需要安装在用户名下的某个文件夹，出现中文目录。 解决方法：将Windows用户名改成英文。 安装完成后我们进入papaja语法的学习。 10.1.2 Part2: Papaja语法格式 （1）YAML头文件 在所有RMarkdown文件中的头部都会有一个YAML头模块，利用—分隔开，包含标题、作者、摘要等各种信息，同时该部分支持markdown语法进行编写。 在papaja的模板中，通讯作者默认数量为一个。affiliation在作者栏中编号，在下方统一存放。authornote为APA格式风格内容，用于展示该论文之前的故事（如参与哪些报告、是否为毕业论文等）。 关于参考文献，本例包含了两个.bib文件，chapter_12-r-references.bib与chapter_12-citation.bib，可以从文献管理软件中导出，papaja将在你引用参考文献时自动从这些文件中查找并使用。 linenumbers属性是一个较为常用的属性，能够为文献提供行标，方便审稿人指出需要修改的位置。 --- title : &quot;Lecture 12: Preparing journal artical with *papaja*&quot; shorttitle : &quot;papaja&quot; author: - name : &quot;Hu Chuan-Peng&quot; affiliation : &quot;1&quot; corresponding : yes # Define only one corresponding author address : &quot;#122 Ninghai Rd, Gulou District, Nanjing&quot; email : &quot;hcp4715@hotmail.com&quot; role: # Contributorship roles (e.g., CRediT, https://casrai.org/credit/) - &quot;Conceptualization&quot; - &quot;Writing - Original Draft Preparation&quot; - &quot;Supervision&quot; - name : &quot;All Students&quot; affiliation : &quot;1,2&quot; role: - &quot;Writing - Original Draft Preparation&quot; - &quot;Writing - Review &amp; Editing&quot; affiliation: - id : &quot;1&quot; institution : &quot;Nanjing Normal Unviersity&quot; - id : &quot;2&quot; institution : &quot;Collaborators&#39; Affiliations&quot; authornote: | Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line. Author Note: This is for demonstration only. abstract: | Psychological science has encountered a serious replication crisis. To make self-correction of the field, researchers actively reform the current practices and increase the opennes, transparency, and reproducibility of studies in the field. Using R language for data analyses is recommended by many. With increasingly emphases on computational reproduciblity, *papaja* was developed to combine data analysis and manuscript preparation. The current chapter aims to demonstrate how to use *papaja*. We will introduce the package and key elements of the it. After the lecture, we expected students able to create an example APA manuscript using open data or examplary data we had provided at the beginning of the class. This demo and practice will further enhance the student&#39;s experience in computational reproducibility. By spreading the ideas of reproducbility and teaching papaja, this class will increase the computational reprodcubility. &lt;!-- https://tinyurl.com/ybremelq --&gt; keywords : &quot;Reproducibility, R, Teaching, Demonstration&quot; wordcount : &quot;X&quot; bibliography : - &quot;chapter_12-r-references.bib&quot; - &quot;chapter_12-citation.bib&quot; floatsintext : no linenumbers : yes draft : no mask : no figurelist : no tablelist : no footnotelist : no classoption : &quot;man&quot; output : papaja::apa6_pdf --- （2）包和文件的调用: 在定义YAML头文件之后，需要进行检查是否该文件已经调用了需要使用的包，以及是否包含了需要的文件（例如.bib格式文件）。 （3）参考文献的引用方式 如例子中(R-papaja?)，这是Papaja的引用格式，包含中括号[]、(符号以及参考文献标识?)。参考文献标识为.bib文件中@后面的内容。这种标识转换后为APA标准引用，以括号的形式引用文献。 还有我们通常使用的另一种引用，即在句子中直接引用文章作者，此时只需要略去大括号即可，如@Hu_2020 10.1.3 Part3: 正文的撰写 了解并添加了YAML头文件和参考文献后，就开始撰写正文。 （1）数据的处理： 在markdown中创建chunks来存放数据处理代码 推荐命令，用于清理环境变量 # empty the global env rm(list = ls()) df.m.basic &lt;- read.csv(&#39;./1012-lesson12/data.csv&#39;, header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 数据处理结束后，在撰写文章时就可以通过**`**来引用数据，例如，** \\ r df.m.basic\\(Age_mean \\` **就会索引df.m.basic中的Age： 引用公式的方式也非常方便，在两个\\$中间键入符号即公式就可，如输入`\\)\\pm\\(`就会得到\\)$。 引用R包：通过如下语句` r cite_r(“chapter_12-r-references.bib”, pkgs = c(“afex”, “emmeans”, “ggplot2”, “dplyr”, “tidyr”, “patchwork”, “r-base”), withhold=FALSE) ` 即可引用，withhold=FALSE表示选中的包为白名单。上文中的七个包为最经常使用的包，可根据数据的分析代码进行修改引用。 (2)斜体书写：在绘图中将图题部分设置斜体，可用如下代码： title &lt;- expression(paste(&quot;Senesitivity (&quot;, italic(&quot;d&#39;&quot;), &quot;)&quot;)) 此时将输出Sensitivity(d)。 (3)拼图操作：将画好的多图进行拼接，可以使用patchwork包 library(patchwork) load(&quot;./1012-lesson12/p1_rt.rdata&quot;) load(&quot;./1012-lesson12/p1_dp.rdata&quot;) p1_dp + p1_rt + plot_annotation(tag_levels = &#39;A&#39;) + plot_layout(nrow = 1, byrow = TRUE, guides = &#39;collect&#39;) #plot_annotation属性：设置图片序号为A，B，C...或1，2，3... #plot_layout属性：图片怎么排布；guides——图例是否统一放置 图片引用 使用如下格式\\@ref(fig:plot1) 即可得到?? 数据分析： 注意：bruceR和papaja可能会因为输出导致冲突，在papaja中分析数据建议回归更原始的包，如在进行ANOVA分析时使用afex函数 10.1.4 总结 papaja的好处是，数据、分析、格式、参考文献等都储存在同一个工作项目中，能够更整齐更方便的撰写文章。 10.1.4.1 小作业 请大家fork我们的bookdown文件 https://github.com/hcp4715/R4PsyBook 做一个小的更改，并pull request。 "],["lesson-13.html", "11 第十三章：效应量和元分析 11.1 效应量简介 11.2 算法实现 11.3 小练习 11.4 元分析简介 11.5 元分析实现", " 11 第十三章：效应量和元分析 11.1 效应量简介 我们今天讲的内容，是在本科心理统计学没有讲的一个很重要的统计学概念——效应量。 今天的这个课程是给大家打一些基础，关于效应量和元分析的一些基础知识。并不是说上完这门课之后，大家就会有一个分析或者说能够熟练的去计算各种各样的效应量。因为大家后面会看到效应量的软件分析，其实很依赖整个研究问题和context。所以如何去解读效应不是一个技术的问题，这既需要懂得统计的方法，也需要你对研究领域很熟悉。提到效应量大家一定会想到Cohen’s d，实际上对效应量的定义是，研究者感兴趣的任何的效应量，只要研究者对它感兴趣，觉得它有意义，就可以认为它是研究中得效应量。所以效应量本质上就是效应的量（effects）。 心理学研究中的效应量通常是Cohen’s d，因为很多时候是比较不同组之间的差异，而不同组之间的差异进行比较的时候，我们希望摆脱它原始单位的影响，使之成为某种程度上没有单位的标准化统计量。而在其他的领域，很多时候关注点不一定在这种标准化的效应量。比方说，我们有的时候会看到，家庭的背景如何影响子女的教育成就。教育学的研究者不会关心标准化的效应量是多少。汇报Cohen’s d等于0.2或者0.4很难去解读它的实际意义。在这种情况下，变化家庭的收入，如每个月多1000元，子女的教育年限会如何增加，就可以把子女的教育用受教育的年份来进行一个量化，把家庭背景用诸如收入水平来进行量化。最后考察的是家庭收入对子女接受教育年限的影响，用年份或学位进行量化。这种研究直接以实际的这个生活中的这个效应作为效应量，而不是以quantity或者相关系数为效应量。所以首先大家一定要明确，效应量不是一个统计的指标，而是一个大家感兴趣的量。 在心理学的研究当中的话常常会碰到三大效应量： - d-family（difference family）：如Cohen’s d, Hedges’ g - r-family（correlation family）：如Pearson r, \\(R^2\\), \\(\\eta^2\\), \\(\\omega^2\\), &amp; f - OR-family（categorical family）：如odds ratio (OR), risk ratio(RR) 这些指标既说明两个变量之间有怎样的相关，也说明了某一个效应所解释的变异在总体的变异当中占多大的比例。不同的研究领域用的效应量也不一样。相对来说相关系数是最通用的，它会比d-family更加通用。如果大家看 Psychological Bulletin，心理学领域元分析发表很多的期刊，影响力非常高，它上面很多元分析都是以相关系数做为effect size。 2011年General of Experimental Psychology General杂志专门回顾了发表的文章中报告的效应量。其中发现了一个非常有趣的效应，报告最多的其实是Partial \\(\\eta\\) Squared，即\\(\\eta^{2}_{p}\\)，很重要的原因是SPSS输出这个效应量。 Lakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science: A practical primer for t-tests and ANOVAs. Frontiers in Psychology, 4, 863. 这篇文章报告了各种不同Cohen’s d的计算方式 推荐在研究中既报告效应量,也报告这个效应量的置信区间 - SPSS提供\\(\\eta^{2}_{p}\\) - JASP提供Cohen’s d，\\(\\eta^{2}_{p}\\)，\\(\\eta^{2}_{G}\\) Lakens(Lakens,2013)提供了基于excel的计算程序，帮助心理学家方便的得到不同的效应量及其置信区间。 Gpower也可以计算效应量，但其输出的\\(\\eta^{2}_{p}\\)与SPSS是不一样的，需要转换。 效应量的估计最终回归到了统计中一个非常重要的概念：点估计。存在点估计也就不得不提到区间估计。所以说有效应量的点估计就会有95%的自行区间 这个参数估计啊或者点估计 在2012年的时候Cumming(Cumming, 2012) 在一本书讲到，我们应该不仅仅使用P值 还要使用effect size，confidence interval和meta-analysis这三个统计指标。它们都有一个共同特点——estimation based，即专门基于估计。 与传统的基于P值二分法相比，基于估计的指标不仅可以看到效应量是否显著，也能看到效应量的具体大小，是把心理学研究和现实的因素关联的很重要的点。例如某个效应很显著，但是其效应量非常微弱，它在社会层面便没有任何意义。有时一个非常小的效应，没有任何坏处且成本很低，当它被实施，会改变一部分人的命运，这个时候或许可以考虑把这个方法应用到现实世界。在解读效应量的时候，注意避免简单的使用所谓的small，medium和large等effect size对Cohen’s d评价，不管它的效应、样本量、研究背景等等。 11.2 算法实现 如何去计算单个研究中或单个样本中的效应量、置信区间以及如何将它们综合起来。下面先给出公式： 独立样本t-test： \\[Cohen&#39;s \\ d_s = \\frac{X_1 - X_2}{\\sqrt{SD_{pool}}} = \\frac{X_1 - X_2}{\\sqrt{\\frac{(n_1 -1)SD_1^2 + (n_2-1)SD_2^2)}{n_1+n2-2}}}\\] \\[Hedges&#39;s \\ g_s = Cohen&#39;s \\ d_s \\times (1 - \\frac{3}{4(n_1 + n_2) - 9}) \\] 配对样本t-test： \\[Cohen&#39;s \\ d_{rm} = \\frac{M_{diff}}{\\sqrt{SD_1^2 + SD_2^2 -2 \\times r \\times SD_1 \\times SD_2}} \\times \\sqrt{2(1-r)} \\] \\[Cohen&#39;s \\ d_{av} = \\sqrt{ \\frac {M_{diff}} {(SD_1 + SD_2)/2}} \\] 接下来尝试计算Cohen’s d，以match数据为例 选定感兴趣的效应量: Match条件下好我与好人的平均反应时间差异，即自我在好的情况下,它的优势效应 首先还是导入数据： rm(list = ls()) if (!requireNamespace(&quot;pacman&quot;, quietly = TRUE)) { install.packages(&quot;pacman&quot;) } # # 检查是否已安装 pacman, 如果未安装，则安装包 pacman::p_load(&quot;tidyverse&quot;, &quot;easystats&quot;) # 使用p_load来载入需要的包 df.mt.raw &lt;- read.csv(&#39;./data/match/match_raw.csv&#39;, # load data: header = T, sep=&quot;,&quot;, stringsAsFactors = FALSE) 应用11章的预处理数据： # from chapter 11, Chunk 3 df.mt.rt.subj &lt;- df.mt.raw %&gt;% dplyr::filter(ACC == 1 &amp; RT &gt; 0.2) %&gt;% tidyr::extract(Shape, into = c(&quot;Valence&quot;, &quot;Identity&quot;), regex = &quot;(moral|immoral)(Self|Other)&quot;, remove = FALSE) %&gt;% dplyr::mutate(Valence = case_when(Valence == &quot;moral&quot; ~ &quot;Good&quot;, Valence == &quot;immoral&quot; ~ &quot;Bad&quot;), RT_ms = RT * 1000) %&gt;% dplyr::mutate(Valence = factor(Valence, levels = c(&quot;Good&quot;, &quot;Bad&quot;)), Identity = factor(Identity, levels = c(&quot;Self&quot;, &quot;Other&quot;))) %&gt;% dplyr::group_by(Sub, Match, Identity, Valence) %&gt;% dplyr::summarise(RT_mean = mean(RT_ms)) %&gt;% dplyr::ungroup() head(df.mt.rt.subj, 5) ## # A tibble: 5 × 5 ## Sub Match Identity Valence RT_mean ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 7302 match Self Good 694. ## 2 7302 match Self Bad 702. ## 3 7302 match Other Good 598. ## 4 7302 match Other Bad 666. ## 5 7302 mismatch Self Good 755. 对于我们感兴趣的效应量，计算需要知道五个变量：条件1的主水平RT、条件2主水平RT、条件1的主水平SD、条件2的主水平SD以及相关系数（可以用每个被试在两个条件之间的反应时间做一个相关）。 # from chapter 11, Chunk 3 df.mt.rt.subj.effect &lt;- df.mt.rt.subj %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% dplyr::group_by(Identity) %&gt;% dplyr::summarise(mean = mean(RT_mean), sd = sd(RT_mean)) df.mt.rt.subj.effect.wide &lt;- df.mt.rt.subj %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% tidyr::pivot_wider(names_from = &quot;Identity&quot;, values_from = &quot;RT_mean&quot;) corr_est &lt;- cor(df.mt.rt.subj.effect.wide$Self, df.mt.rt.subj.effect.wide$Other) 得到变量后，将Cohen’s drm公式转化为代码： Cohens_d_manu &lt;- ((df.mt.rt.subj.effect$mean[1] - df.mt.rt.subj.effect$mean[2])/sqrt(df.mt.rt.subj.effect$sd[1]**2 + df.mt.rt.subj.effect$sd[2]**2 - 2*corr_est*df.mt.rt.subj.effect$sd[1]*df.mt.rt.subj.effect$sd[2]))*sqrt(2*(1-corr_est)) Cohens_d_manu ## [1] -0.5676815 也可以使用其他方法计算，发现得到不同的效应量，原因是公式并不一致。 SelfOther_diff &lt;- t.test(df.mt.rt.subj.effect.wide$Self, df.mt.rt.subj.effect.wide$Other, paired = TRUE) effectsize::effectsize(SelfOther_diff, paired = TRUE) ## Cohen&#39;s d | 95% CI ## -------------------------- ## -0.48 | [-0.79, -0.16] 所以在调用R包时一定要注意查看其内部公式！ 11.3 小练习 在match数据中，尝试计算自我条件下好与坏的Cohen’s d。分别使用三种方法，（1）手动；（2）effectsize工具包；（3）bruceR工具包-T test 11.4 元分析简介 想象你自己做了一个治疗研究，在平均主义这个框架之下,我们会通常认为真实的世界中存在对于该研究特定的效应量。假如这个研究被一万次重复，就会得到一万个效应，且这一万个效应会趋向于一个真实的情况,你的治疗方法是不是比没有治疗过程或选用其他的旧的治疗方法更有效率。 虽然说这个假定可能不合理,但是平均主义就是这样的假定。 假如说有很多人同时在不同地方做类似实验,把这些效应拿过来,对它们进行综合。本质上,某种程度这些实验关注着同一个效应,综合之后应该能够得到一个对真实的更好的估计，在统计过程当中是这样。所以就有了一个对效应量的综合方法——元分析。 元分析是一个统计方法，从这个角度来讲，是对已经存在的多个效应量,通过统计的方法把它进行综合。综合时需要效应量，相关系数R或者是Cohen’s d， 同时我们也需要效应量的估计误差作为权重。接下来都是技术的问题。 怎么去算这个权重呢? 在不同的方法里面或者不同的模型里面,会有自己的具体计算公式。很多时候,如果大家不想了解细节的话,可以不用去管它。如果想了解细节的话,可以去看权重到底是如何确定的。一般来说传统的元分析模型,是通过效应量变异大小的导数对它进行加权。例如对标准差的平方求导数，然后进行加权。当然还会有其他的方法,这里就不细致的展开。 元分析也是一种文章类型，例如之前提到的psychological bulletin，具有很高的影响因子。但也存在问题，有的时候元分析不被认为是实证的研究,因为其没有太多的创新性。 11.5 元分析实现 下面展示一下如何在自己已有的数据中，用R来做一个简单元分析： 把数据分成两部分, 一部分21名被试，一部分23名被试。 subjs &lt;- unique(df.mt.rt.subj$Sub) set.seed(1234) subj_ls1 &lt;- sample(subjs, 21) df.mt.rt.subj.ls1 &lt;- df.mt.rt.subj %&gt;% dplyr::filter(Sub %in% subj_ls1) df.mt.rt.subj.ls2 &lt;- df.mt.rt.subj %&gt;% dplyr::filter(!(Sub %in% subj_ls1)) 假定两组数据分别为实验1a与实验1b，并计算两个实验同样条件下的效应量。 与上文中计算效应量的方式一致，首先计算均值等所需变量： ## effect size of group 1 df.mt.rt.subj.effect.ls1 &lt;- df.mt.rt.subj.ls1 %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% dplyr::group_by(Identity) %&gt;% dplyr::summarise(mean = mean(RT_mean), sd = sd(RT_mean)) %&gt;% dplyr::ungroup() %&gt;% tidyr::pivot_wider(names_from = Identity, values_from = c(mean, sd)) colnames(df.mt.rt.subj.effect.ls1) &lt;- c(&quot;Self_RT_M_mean&quot;,&quot;Other_RT_M_mean&quot;, &quot;Self_RT_M_sd&quot;, &quot;Other_RT_M_sd&quot;) df.mt.rt.subj.effect.ls1.wide &lt;- df.mt.rt.subj.ls1 %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% tidyr::pivot_wider(names_from = &quot;Identity&quot;, values_from = &quot;RT_mean&quot;) corr_est.ls1 &lt;- cor(df.mt.rt.subj.effect.ls1.wide$Self, df.mt.rt.subj.effect.ls1.wide$Other) df.mt.rt.subj.effect.ls1$Sample_size &lt;- length(unique(df.mt.rt.subj.ls1$Sub)) df.mt.rt.subj.effect.ls1$ri &lt;- corr_est.ls1 ## effect size of group 2 df.mt.rt.subj.effect.ls2 &lt;- df.mt.rt.subj.ls2 %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% dplyr::group_by(Identity) %&gt;% dplyr::summarise(mean = mean(RT_mean), sd = sd(RT_mean)) %&gt;% dplyr::ungroup() %&gt;% tidyr::pivot_wider(names_from = Identity, values_from = c(mean, sd)) colnames(df.mt.rt.subj.effect.ls2) &lt;- c(&quot;Self_RT_M_mean&quot;,&quot;Other_RT_M_mean&quot;, &quot;Self_RT_M_sd&quot;, &quot;Other_RT_M_sd&quot;) df.mt.rt.subj.effect.ls2.wide &lt;- df.mt.rt.subj.ls2 %&gt;% dplyr::filter(Match == &quot;match&quot; &amp; Valence == &quot;Good&quot;) %&gt;% tidyr::pivot_wider(names_from = &quot;Identity&quot;, values_from = &quot;RT_mean&quot;) corr_est.ls2 &lt;- cor(df.mt.rt.subj.effect.ls2.wide$Self, df.mt.rt.subj.effect.ls2.wide$Other) df.mt.rt.subj.effect.ls2$Sample_size &lt;- length(unique(df.mt.rt.subj.ls2$Sub)) df.mt.rt.subj.effect.ls2$ri &lt;- corr_est.ls2 合并数据，计算效应量和效应量的误差，调用R包metafor： # and nrow with 1 df.mt.meta &lt;- rbind(df.mt.rt.subj.effect.ls1, df.mt.rt.subj.effect.ls2) df.es &lt;- metafor::escalc( measure = &quot;SMCRH&quot;, #standardized mean change using raw score standardization with heteroscedastic population variances at the two measurement occasions (Bonett, 2008) m1i = Self_RT_M_mean, m2i = Other_RT_M_mean, sd1i = Self_RT_M_sd, sd2i = Other_RT_M_sd, ni = Sample_size, ri = ri, data = df.mt.meta ) %&gt;% dplyr::mutate(unique_ID = c(&quot;study1a&quot;, &quot;study1b&quot;)) 我们此时得到了两组数据的效应量以及其误差(Y1:-0.4271211,Y2:-0.9114523,V1:0.0784022,V2:0.1185285) 接着建立随机效应模型： # 随机效果模型 rma1 &lt;- metafor::rma(yi, vi, data = df.es) 此处应该添加chapter-13meta结果与forest plot 这是最简单的可视化，在发表时还应考虑将图表绘制的更加直观与美观。 其中有许多细节，例如22的实验中，效应量数量为C82=28，在报告时需要选择合适的效应；选择的计算方法（效应量公式）也值得考虑，是手动输入还是函数中自带的公式；在一些报告中只会提供均值与标准差，而不提供相关系数，那么就只能依赖于作者报告的数据选择。 元分析还可以应用于自己的研究中： - 如果自己手中有许多实验，对这些实验进行效应量计算的工作，能使得实验更加准确（mini metaanalysis） - 在做预实验时发现效应不够显著，又做了正式实验，可以通过元分析的方式将样本量增加，也能通过元分析判断效应量是否稳定。 在使用Mini Meta-Analysis时不能只把显著的效应综合起来，只把显著效应综合起来的话，效应就被高估了* "],["references.html", "References", " References "]]
